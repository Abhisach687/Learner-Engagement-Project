{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models directory exists: True\n",
      "Checkpoint path writable: True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import torch\n",
    "import optuna\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import io\n",
    "import lmdb  # pip install lmdb\n",
    "import logging\n",
    "import sqlite3\n",
    "from optuna.pruners import MedianPruner\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "import matplotlib.pyplot as plt\n",
    "import timm  # pip install timm\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# ------------------------------\n",
    "# CONSTANTS & HYPERPARAMETERS\n",
    "# ------------------------------\n",
    "GRADIENT_ACCUM_STEPS = 2        # Changed from 4 to 2 for optimal VRAM usage\n",
    "NUM_FRAMES = 50\n",
    "# Progressive resolution schedule: (resolution, epochs)\n",
    "PROG_SCHEDULE = [(112, 6), (224, 8), (300, 6)]  # Modified schedule\n",
    "BATCH_SIZES = {\n",
    "    112: 8,  # Higher batch size for low resolution\n",
    "    224: 6,  # Medium batch size\n",
    "    300: 4   # Lower batch size for high resolution\n",
    "}\n",
    "FOCAL_ALPHA = 0.25\n",
    "FOCAL_GAMMA = 2.0\n",
    "\n",
    "# ------------------------------\n",
    "# Environment & Paths\n",
    "# ------------------------------\n",
    "BASE_DIR = Path(\"C:/Users/abhis/Downloads/Documents/Learner Engagement Project\")\n",
    "DATA_DIR = BASE_DIR / \"data\" / \"DAiSEE\"\n",
    "FRAMES_DIR = DATA_DIR / \"ExtractedFrames\"\n",
    "LABELS_DIR = DATA_DIR / \"Labels\"\n",
    "MODEL_DIR = BASE_DIR / \"models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR = BASE_DIR / \"cache\"\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Models directory exists:\", os.path.exists(MODEL_DIR))\n",
    "print(\"Checkpoint path writable:\", os.access(MODEL_DIR, os.W_OK))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ------------------------------\n",
    "# Image Transformations\n",
    "# ------------------------------\n",
    "def get_transform(resolution):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Utility Functions\n",
    "# ------------------------------\n",
    "def get_csv_clip_id(video_stem: str) -> str:\n",
    "    base = video_stem.strip()\n",
    "    return base.replace(\"110001\", \"202614\", 1) if base.startswith(\"110001\") else base\n",
    "\n",
    "def select_impactful_frames(video_folder: Path, num_frames=50):\n",
    "    frame_files = sorted(video_folder.glob(\"frame_*.jpg\"))\n",
    "    total = len(frame_files)\n",
    "    if total == 0:\n",
    "        return []\n",
    "    if total <= num_frames:\n",
    "        return frame_files\n",
    "    indices = np.linspace(0, total - 1, num_frames, dtype=int)\n",
    "    return [frame_files[i] for i in indices]\n",
    "\n",
    "# ------------------------------\n",
    "# Precomputation & Caching Functions\n",
    "# ------------------------------\n",
    "def precompute_best_frames(csv_file: Path, video_root: Path, num_frames=50, resolution=224):\n",
    "    data = pd.read_csv(csv_file, dtype=str)\n",
    "    data.columns = data.columns.str.strip()\n",
    "    split = csv_file.stem.replace(\"Labels\", \"\").strip()\n",
    "    valid_indices = []\n",
    "    precomputed = []\n",
    "    skipped = 0\n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data),\n",
    "                         desc=f\"Precomputing frames for {csv_file.stem} at {resolution}x{resolution}\"):\n",
    "        clip_id = get_csv_clip_id(row[\"ClipID\"].split('.')[0])\n",
    "        video_folder = video_root / split / clip_id\n",
    "        if video_folder.exists():\n",
    "            frames = select_impactful_frames(video_folder, num_frames)\n",
    "            if len(frames) >= num_frames:\n",
    "                precomputed.append(frames[:num_frames])\n",
    "                valid_indices.append(idx)\n",
    "            else:\n",
    "                skipped += 1\n",
    "        else:\n",
    "            skipped += 1\n",
    "    print(f\"Precomputation: Skipped {skipped} videos out of {len(data)}.\")\n",
    "    cache_data = {\"valid_indices\": valid_indices, \"precomputed_frames\": precomputed}\n",
    "    cache_file = CACHE_DIR / f\"precomputed_{csv_file.stem}_frame_{num_frames}_{resolution}.pkl\"\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(cache_data, f)\n",
    "    print(f\"Precomputed results saved to {cache_file}\")\n",
    "    return cache_data\n",
    "\n",
    "def convert_pkl_to_lmdb(csv_file: Path, num_frames=50, resolution=224,\n",
    "                          transform=None, lmdb_map_size=1 * 1024**3):\n",
    "    if transform is None:\n",
    "        transform = get_transform(resolution)\n",
    "    pkl_file = CACHE_DIR / f\"precomputed_{csv_file.stem}_frame_{num_frames}_{resolution}.pkl\"\n",
    "    lmdb_path = CACHE_DIR / f\"lmdb_{csv_file.stem}_frame_{num_frames}_{resolution}\"\n",
    "    # If LMDB already exists, return it.\n",
    "    if (lmdb_path / \"data.mdb\").exists():\n",
    "        print(f\"LMDB database already exists at {lmdb_path}\")\n",
    "        return lmdb_path\n",
    "\n",
    "    env = lmdb.open(str(lmdb_path), map_size=lmdb_map_size)\n",
    "    if not pkl_file.exists():\n",
    "        precompute_best_frames(csv_file, FRAMES_DIR, num_frames=num_frames, resolution=resolution)\n",
    "    with open(pkl_file, \"rb\") as f:\n",
    "        cache = pickle.load(f)\n",
    "    valid_indices = cache[\"valid_indices\"]\n",
    "    file_paths_list = cache[\"precomputed_frames\"]\n",
    "\n",
    "    # Use EfficientNetV2-L (tf variant) from timm with frozen backbone\n",
    "    backbone = timm.create_model(\"tf_efficientnetv2_l\", pretrained=True)\n",
    "    backbone.reset_classifier(0)\n",
    "    backbone.eval()\n",
    "    backbone.to(device)\n",
    "    for param in backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    print(f\"Converting frame paths to LMDB features for {csv_file.stem} at {resolution}x{resolution} ...\")\n",
    "    with env.begin(write=True) as txn:\n",
    "        for idx, paths in tqdm(enumerate(file_paths_list), total=len(file_paths_list)):\n",
    "            video_features = []\n",
    "            for fp in paths:\n",
    "                try:\n",
    "                    img = Image.open(fp).convert(\"RGB\")\n",
    "                except Exception:\n",
    "                    img = Image.new('RGB', (resolution, resolution))\n",
    "                tensor = transform(img).unsqueeze(0).to(device)\n",
    "                with torch.no_grad(), autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    feat = backbone(tensor)\n",
    "                    feat = feat.squeeze(0).cpu().half().detach()  # ensure detached and on CPU\n",
    "                # Convert to NumPy array to avoid pickling issues with torch tensors\n",
    "                video_features.append(feat.numpy())\n",
    "            if video_features:\n",
    "                video_features_np = np.stack(video_features)  # shape: (num_frames, feature_dim)\n",
    "                key = f\"video_{valid_indices[idx]}\".encode(\"utf-8\")\n",
    "                txn.put(key, pickle.dumps(video_features_np))\n",
    "    env.close()\n",
    "    print(f\"LMDB database created at {lmdb_path}\")\n",
    "    return lmdb_path\n",
    "\n",
    "# ------------------------------\n",
    "# LMDB Dataset Classes\n",
    "# ------------------------------\n",
    "class VideoDatasetLMDB(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, lmdb_path, num_frames=50, resolution=224):\n",
    "        self.data = pd.read_csv(csv_file, dtype=str)\n",
    "        self.data.columns = self.data.columns.str.strip()\n",
    "        self.resolution = resolution\n",
    "        pkl_file = CACHE_DIR / f\"precomputed_{csv_file.stem}_frame_{num_frames}_{resolution}.pkl\"\n",
    "        with open(pkl_file, \"rb\") as f:\n",
    "            cache = pickle.load(f)\n",
    "        self.valid_indices = cache[\"valid_indices\"]\n",
    "        self.data = self.data.iloc[self.valid_indices].reset_index(drop=True)\n",
    "        self.num_frames = num_frames\n",
    "        self.lmdb_path = str(lmdb_path)\n",
    "        self.env = None\n",
    "\n",
    "    def _init_env(self):\n",
    "        if self.env is None:\n",
    "            self.env = lmdb.open(self.lmdb_path, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "        return self.env\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        env = self._init_env()\n",
    "        original_idx = self.valid_indices[idx]\n",
    "        key = f\"video_{original_idx}\".encode(\"utf-8\")\n",
    "        with env.begin(write=False) as txn:\n",
    "            data_bytes = txn.get(key)\n",
    "            if data_bytes is None:\n",
    "                raise IndexError(f\"Key {key} not found in LMDB\")\n",
    "            features_np = pickle.loads(data_bytes)\n",
    "            features = torch.from_numpy(features_np)\n",
    "        labels = self.data.iloc[idx][[\"Engagement\", \"Boredom\", \"Confusion\", \"Frustration\"]].astype(int)\n",
    "        return features, torch.tensor(labels.values, dtype=torch.long)\n",
    "\n",
    "class VideoDatasetRaw(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, video_root, num_frames=50, transform=None):\n",
    "        self.data = pd.read_csv(csv_file, dtype=str)\n",
    "        self.data.columns = self.data.columns.str.strip()\n",
    "        pkl_file = CACHE_DIR / f\"precomputed_{csv_file.stem}_frame_{num_frames}_raw.pkl\"\n",
    "        if not pkl_file.exists():\n",
    "            cache = precompute_best_frames(csv_file, video_root, num_frames=num_frames)\n",
    "            with open(pkl_file, \"wb\") as f:\n",
    "                pickle.dump(cache, f)\n",
    "        else:\n",
    "            with open(pkl_file, \"rb\") as f:\n",
    "                cache = pickle.load(f)\n",
    "        self.valid_indices = cache[\"valid_indices\"]\n",
    "        self.file_paths = cache[\"precomputed_frames\"]\n",
    "        self.data = self.data.iloc[self.valid_indices].reset_index(drop=True)\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        paths = self.file_paths[idx]\n",
    "        frames = []\n",
    "        for fp in paths:\n",
    "            try:\n",
    "                img = Image.open(fp).convert(\"RGB\")\n",
    "            except Exception:\n",
    "                img = Image.new('RGB', (self.transform.transforms[0].size, self.transform.transforms[0].size))\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            frames.append(img)\n",
    "        video_tensor = torch.stack(frames)  # (T, C, H, W)\n",
    "        labels = self.data.iloc[idx][[\"Engagement\", \"Boredom\", \"Confusion\", \"Frustration\"]].astype(int)\n",
    "        return video_tensor, torch.tensor(labels.values, dtype=torch.long)\n",
    "\n",
    "# ------------------------------\n",
    "# Focal Loss Implementation\n",
    "# ------------------------------\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA, reduction=\"mean\"):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.ce = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = self.alpha * ((1 - pt) ** self.gamma) * ce_loss\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "# ------------------------------\n",
    "# CBAM Module (Spatial-Only)\n",
    "# ------------------------------\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        # Remove channel attention, keep only spatial attention\n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Conv1d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T)\n",
    "        # Apply spatial attention only - no channel attention\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        spatial_attn = self.spatial_attention(torch.cat([avg_out, max_out], dim=1))\n",
    "        return x * spatial_attn\n",
    "    \n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.query = nn.Linear(feature_dim, feature_dim)\n",
    "        self.key = nn.Linear(feature_dim, feature_dim)\n",
    "        self.value = nn.Linear(feature_dim, feature_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    def forward(self, spatial_feat, temporal_feat):\n",
    "        # spatial_feat: (B, feature_dim)\n",
    "        # temporal_feat: (B, feature_dim)\n",
    "        query = self.query(spatial_feat).unsqueeze(1)  # (B, 1, feature_dim)\n",
    "        key = self.key(temporal_feat).unsqueeze(2)       # (B, feature_dim, 1)\n",
    "        attn = self.softmax(torch.bmm(query, key))       # (B, 1, 1)\n",
    "        value = self.value(temporal_feat)\n",
    "        return attn.squeeze(-1) * value  # (B, feature_dim)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Model Architecture\n",
    "# ------------------------------\n",
    "class EfficientNetV2L_BiLSTM_CrossAttn_CBAM(nn.Module):\n",
    "    def __init__(self, lstm_hidden=256, lstm_layers=2, dropout_rate=0.5, classifier_hidden=256):\n",
    "        super(EfficientNetV2L_BiLSTM_CrossAttn_CBAM, self).__init__()\n",
    "        self.backbone = timm.create_model(\"tf_efficientnetv2_l\", pretrained=True)\n",
    "        self.backbone.reset_classifier(0)\n",
    "        # Freeze early layers (up to block index 5)\n",
    "        if hasattr(self.backbone, \"blocks\"):\n",
    "            for i, block in enumerate(self.backbone.blocks):\n",
    "                if i < 6:\n",
    "                    for param in block.parameters():\n",
    "                        param.requires_grad = False\n",
    "        # Dynamically set feature dimension based on the backbone\n",
    "        if hasattr(self.backbone, \"num_features\"):\n",
    "            self.feature_dim = self.backbone.num_features\n",
    "        else:\n",
    "            self.feature_dim = 1536  # fallback if attribute not present\n",
    "        \n",
    "        # Spatial-only CBAM\n",
    "        self.cbam = CBAM(self.feature_dim, kernel_size=7)\n",
    "        \n",
    "        # 2-layer BiLSTM with dropout\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=self.feature_dim, \n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=lstm_layers,  # Changed from 1 to 2\n",
    "            batch_first=True, \n",
    "            bidirectional=True,\n",
    "            dropout=0.3  # Added dropout between layers\n",
    "        )\n",
    "        \n",
    "        self.spatial_proj = nn.Linear(self.feature_dim, classifier_hidden)\n",
    "        self.temporal_proj = nn.Linear(2 * lstm_hidden, classifier_hidden)\n",
    "        self.cross_attn = CrossAttention(classifier_hidden)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(classifier_hidden * 2, 16)  # 16 outputs (reshaped to 4x4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Support both raw image inputs (B, T, C, H, W) and precomputed features (B, T, feature_dim)\n",
    "        if x.dim() == 5:\n",
    "            # Raw images: (B, T, C, H, W)\n",
    "            B, T, C, H, W = x.size()\n",
    "            x = x.view(-1, C, H, W)  # (B*T, C, H, W)\n",
    "            features = self.backbone(x)  # (B*T, feature_dim)\n",
    "            features = features.view(B, T, self.feature_dim)  # (B, T, feature_dim)\n",
    "        elif x.dim() == 3:\n",
    "            # Precomputed features: (B, T, feature_dim)\n",
    "            features = x\n",
    "            B, T, _ = features.size()\n",
    "        else:\n",
    "            raise ValueError(\"Input tensor must have 3 or 5 dimensions.\")\n",
    "            \n",
    "        # Apply CBAM: convert to (B, feature_dim, T)\n",
    "        features = features.permute(0, 2, 1)\n",
    "        features = self.cbam(features)\n",
    "        features = features.permute(0, 2, 1)  # back to (B, T, feature_dim)\n",
    "        \n",
    "        # Process temporal information with BiLSTM\n",
    "        lstm_out, (h_n, _) = self.bilstm(features)\n",
    "        \n",
    "        # Combine final hidden states with mean pooling\n",
    "        final_hidden = torch.cat((h_n[-2], h_n[-1]), dim=1)  # Last layer, both directions\n",
    "        mean_pooled = torch.mean(lstm_out, dim=1)  # Mean across time dimension\n",
    "        temporal_context = final_hidden + 0.3 * mean_pooled  # Weighted combination\n",
    "        \n",
    "        # Spatial features\n",
    "        spatial_feature = features.mean(dim=1)  # (B, feature_dim)\n",
    "        \n",
    "        spatial_proj = self.spatial_proj(spatial_feature)  # (B, classifier_hidden)\n",
    "        temporal_proj = self.temporal_proj(temporal_context)  # (B, classifier_hidden)\n",
    "        cross_context = self.cross_attn(spatial_proj, temporal_proj)  # (B, classifier_hidden)\n",
    "        \n",
    "        fusion = torch.cat((spatial_proj, cross_context), dim=1)  # (B, 2*classifier_hidden)\n",
    "        fusion = self.dropout(fusion)\n",
    "        logits = self.classifier(fusion)  # (B, 16)\n",
    "        \n",
    "        return logits.view(B, 4, 4)\n",
    "\n",
    "# ------------------------------\n",
    "# Training Function with Progressive Resolution, Mixed Precision & Gradient Accumulation\n",
    "# ------------------------------\n",
    "def progressive_train_model(model, total_epochs, lr, checkpoint_path, batch_size,\n",
    "                            patience=5, gradient_accum_steps=GRADIENT_ACCUM_STEPS):\n",
    "    # Set parameters for AdamW optimizer\n",
    "    backbone_params = list(model.backbone.parameters())\n",
    "    backbone_param_ids = {id(p) for p in backbone_params}\n",
    "    other_params = [p for p in model.parameters() if id(p) not in backbone_param_ids]\n",
    "    \n",
    "    optimizer = optim.AdamW([\n",
    "        {\"params\": backbone_params, \"lr\": lr * 0.1},  # Lower LR for backbone\n",
    "        {\"params\": other_params, \"lr\": lr}\n",
    "    ], weight_decay=1e-4)\n",
    "    \n",
    "    # Calculate total epochs for cosine annealing\n",
    "    total_steps = sum(ep * (len(pd.read_csv(train_csv)) // BATCH_SIZES[res] + 1) \n",
    "                      for res, ep in PROG_SCHEDULE)\n",
    "    \n",
    "    # Cosine annealing scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, \n",
    "        T_max=total_steps // gradient_accum_steps,\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    scaler = GradScaler()\n",
    "    focal_loss = FocalLoss(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA).to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    current_epoch = 0\n",
    "    for res_idx, (resolution, epochs) in enumerate(PROG_SCHEDULE):\n",
    "        # Update batch size based on resolution\n",
    "        current_batch_size = BATCH_SIZES[resolution]\n",
    "        \n",
    "        print(f\"\\nTraining at resolution {resolution}px for {epochs} epochs\")\n",
    "        print(f\"Batch size: {current_batch_size}, Gradient accumulation: {gradient_accum_steps}x\")\n",
    "        \n",
    "        # Freezing strategy based on resolution stage\n",
    "        if res_idx == 0:  # First resolution (112px)\n",
    "            # Freeze backbone except final layers\n",
    "            for i, layer in enumerate(model.backbone.blocks):\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = (i >= 6)\n",
    "                    \n",
    "        elif res_idx == 1:  # Second resolution (224px)\n",
    "            # Unfreeze more backbone layers\n",
    "            for param in model.backbone.parameters():\n",
    "                param.requires_grad = True\n",
    "                \n",
    "        # At highest resolution, everything is already unfrozen\n",
    "        \n",
    "        # Setup data for current resolution\n",
    "        transform = get_transform(resolution)\n",
    "        train_lmdb = convert_pkl_to_lmdb(train_csv, num_frames=NUM_FRAMES, resolution=resolution,\n",
    "                                          transform=transform, lmdb_map_size=1 * 1024**3)\n",
    "        val_lmdb = convert_pkl_to_lmdb(val_csv, num_frames=NUM_FRAMES, resolution=resolution,\n",
    "                                        transform=transform, lmdb_map_size=1 * 1024**3)\n",
    "        train_set = VideoDatasetLMDB(train_csv, train_lmdb, num_frames=NUM_FRAMES, resolution=resolution)\n",
    "        val_set = VideoDatasetLMDB(val_csv, val_lmdb, num_frames=NUM_FRAMES, resolution=resolution)\n",
    "        \n",
    "        train_loader = DataLoader(train_set, batch_size=current_batch_size, shuffle=True, \n",
    "                                  num_workers=2, pin_memory=True)\n",
    "        val_loader = DataLoader(val_set, batch_size=current_batch_size, shuffle=False, \n",
    "                                num_workers=2, pin_memory=True)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Progressive Training: Epoch {current_epoch+1}/{total_epochs} at resolution {resolution}x{resolution}\")\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            for i, (features, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "                features = features.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "                \n",
    "                with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    outputs = model(features)\n",
    "                    outputs = outputs.view(outputs.size(0), 4, 4)\n",
    "                    # Compute loss over the 4 outputs\n",
    "                    loss = sum(focal_loss(outputs[:, d], labels[:, d]) for d in range(4)) / 4.0\n",
    "                \n",
    "                # Apply gradient accumulation\n",
    "                scaler.scale(loss / gradient_accum_steps).backward()\n",
    "                \n",
    "                if (i + 1) % gradient_accum_steps == 0:\n",
    "                    scaler.step(optimizer)\n",
    "                    scheduler.step()  # Update learning rate\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "                \n",
    "                running_loss += loss.item() * features.size(0)\n",
    "                \n",
    "                # Cleanup to avoid memory issues\n",
    "                del features, labels, outputs, loss\n",
    "                if (i + 1) % 30 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "            \n",
    "            train_loss = running_loss / len(train_loader.dataset)\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad(), autocast(device_type='cuda', dtype=torch.float16):\n",
    "                for features, labels in val_loader:\n",
    "                    features = features.to(device, non_blocking=True)\n",
    "                    labels = labels.to(device, non_blocking=True)\n",
    "                    outputs = model(features)\n",
    "                    outputs = outputs.view(outputs.size(0), 4, 4)\n",
    "                    loss = sum(focal_loss(outputs[:, d], labels[:, d]) for d in range(4)) / 4.0\n",
    "                    val_loss += loss.item() * features.size(0)\n",
    "            \n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            print(f\"Epoch {current_epoch+1}/{total_epochs} at {resolution}x{resolution} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                state = {\n",
    "                    \"epoch\": current_epoch + 1,\n",
    "                    \"best_val_loss\": best_val_loss,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict()\n",
    "                }\n",
    "                temp_path = checkpoint_path.with_suffix(\".tmp\")\n",
    "                torch.save(state, temp_path, _use_new_zipfile_serialization=False)\n",
    "                if checkpoint_path.exists():\n",
    "                    checkpoint_path.unlink()\n",
    "                temp_path.rename(checkpoint_path)\n",
    "                early_stop_counter = 0\n",
    "                print(f\"Model saved to {checkpoint_path}\")\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                \n",
    "            if early_stop_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {current_epoch+1}. Best val loss: {best_val_loss:.4f}\")\n",
    "                return best_val_loss\n",
    "                \n",
    "            current_epoch += 1\n",
    "            \n",
    "    return best_val_loss\n",
    "\n",
    "# ------------------------------\n",
    "# Evaluation Function\n",
    "# ------------------------------\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad(), autocast(device_type='cuda', dtype=torch.float16):\n",
    "        for frames, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            frames = frames.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model(frames)\n",
    "            outputs = outputs.view(outputs.size(0), 4, 4)\n",
    "            preds = torch.argmax(outputs, dim=2)\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "    for i, metric in enumerate([\"Engagement\", \"Boredom\", \"Confusion\", \"Frustration\"]):\n",
    "        print(f\"Classification report for {metric}:\")\n",
    "        print(classification_report(all_labels[:, i], all_preds[:, i], digits=3))\n",
    "        cm = confusion_matrix(all_labels[:, i], all_preds[:, i])\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Confusion Matrix for {metric}\")\n",
    "        plt.colorbar()\n",
    "        plt.xticks(np.arange(cm.shape[0]), np.arange(cm.shape[0]))\n",
    "        plt.yticks(np.arange(cm.shape[1]), np.arange(cm.shape[1]))\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing frames for TrainLabels at 112x112:   0%|          | 0/5358 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing frames for TrainLabels at 112x112: 100%|██████████| 5358/5358 [00:18<00:00, 289.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputation: Skipped 507 videos out of 5358.\n",
      "Precomputed results saved to C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\precomputed_TrainLabels_frame_50_112.pkl\n",
      "LMDB database already exists at C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\lmdb_TrainLabels_frame_50_112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing frames for TrainLabels at 224x224: 100%|██████████| 5358/5358 [00:17<00:00, 314.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputation: Skipped 507 videos out of 5358.\n",
      "Precomputed results saved to C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\precomputed_TrainLabels_frame_50_224.pkl\n",
      "LMDB database already exists at C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\lmdb_TrainLabels_frame_50_224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing frames for TrainLabels at 300x300: 100%|██████████| 5358/5358 [00:17<00:00, 303.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputation: Skipped 507 videos out of 5358.\n",
      "Precomputed results saved to C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\precomputed_TrainLabels_frame_50_300.pkl\n",
      "LMDB database already exists at C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\lmdb_TrainLabels_frame_50_300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing frames for ValidationLabels at 112x112: 100%|██████████| 1429/1429 [00:04<00:00, 297.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputation: Skipped 0 videos out of 1429.\n",
      "Precomputed results saved to C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\precomputed_ValidationLabels_frame_50_112.pkl\n",
      "LMDB database already exists at C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\lmdb_ValidationLabels_frame_50_112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing frames for ValidationLabels at 224x224: 100%|██████████| 1429/1429 [00:05<00:00, 282.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputation: Skipped 0 videos out of 1429.\n",
      "Precomputed results saved to C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\precomputed_ValidationLabels_frame_50_224.pkl\n",
      "LMDB database already exists at C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\lmdb_ValidationLabels_frame_50_224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing frames for ValidationLabels at 300x300: 100%|██████████| 1429/1429 [00:05<00:00, 274.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputation: Skipped 0 videos out of 1429.\n",
      "Precomputed results saved to C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\precomputed_ValidationLabels_frame_50_300.pkl\n",
      "LMDB database already exists at C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\lmdb_ValidationLabels_frame_50_300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing frames for TestLabels at 112x112: 100%|██████████| 1784/1784 [00:06<00:00, 291.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputation: Skipped 146 videos out of 1784.\n",
      "Precomputed results saved to C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\precomputed_TestLabels_frame_50_112.pkl\n",
      "LMDB database already exists at C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\lmdb_TestLabels_frame_50_112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing frames for TestLabels at 224x224: 100%|██████████| 1784/1784 [00:06<00:00, 293.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputation: Skipped 146 videos out of 1784.\n",
      "Precomputed results saved to C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\precomputed_TestLabels_frame_50_224.pkl\n",
      "LMDB database already exists at C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\lmdb_TestLabels_frame_50_224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing frames for TestLabels at 300x300: 100%|██████████| 1784/1784 [00:05<00:00, 298.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputation: Skipped 146 videos out of 1784.\n",
      "Precomputed results saved to C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\precomputed_TestLabels_frame_50_300.pkl\n",
      "LMDB database already exists at C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\lmdb_TestLabels_frame_50_300\n",
      "Database created/connected at: C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\notebooks\\tuning_eff_v2l_bilstm_spatialcbam_optim.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-02 12:41:11,768] Using an existing study with name 'efficientnetv2l_bilstm_spatialcbam_study' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna tuning complete. Total successful trials: 30\n",
      "Best trial parameters: {'batch_size': 4, 'lr': 7.941136280152968e-05, 'lstm_hidden': 256, 'dropout_rate': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/tf_efficientnetv2_l.in21k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/tf_efficientnetv2_l.in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Evaluating:   1%|          | 3/410 [01:08<2:23:18, 21.13s/it]"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Main Execution\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    torch.multiprocessing.set_start_method('spawn')\n",
    "    \n",
    "    # CSV file paths\n",
    "    train_csv = LABELS_DIR / \"TrainLabels.csv\"\n",
    "    val_csv = LABELS_DIR / \"ValidationLabels.csv\"\n",
    "    test_csv = LABELS_DIR / \"TestLabels.csv\"\n",
    "    \n",
    "    # Precompute caches and LMDB for each resolution\n",
    "    resolutions = [112, 224, 300]\n",
    "    for csv in [train_csv, val_csv, test_csv]:\n",
    "        for res in resolutions:\n",
    "            precompute_best_frames(csv, FRAMES_DIR, num_frames=NUM_FRAMES, resolution=res)\n",
    "            convert_pkl_to_lmdb(csv, num_frames=NUM_FRAMES, resolution=res,\n",
    "                                transform=get_transform(res), lmdb_map_size=1 * 1024**3)\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Hyperparameter Tuning using Progressive Training over All 3 Resolutions\n",
    "    # ------------------------------\n",
    "    def objective(trial):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [4, 6, 8])\n",
    "        lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "        lstm_hidden = trial.suggest_categorical(\"lstm_hidden\", [256, 384])\n",
    "        lstm_layers = 2  # Fixed to 2 layers\n",
    "        dropout_rate = trial.suggest_categorical(\"dropout_rate\", [0.3, 0.4, 0.5])\n",
    "        total_epochs = sum(eps for _, eps in PROG_SCHEDULE)\n",
    "        \n",
    "        current_res = PROG_SCHEDULE[0][0]  # First resolution\n",
    "        current_batch_size = BATCH_SIZES[current_res]\n",
    "        \n",
    "        model = EfficientNetV2L_BiLSTM_CrossAttn_CBAM(\n",
    "            lstm_hidden=lstm_hidden, \n",
    "            lstm_layers=lstm_layers,\n",
    "            dropout_rate=dropout_rate, \n",
    "            classifier_hidden=256\n",
    "        ).to(device)\n",
    "        \n",
    "        trial_checkpoint = MODEL_DIR / f\"trial_eff_v2l_{trial.number}__bilstm_spatialcbam_checkpoint.pth\"\n",
    "        trial_checkpoint.parent.mkdir(parents=True, exist_ok=True)\n",
    "        loss = progressive_train_model(\n",
    "            model, \n",
    "            total_epochs, \n",
    "            lr, \n",
    "            trial_checkpoint, \n",
    "            current_batch_size,\n",
    "            patience=3, \n",
    "            gradient_accum_steps=GRADIENT_ACCUM_STEPS\n",
    "        )\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        return loss\n",
    "    \n",
    "    db_path = BASE_DIR / \"notebooks\" / \"tuning_eff_v2l_bilstm_spatialcbam_optim.db\"\n",
    "    db_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        print(f\"Database created/connected at: {db_path}\")\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"DB Error: {e}\")\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        pruner=MedianPruner(n_startup_trials=2, n_warmup_steps=10),\n",
    "        study_name=\"efficientnetv2l_bilstm_spatialcbam_study\",\n",
    "        storage=f\"sqlite:///{db_path}\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    target_trials = 30\n",
    "    while True:\n",
    "        successes = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE and np.isfinite(t.value)]\n",
    "        remaining = target_trials - len(successes)\n",
    "        if remaining <= 0:\n",
    "            break\n",
    "        print(f\"Running {remaining} additional trial(s) to reach {target_trials} successful trials...\")\n",
    "        study.optimize(objective, n_trials=remaining, catch=(Exception,))\n",
    "    print(f\"Optuna tuning complete. Total successful trials: {len(successes)}\")\n",
    "    best_trial = min(successes, key=lambda t: t.value) if successes else optuna.trial.FrozenTrial(\n",
    "        number=0,\n",
    "        state=optuna.trial.TrialState.COMPLETE,\n",
    "        value=float('inf'),\n",
    "        datetime_start=datetime.datetime.now(),\n",
    "        datetime_complete=datetime.datetime.now(),\n",
    "        params={\n",
    "            \"batch_size\": 8,\n",
    "            \"lr\": 1e-4,\n",
    "            \"lstm_hidden\": 256,\n",
    "            \"dropout_rate\": 0.5\n",
    "        },\n",
    "        distributions={},\n",
    "        user_attrs={},\n",
    "        system_attrs={},\n",
    "        intermediate_values={}\n",
    "    )\n",
    "    print(f\"Best trial parameters: {best_trial.params}\")\n",
    "    \n",
    "    # -----------------------------------------------------\n",
    "    # Final Training\n",
    "    # -----------------------------------------------------\n",
    "    total_epochs = sum(eps for _, eps in PROG_SCHEDULE)\n",
    "    final_checkpoint = MODEL_DIR / \"final_model_eff_v2l_bilstm_spatialcbam_checkpoint.pth\"  # Updated name\n",
    "    if not final_checkpoint.exists():\n",
    "        print(\"\\n--- Starting Final Training ---\")\n",
    "        params = best_trial.params\n",
    "        batch_size = params.get(\"batch_size\", 4)\n",
    "        lr = params.get(\"lr\", 1e-4)\n",
    "        lstm_hidden = params.get(\"lstm_hidden\", 256)\n",
    "        lstm_layers = 2  # Force 2 layers for BiLSTM\n",
    "        dropout_rate = params.get(\"dropout_rate\", 0.5)\n",
    "        final_model = EfficientNetV2L_BiLSTM_CrossAttn_CBAM(\n",
    "            lstm_hidden=lstm_hidden, \n",
    "            lstm_layers=lstm_layers,\n",
    "            dropout_rate=dropout_rate, \n",
    "            classifier_hidden=256\n",
    "        ).to(device)\n",
    "        \n",
    "        # Unfreeze backbone layers for fine-tuning\n",
    "        if hasattr(final_model.backbone, \"blocks\"):\n",
    "            for i, block in enumerate(final_model.backbone.blocks):\n",
    "                if i >= 6:\n",
    "                    for param in block.parameters():\n",
    "                        param.requires_grad = True\n",
    "                        \n",
    "        final_checkpoint.parent.mkdir(parents=True, exist_ok=True)\n",
    "        final_loss = progressive_train_model(\n",
    "            final_model, \n",
    "            total_epochs, \n",
    "            lr, \n",
    "            final_checkpoint, \n",
    "            batch_size,\n",
    "            patience=5, \n",
    "            gradient_accum_steps=GRADIENT_ACCUM_STEPS\n",
    "        )\n",
    "    \n",
    "    # -----------------------------------------------------\n",
    "    # Evaluation on Test Set \n",
    "    # -----------------------------------------------------\n",
    "    test_transform = get_transform(300)  # Highest resolution\n",
    "    test_set = VideoDatasetRaw(test_csv, FRAMES_DIR, num_frames=NUM_FRAMES, transform=test_transform)\n",
    "    test_loader = DataLoader(test_set, batch_size=4, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    eval_model = EfficientNetV2L_BiLSTM_CrossAttn_CBAM(\n",
    "        lstm_hidden=best_trial.params.get(\"lstm_hidden\", 256),\n",
    "        lstm_layers=2,  # Force 2-layer BiLSTM\n",
    "        dropout_rate=best_trial.params.get(\"dropout_rate\", 0.5),\n",
    "        classifier_hidden=256\n",
    "    ).to(device)\n",
    "\n",
    "    state = torch.load(final_checkpoint, map_location=device)\n",
    "    eval_model.load_state_dict(state[\"model_state_dict\"])\n",
    "    eval_model.to(device)\n",
    "    evaluate_model(eval_model, test_loader)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"\\n--- Evaluation Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
