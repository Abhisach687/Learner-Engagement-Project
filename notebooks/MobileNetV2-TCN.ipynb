{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries, set up paths, and device\n",
    "\n",
    "This cell imports necessary libraries and defines file paths and device settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import optuna\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm \n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torch.amp import GradScaler, autocast\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define paths\n",
    "BASE_DIR = Path(\"C:/Users/abhis/Downloads/Documents/Learner Engagement Project\")\n",
    "DATA_DIR = BASE_DIR / \"data\" / \"DAiSEE\"\n",
    "FRAMES_DIR = DATA_DIR / \"ExtractedFrames\"\n",
    "LABELS_DIR = DATA_DIR / \"Labels\"\n",
    "MODEL_DIR = BASE_DIR / \"models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Precomputed directory for caching best frames\n",
    "CACHE_DIR = BASE_DIR / \"cache\"\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Set device and CUDA configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Define data transforms for training and validation.\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define helper function for selecting 30 best frames using face detection and sharpness\n",
    "\n",
    "This cell defines a function that, given a folder of extracted frames, divides them into 30 equal temporal segments and selects the best frame from each segment based on face detection and sharpness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping CSV clip IDs.\n",
    "def get_csv_clip_id(video_stem: str) -> str:\n",
    "    base = video_stem.strip()\n",
    "    if base.startswith(\"110001\"):\n",
    "        base = base.replace(\"110001\", \"202614\", 1)\n",
    "    return base\n",
    "\n",
    "# Select best frames using face detection and Laplacian variance.\n",
    "def select_impactful_frames(video_folder: Path, num_frames=30):\n",
    "    frame_files = sorted(video_folder.glob(\"frame_*.jpg\"))\n",
    "    total_frames = len(frame_files)\n",
    "    if total_frames == 0:\n",
    "        return []\n",
    "    if total_frames <= num_frames:\n",
    "        return frame_files\n",
    "    segment_size = total_frames // num_frames\n",
    "    cascade_path = cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    "    face_cascade = cv2.CascadeClassifier(cascade_path)\n",
    "    selected_frames = []\n",
    "    for i in range(num_frames):\n",
    "        start_idx = i * segment_size\n",
    "        end_idx = (i + 1) * segment_size if i < num_frames - 1 else total_frames\n",
    "        best_score = -1\n",
    "        best_frame = None\n",
    "        for fp in frame_files[start_idx:end_idx]:\n",
    "            img = cv2.imread(str(fp))\n",
    "            if img is None:\n",
    "                continue\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "            if len(faces) > 0:\n",
    "                face = max(faces, key=lambda r: r[2]*r[3])\n",
    "                x, y, w, h = face\n",
    "                region = gray[y:y+h, x:x+w]\n",
    "                quality = cv2.Laplacian(region, cv2.CV_64F).var()\n",
    "            else:\n",
    "                quality = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "            if quality > best_score:\n",
    "                best_score = quality\n",
    "                best_frame = fp\n",
    "        if best_frame is not None:\n",
    "            selected_frames.append(best_frame)\n",
    "    return selected_frames\n",
    "\n",
    "def precompute_best_frames(csv_file: Path, video_root: Path, num_frames=30):\n",
    "    \"\"\"\n",
    "    Precompute and cache the best frame paths for each video in the CSV.\n",
    "    The results are saved to a pickle file and returned.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(csv_file, dtype=str)\n",
    "    data.columns = data.columns.str.strip()\n",
    "    split = Path(csv_file).stem.replace(\"Labels\", \"\").strip()\n",
    "    precomputed = []  # list of lists for each video\n",
    "    valid_indices = []  # valid data indices\n",
    "    skipped_count = 0\n",
    "\n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data), desc=\"Precomputing best frames\", dynamic_ncols=True):\n",
    "        clip_id = str(row[\"ClipID\"]).strip()\n",
    "        if clip_id.endswith(('.avi', '.mp4')):\n",
    "            clip_id = clip_id.rsplit('.', 1)[0]\n",
    "        mapped_id = get_csv_clip_id(clip_id)\n",
    "        video_folder = video_root / split / mapped_id\n",
    "        if video_folder.exists():\n",
    "            frame_files = sorted(video_folder.glob(\"frame_*.jpg\"))\n",
    "            if len(frame_files) >= num_frames:\n",
    "                selected_frames = select_impactful_frames(video_folder, num_frames)\n",
    "                precomputed.append(selected_frames)\n",
    "                valid_indices.append(idx)\n",
    "            else:\n",
    "                skipped_count += 1\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "    print(f\"Precomputation: Skipped {skipped_count} videos out of {len(data)}.\")\n",
    "    cache_data = {\"valid_indices\": valid_indices, \"precomputed_frames\": precomputed}\n",
    "    cache_file = CACHE_DIR / f\"precomputed_{Path(csv_file).stem}_frame_{num_frames}.pkl\"\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(cache_data, f)\n",
    "    print(f\"Precomputed results saved to {cache_file}\")\n",
    "    return cache_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the custom Dataset for video classification\n",
    "\n",
    "This cell defines a PyTorch Dataset that reads a CSV file with video IDs and 4 labels. For each video, it loads the 30 best frames using the above function, applies transforms, and returns a tensor and its label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, video_root, transform=None, num_frames=30):\n",
    "        self.csv_file = Path(csv_file)\n",
    "        self.data = pd.read_csv(self.csv_file, dtype=str)\n",
    "        self.data.columns = self.data.columns.str.strip()\n",
    "        self.video_root = Path(video_root)\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "        self.split = self.csv_file.stem.replace(\"Labels\", \"\").strip()\n",
    "        cache_file = CACHE_DIR / f\"precomputed_{Path(csv_file).stem}_frame_{num_frames}.pkl\"\n",
    "        if cache_file.exists():\n",
    "            with open(cache_file, \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            valid_indices = cache_data[\"valid_indices\"]\n",
    "            self.precomputed_frames = cache_data[\"precomputed_frames\"]\n",
    "            self.data = self.data.iloc[valid_indices].reset_index(drop=True)\n",
    "            print(f\"Loaded precomputed frames for {len(self.data)} videos from cache.\")\n",
    "        else:\n",
    "            valid_rows = []\n",
    "            self.precomputed_frames = []\n",
    "            skipped_count = 0\n",
    "            for idx, row in self.data.iterrows():\n",
    "                clip_id = str(row[\"ClipID\"]).strip()\n",
    "                if clip_id.endswith(('.avi', '.mp4')):\n",
    "                    clip_id = clip_id.rsplit('.', 1)[0]\n",
    "                mapped_id = get_csv_clip_id(clip_id)\n",
    "                video_folder = self.video_root / self.split / mapped_id\n",
    "                if video_folder.exists():\n",
    "                    frame_files = sorted(video_folder.glob(\"frame_*.jpg\"))\n",
    "                    if len(frame_files) >= num_frames:\n",
    "                        selected_frames = select_impactful_frames(video_folder, num_frames)\n",
    "                        valid_rows.append(row)\n",
    "                        self.precomputed_frames.append(selected_frames)\n",
    "                    else:\n",
    "                        skipped_count += 1\n",
    "                else:\n",
    "                    skipped_count += 1\n",
    "            self.data = pd.DataFrame(valid_rows)\n",
    "            print(f\"Computed frames on the fly: Skipped {skipped_count} videos.\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        selected_frame_paths = self.precomputed_frames[idx]\n",
    "        frames = []\n",
    "        for fp in selected_frame_paths:\n",
    "            try:\n",
    "                # Attempt to load frame using PIL\n",
    "                img = Image.open(fp).convert(\"RGB\")\n",
    "            except (FileNotFoundError, OSError):\n",
    "                # Create a black PIL image placeholder\n",
    "                img = Image.new('RGB', (224, 224))\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            frames.append(img)\n",
    "        \n",
    "        # Ensure exactly num_frames by adding placeholder images if needed\n",
    "        while len(frames) < self.num_frames:\n",
    "            placeholder_img = Image.new('RGB', (224, 224))\n",
    "            if self.transform:\n",
    "                placeholder_img = self.transform(placeholder_img)\n",
    "            frames.append(placeholder_img)\n",
    "        \n",
    "        frames_tensor = torch.stack(frames)\n",
    "        labels = torch.tensor([\n",
    "            int(row[\"Engagement\"]),\n",
    "            int(row[\"Boredom\"]),\n",
    "            int(row[\"Confusion\"]),\n",
    "            int(row[\"Frustration\"])\n",
    "        ], dtype=torch.long)\n",
    "        return frames_tensor, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the MobileNetV2-TCN model\n",
    "\n",
    "This cell defines the MobileNetV2-TCN model. It processes a sequence of frames by applying MobileNetV2 on each frame, stacking the features, and feeding them to a temporal convolution network (TCN).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetTCN(nn.Module):\n",
    "    def __init__(self, hidden_ch=512, freeze_block=0):\n",
    "        super(MobileNetTCN, self).__init__()\n",
    "        self.mobilenet = mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "        self.freeze_blocks(freeze_block)\n",
    "        self.mobilenet.classifier = nn.Identity()\n",
    "        \n",
    "        # Adjust TCN layers using hyperparameter hidden_ch.\n",
    "        self.tcn = nn.Sequential(\n",
    "            nn.Conv1d(1280, hidden_ch, kernel_size=3, dilation=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_ch, 16, kernel_size=1)\n",
    "        )\n",
    "    \n",
    "    def freeze_blocks(self, freeze_block):\n",
    "        # Freeze the first 'freeze_block' blocks in MobileNetV2 features.\n",
    "        if freeze_block > 0:\n",
    "            for i in range(freeze_block):\n",
    "                if i < len(self.mobilenet.features):\n",
    "                    for param in self.mobilenet.features[i].parameters():\n",
    "                        param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, num_frames, C, H, W = x.size()\n",
    "        x_reshaped = x.view(-1, C, H, W)\n",
    "        features_reshaped = self.mobilenet(x_reshaped)\n",
    "        features = features_reshaped.view(batch_size, num_frames, -1).permute(0, 2, 1)\n",
    "        out = self.tcn(features)\n",
    "        out = out[:, :, -1]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training, checkpointing, and evaluation functions\n",
    "\n",
    "This cell defines the training loop which uses mixed precision (`torch.cuda.amp`), shows progress with `tqdm`, and saves checkpoints to resume training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, best_val_loss, checkpoint_path):\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(state, checkpoint_path)\n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path):\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        state = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(state[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
    "        return state[\"epoch\"], state[\"best_val_loss\"]\n",
    "    return 0, float(\"inf\")\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, lr, checkpoint_path, patience=5, gradient_accum_steps=1):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler = GradScaler()\n",
    "    start_epoch, best_val_loss = load_checkpoint(model, optimizer, checkpoint_path)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    # Outer tqdm for epochs\n",
    "    epoch_pbar = tqdm(\n",
    "        range(start_epoch, epochs),\n",
    "        desc=\"Epochs\",\n",
    "        total=epochs - start_epoch,\n",
    "        position=0,\n",
    "        leave=True,\n",
    "        dynamic_ncols=True\n",
    "    )\n",
    "\n",
    "    for epoch in epoch_pbar:\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Inner tqdm for batch-level progress within each epoch\n",
    "        train_iter = tqdm(\n",
    "            enumerate(train_loader),\n",
    "            total=len(train_loader),\n",
    "            desc=f\"Epoch {epoch+1} [Train]\",\n",
    "            position=1,\n",
    "            leave=False,\n",
    "            dynamic_ncols=True\n",
    "        )\n",
    "\n",
    "        for i, (frames, labels) in train_iter:\n",
    "            frames, labels = frames.to(device), labels.to(device)\n",
    "\n",
    "            with autocast(enabled=True, dtype=torch.float16, device_type='cuda'):\n",
    "                outputs = model(frames)\n",
    "                # outputs_reshaped: [batch_size, 4 (dimensions), 4 (classes each dimension)]\n",
    "                outputs_reshaped = outputs.view(outputs.size(0), 4, 4)\n",
    "                # Summation of cross-entropy across the 4 dimensions\n",
    "                loss = sum(loss_fn(outputs_reshaped[:, d], labels[:, d]) for d in range(4)) / 4.0\n",
    "\n",
    "            # Gradient scaling / accumulation\n",
    "            scaler.scale(loss / gradient_accum_steps).backward()\n",
    "            if (i + 1) % gradient_accum_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item() * frames.size(0)\n",
    "\n",
    "        # Compute average training loss over the entire train set\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for frames, labels in val_loader:\n",
    "                frames, labels = frames.to(device), labels.to(device)\n",
    "                with autocast(enabled=True, dtype=torch.float16, device_type='cuda'):\n",
    "                    outputs = model(frames)\n",
    "                    outputs_reshaped = outputs.view(outputs.size(0), 4, 4)\n",
    "                    loss = sum(loss_fn(outputs_reshaped[:, d], labels[:, d]) for d in range(4)) / 4.0\n",
    "                    val_loss += loss.item() * frames.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        # Update the epoch-level progress bar with train & val losses\n",
    "        epoch_pbar.set_postfix({\"train_loss\": f\"{train_loss:.4f}\", \"val_loss\": f\"{val_loss:.4f}\"})\n",
    "\n",
    "        # Print to console as well if desired\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_checkpoint(model, optimizer, epoch + 1, best_val_loss, checkpoint_path)\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. Best loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Optuna objective for hyperparameter tuning (using SQLite storage)\n",
    "\n",
    "This cell defines an Optuna objective that trains the MobileNetV2-TCN model for a few epochs using hyperparameters suggested by the trial. The study is configured to use an SQLite database (`tuning.db`) for saving progress so tuning can be resumed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    num_frames = trial.suggest_categorical(\"num_frames\", [30])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 5e-4, log=True)\n",
    "    epochs = trial.suggest_int(\"epochs\", 3, 5)\n",
    "    hidden_ch = trial.suggest_categorical(\"hidden_ch\", [64, 128, 256])\n",
    "    freeze_block = trial.suggest_int(\"freeze_block\", 0, 4)\n",
    "\n",
    "    from torch.utils.data import DataLoader\n",
    "    train_dataset = VideoDataset(LABELS_DIR / \"TrainLabels.csv\", FRAMES_DIR, transform=train_transform, num_frames=num_frames)\n",
    "    val_dataset = VideoDataset(LABELS_DIR / \"ValidationLabels.csv\", FRAMES_DIR, transform=val_transform, num_frames=num_frames)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    model = MobileNetTCN(hidden_ch=hidden_ch, freeze_block=freeze_block)\n",
    "    trial_checkpoint = MODEL_DIR / f\"checkpoint_trial_{trial.number}.pth\"\n",
    "\n",
    "    try:\n",
    "        best_val_loss = train_model(model, train_loader, val_loader, epochs, lr, trial_checkpoint, patience=3)\n",
    "        return best_val_loss\n",
    "    except Exception as e:\n",
    "        print(f\"Trial {trial.number} failed: {e}\")\n",
    "        return float(\"inf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate and visualize results\n",
    "\n",
    "This cell evaluates the final model on the test set and prints a classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for frames, labels in tqdm(test_loader, desc=\"Evaluating\", dynamic_ncols=True):\n",
    "            frames = frames.to(device)\n",
    "            outputs = model(frames)\n",
    "            outputs_reshaped = outputs.view(outputs.size(0), 4, 4)\n",
    "            preds = torch.argmax(outputs_reshaped, dim=2)\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels)\n",
    "    all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "    \n",
    "    dims = [\"Engagement\", \"Boredom\", \"Confusion\", \"Frustration\"]\n",
    "    for i, dim in enumerate(dims):\n",
    "        print(f\"Classification report for {dim}:\")\n",
    "        print(classification_report(all_labels[:, i], all_preds[:, i], digits=3))\n",
    "        \n",
    "        cm = confusion_matrix(all_labels[:, i], all_preds[:, i])\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Confusion Matrix for {dim}\")\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(cm.shape[0])\n",
    "        plt.xticks(tick_marks, tick_marks)\n",
    "        plt.yticks(tick_marks, tick_marks)\n",
    "        plt.xlabel(\"Predicted label\")\n",
    "        plt.ylabel(\"True label\")\n",
    "        thresh = cm.max() / 2.\n",
    "        for j in range(cm.shape[0]):\n",
    "            for k in range(cm.shape[1]):\n",
    "                plt.text(k, j, format(cm[j, k], 'd'),\n",
    "                         horizontalalignment=\"center\",\n",
    "                         color=\"white\" if cm[j, k] > thresh else \"black\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\AppData\\Local\\Temp\\ipykernel_27868\\2305146823.py:21: ExperimentalWarning: RetryFailedTrialCallback is experimental (supported from v2.8.0). The interface can change in the future.\n",
      "  failed_trial_callback=optuna.storages.RetryFailedTrialCallback(max_retry=6)\n",
      "[I 2025-02-17 22:35:06,097] Using an existing study with name 'mobilev2_tcn_study' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna hyperparameter tuning...\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "'float' is not among the defined enum values. Enum name: trialvaluetype. Possible values: FINITE, INF_POS, INF_NEG",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sqlalchemy\\sql\\sqltypes.py:1654\u001b[0m, in \u001b[0;36mEnum._object_value_for_elem\u001b[1;34m(self, elem)\u001b[0m\n\u001b[0;32m   1653\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1654\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object_lookup\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'float'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Optuna hyperparameter tuning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Count how many trials are already done (completed/failed/pruned)\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m completed_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m([t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrials\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;129;01min\u001b[39;00m {optuna\u001b[38;5;241m.\u001b[39mtrial\u001b[38;5;241m.\u001b[39mTrialState\u001b[38;5;241m.\u001b[39mCOMPLETE, optuna\u001b[38;5;241m.\u001b[39mtrial\u001b[38;5;241m.\u001b[39mTrialState\u001b[38;5;241m.\u001b[39mFAIL, optuna\u001b[38;5;241m.\u001b[39mtrial\u001b[38;5;241m.\u001b[39mTrialState\u001b[38;5;241m.\u001b[39mPRUNED}])\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Initialize the progress bar with 'initial' = already completed\u001b[39;00m\n\u001b[0;32m     36\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39mn_trials, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptuna Trials\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrial\u001b[39m\u001b[38;5;124m\"\u001b[39m, dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, initial\u001b[38;5;241m=\u001b[39mcompleted_trials)\n",
      "File \u001b[1;32mc:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\optuna\\study\\study.py:247\u001b[0m, in \u001b[0;36mStudy.trials\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrials\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[FrozenTrial]:\n\u001b[0;32m    233\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return all trials in the study.\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m    The returned trials are ordered by trial number.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    244\u001b[0m \n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_trials\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\optuna\\study\\study.py:289\u001b[0m, in \u001b[0;36mStudy.get_trials\u001b[1;34m(self, deepcopy, states)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_trials\u001b[39m(\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    251\u001b[0m     deepcopy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    252\u001b[0m     states: Container[TrialState] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    253\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[FrozenTrial]:\n\u001b[0;32m    254\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return all trials in the study.\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03m    The returned trials are ordered by trial number.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;124;03m        A list of :class:`~optuna.trial.FrozenTrial` objects.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_trials\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\optuna\\study\\study.py:309\u001b[0m, in \u001b[0;36mStudy._get_trials\u001b[1;34m(self, deepcopy, states, use_cache)\u001b[0m\n\u001b[0;32m    306\u001b[0m         filtered_trials \u001b[38;5;241m=\u001b[39m trials\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m copy\u001b[38;5;241m.\u001b[39mdeepcopy(filtered_trials) \u001b[38;5;28;01mif\u001b[39;00m deepcopy \u001b[38;5;28;01melse\u001b[39;00m filtered_trials\n\u001b[1;32m--> 309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_all_trials\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_study_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\optuna\\storages\\_cached_storage.py:221\u001b[0m, in \u001b[0;36m_CachedStorage.get_all_trials\u001b[1;34m(self, study_id, deepcopy, states)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_all_trials\u001b[39m(\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    217\u001b[0m     study_id: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m    218\u001b[0m     deepcopy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m     states: Container[TrialState] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    220\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[FrozenTrial]:\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_trials_from_remote_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    224\u001b[0m         study \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_studies[study_id]\n",
      "File \u001b[1;32mc:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\optuna\\storages\\_cached_storage.py:242\u001b[0m, in \u001b[0;36m_CachedStorage._read_trials_from_remote_storage\u001b[1;34m(self, study_id)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_studies[study_id] \u001b[38;5;241m=\u001b[39m _StudyInfo()\n\u001b[0;32m    241\u001b[0m study \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_studies[study_id]\n\u001b[1;32m--> 242\u001b[0m trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_trials\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudy_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mincluded_trial_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munfinished_trial_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial_id_greater_than\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_finished_trial_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trials:\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\optuna\\storages\\_rdb\\storage.py:852\u001b[0m, in \u001b[0;36mRDBStorage._get_trials\u001b[1;34m(self, study_id, states, included_trial_ids, trial_id_greater_than)\u001b[0m\n\u001b[0;32m    850\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    851\u001b[0m         _query \u001b[38;5;241m=\u001b[39m query\n\u001b[1;32m--> 852\u001b[0m     trial_models \u001b[38;5;241m=\u001b[39m \u001b[43m_query\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morder_by\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrialModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrial_id\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sqlalchemy_exc\u001b[38;5;241m.\u001b[39mOperationalError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;66;03m# Likely exceeding the number of maximum allowed variables using IN.\u001b[39;00m\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# This number differ between database dialects. For SQLite for instance, see\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# https://www.sqlite.org/limits.html and the section describing\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;66;03m# SQLITE_MAX_VARIABLE_NUMBER.\u001b[39;00m\n\u001b[0;32m    859\u001b[0m     _logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    860\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCaught an error from sqlalchemy: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Falling back to a slower alternative. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    861\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(e))\n\u001b[0;32m    862\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sqlalchemy\\orm\\query.py:2699\u001b[0m, in \u001b[0;36mQuery.all\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mall\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[_T]:\n\u001b[0;32m   2678\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the results represented by this :class:`_query.Query`\u001b[39;00m\n\u001b[0;32m   2679\u001b[0m \u001b[38;5;124;03m    as a list.\u001b[39;00m\n\u001b[0;32m   2680\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2697\u001b[0m \u001b[38;5;124;03m        :meth:`_engine.Result.scalars` - v2 comparable method.\u001b[39;00m\n\u001b[0;32m   2698\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\result.py:1767\u001b[0m, in \u001b[0;36mScalarResult.all\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mall\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence[_R]:\n\u001b[0;32m   1760\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return all scalar values in a sequence.\u001b[39;00m\n\u001b[0;32m   1761\u001b[0m \n\u001b[0;32m   1762\u001b[0m \u001b[38;5;124;03m    Equivalent to :meth:`_engine.Result.all` except that\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1765\u001b[0m \n\u001b[0;32m   1766\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_allrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\result.py:548\u001b[0m, in \u001b[0;36mResultInternal._allrows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    544\u001b[0m post_creational_filter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_creational_filter\n\u001b[0;32m    546\u001b[0m make_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_row_getter\n\u001b[1;32m--> 548\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetchall_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    549\u001b[0m made_rows: List[_InterimRowType[_R]]\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m make_row:\n",
      "File \u001b[1;32mc:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\result.py:1674\u001b[0m, in \u001b[0;36mFilterResult._fetchall_impl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fetchall_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[_InterimRowType[Row[Any]]]:\n\u001b[1;32m-> 1674\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_real_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetchall_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\result.py:2268\u001b[0m, in \u001b[0;36mIteratorResult._fetchall_impl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_hard_closed()\n\u001b[0;32m   2267\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2269\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_soft_close()\n",
      "File \u001b[1;32mc:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sqlalchemy\\orm\\loading.py:246\u001b[0m, in \u001b[0;36minstances.<locals>.chunks\u001b[1;34m(size)\u001b[0m\n\u001b[0;32m    244\u001b[0m     context\u001b[38;5;241m.\u001b[39mpost_load_paths\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m path, post_load \u001b[38;5;129;01min\u001b[39;00m post_loads:\n\u001b[1;32m--> 246\u001b[0m         \u001b[43mpost_load\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m yield_per:\n\u001b[0;32m    249\u001b[0m     context\u001b[38;5;241m.\u001b[39mpost_load_paths\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[1;32mc:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sqlalchemy\\orm\\loading.py:1560\u001b[0m, in \u001b[0;36mPostLoad.invoke\u001b[1;34m(self, context, path)\u001b[0m\n\u001b[0;32m   1554\u001b[0m     states \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1555\u001b[0m         (state, overwrite)\n\u001b[0;32m   1556\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m state, overwrite \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1557\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mmanager\u001b[38;5;241m.\u001b[39mmapper\u001b[38;5;241m.\u001b[39misa(limit_to_mapper)\n\u001b[0;32m   1558\u001b[0m     ]\n\u001b[0;32m   1559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m states:\n\u001b[1;32m-> 1560\u001b[0m         \u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1561\u001b[0m \u001b[43m            \u001b[49m\u001b[43meffective_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\n\u001b[0;32m   1562\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1563\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[1;32mc:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sqlalchemy\\orm\\strategies.py:3341\u001b[0m, in \u001b[0;36mSelectInLoader._load_for_path\u001b[1;34m(self, context, path, states, load_only, effective_entity, loadopt, recursion_depth, execution_options)\u001b[0m\n\u001b[0;32m   3332\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_via_child(\n\u001b[0;32m   3333\u001b[0m         our_states,\n\u001b[0;32m   3334\u001b[0m         none_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3338\u001b[0m         execution_options,\n\u001b[0;32m   3339\u001b[0m     )\n\u001b[0;32m   3340\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3341\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_via_parent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mour_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\n\u001b[0;32m   3343\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sqlalchemy\\orm\\strategies.py:3416\u001b[0m, in \u001b[0;36mSelectInLoader._load_via_parent\u001b[1;34m(self, our_states, query_info, q, context, execution_options)\u001b[0m\n\u001b[0;32m   3410\u001b[0m primary_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3411\u001b[0m     key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m query_info\u001b[38;5;241m.\u001b[39mzero_idx \u001b[38;5;28;01melse\u001b[39;00m key\n\u001b[0;32m   3412\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, state, state_dict, overwrite \u001b[38;5;129;01min\u001b[39;00m chunk\n\u001b[0;32m   3413\u001b[0m ]\n\u001b[0;32m   3415\u001b[0m data \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m-> 3416\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprimary_keys\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimary_keys\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3421\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3422\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3423\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3426\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, state, state_dict, overwrite \u001b[38;5;129;01min\u001b[39;00m chunk:\n",
      "File \u001b[1;32mc:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\result.py:512\u001b[0m, in \u001b[0;36mResultInternal._iterator_getter.<locals>.iterrows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21miterrows\u001b[39m(\u001b[38;5;28mself\u001b[39m: Result[Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[_R]:\n\u001b[1;32m--> 512\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_row\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetchiter_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_InterimRowType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mAny\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmake_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_row\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmake_row\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_row\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhashed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sqlalchemy\\orm\\loading.py:219\u001b[0m, in \u001b[0;36minstances.<locals>.chunks\u001b[1;34m(size)\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 219\u001b[0m     fetch \u001b[38;5;241m=\u001b[39m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_all_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m single_entity:\n\u001b[0;32m    222\u001b[0m     proc \u001b[38;5;241m=\u001b[39m process[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\result.py:541\u001b[0m, in \u001b[0;36mResultInternal._raw_all_rows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m make_row \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    540\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetchall_impl()\n\u001b[1;32m--> 541\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mmake_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows]\n",
      "File \u001b[1;32mlib\\\\sqlalchemy\\\\cyextension\\\\resultproxy.pyx:22\u001b[0m, in \u001b[0;36msqlalchemy.cyextension.resultproxy.BaseRow.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mlib\\\\sqlalchemy\\\\cyextension\\\\resultproxy.pyx:79\u001b[0m, in \u001b[0;36msqlalchemy.cyextension.resultproxy._apply_processors\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sqlalchemy\\sql\\sqltypes.py:1774\u001b[0m, in \u001b[0;36mEnum.result_processor.<locals>.process\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m   1771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parent_processor:\n\u001b[0;32m   1772\u001b[0m     value \u001b[38;5;241m=\u001b[39m parent_processor(value)\n\u001b[1;32m-> 1774\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object_value_for_elem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sqlalchemy\\sql\\sqltypes.py:1656\u001b[0m, in \u001b[0;36mEnum._object_value_for_elem\u001b[1;34m(self, elem)\u001b[0m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object_lookup[elem]\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 1656\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(\n\u001b[0;32m   1657\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not among the defined enum values. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1658\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnum name: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Possible values: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1659\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m   1660\u001b[0m             elem,\n\u001b[0;32m   1661\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m   1662\u001b[0m             langhelpers\u001b[38;5;241m.\u001b[39mrepr_tuple_names(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menums),\n\u001b[0;32m   1663\u001b[0m         )\n\u001b[0;32m   1664\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[1;31mLookupError\u001b[0m: 'float' is not among the defined enum values. Enum name: trialvaluetype. Possible values: FINITE, INF_POS, INF_NEG"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Step 1: (Optional) Precompute and cache best frames\n",
    "    train_csv = LABELS_DIR / \"TrainLabels.csv\"\n",
    "    val_csv = LABELS_DIR / \"ValidationLabels.csv\"\n",
    "    \n",
    "    cache_file_train = CACHE_DIR / f\"precomputed_{Path(train_csv).stem}_frame_30.pkl\"\n",
    "    if not cache_file_train.exists():\n",
    "        print(\"Precomputing best frames for training data...\")\n",
    "        precompute_best_frames(train_csv, FRAMES_DIR, num_frames=30)\n",
    "    \n",
    "    cache_file_val = CACHE_DIR / f\"precomputed_{Path(val_csv).stem}_frame_30.pkl\"      \n",
    "    if not cache_file_val.exists():\n",
    "        print(\"Precomputing best frames for validation data...\")      \n",
    "        precompute_best_frames(val_csv, FRAMES_DIR, num_frames=30)\n",
    "    \n",
    "    # ----------------------\n",
    "    # Step 2: Run Optuna tuning with early stopping\n",
    "    # ----------------------\n",
    "    storage = optuna.storages.RDBStorage(\n",
    "        url=\"sqlite:///tuning.db\",\n",
    "        failed_trial_callback=optuna.storages.RetryFailedTrialCallback(max_retry=6)\n",
    "    )\n",
    "    n_trials = 10\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        study_name=\"mobilev2_tcn_study\",\n",
    "        storage=storage,\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    print(\"Starting Optuna hyperparameter tuning...\")\n",
    "\n",
    "    # Count how many trials are already done (completed/failed/pruned)\n",
    "    completed_trials = len([t for t in study.trials if t.state in {optuna.trial.TrialState.COMPLETE, optuna.trial.TrialState.FAIL, optuna.trial.TrialState.PRUNED}])\n",
    "\n",
    "    # Initialize the progress bar with 'initial' = already completed\n",
    "    pbar = tqdm(total=n_trials, desc=\"Optuna Trials\", unit=\"trial\", dynamic_ncols=True, initial=completed_trials)\n",
    "\n",
    "    def update(study, trial):\n",
    "        pbar.update()\n",
    "\n",
    "    study.optimize(objective, n_trials=n_trials, catch=(Exception,), callbacks=[update])\n",
    "    pbar.close()\n",
    "    print(f\"Optuna tuning complete.\\nBest trial: {study.best_trial}\")\n",
    "    \n",
    "    # ----------------------\n",
    "    # Step 3: Final training with best hyperparameters and early stopping.\n",
    "    # ----------------------\n",
    "    best_trial = study.best_trial\n",
    "    num_frames = best_trial.params[\"num_frames\"]\n",
    "    batch_size = best_trial.params[\"batch_size\"]\n",
    "    lr = best_trial.params[\"lr\"]\n",
    "    epochs = best_trial.params[\"epochs\"]\n",
    "    hidden_ch = best_trial.params[\"hidden_ch\"]\n",
    "    freeze_block = best_trial.params[\"freeze_block\"]\n",
    "    \n",
    "    # Use num_workers=0 to avoid Windows spawn delays.\n",
    "    train_dataset = VideoDataset(train_csv, FRAMES_DIR, transform=train_transform, num_frames=num_frames)\n",
    "    val_dataset = VideoDataset(val_csv, FRAMES_DIR, transform=val_transform, num_frames=num_frames)\n",
    "    \n",
    "    from torch.utils.data import DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    final_model = MobileNetTCN(hidden_ch=hidden_ch, freeze_block=freeze_block)\n",
    "    final_checkpoint = MODEL_DIR / \"final_model_checkpoint.pth\"\n",
    "    patience = 5  # Set patience for final training\n",
    "    print(\"Starting final training with best hyperparameters...\")\n",
    "    train_model(final_model, train_loader, val_loader, epochs, lr, final_checkpoint, patience=patience)\n",
    "    \n",
    "    # ----------------------\n",
    "    # Step 4: Evaluate final model on test data.\n",
    "    # ----------------------\n",
    "    test_csv = LABELS_DIR / \"TestLabels.csv\"  # Update if needed\n",
    "    test_dataset = VideoDataset(test_csv, FRAMES_DIR, transform=val_transform, num_frames=num_frames)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    print(\"Evaluating final model...\")\n",
    "    evaluate_model(final_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
