{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries, set up paths, and device\n",
    "\n",
    "This cell imports necessary libraries and defines file paths and device settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import optuna\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torch.amp import GradScaler, autocast\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define paths\n",
    "BASE_DIR = Path(\"C:/Users/abhis/Downloads/Documents/Learner Engagement Project\")\n",
    "DATA_DIR = BASE_DIR / \"data\" / \"DAiSEE\"\n",
    "FRAMES_DIR = DATA_DIR / \"ExtractedFrames\"\n",
    "LABELS_DIR = DATA_DIR / \"Labels\"\n",
    "MODEL_DIR = BASE_DIR / \"models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Precomputed directory for caching best frames\n",
    "CACHE_DIR = BASE_DIR / \"cache\"\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Set device and CUDA configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"PYTORCH_NO_CUDA_MEMORY_CACHING\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Define data transforms for training and validation.\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define helper function for selecting 30 best frames using face detection and sharpness\n",
    "\n",
    "This cell defines a function that, given a folder of extracted frames, divides them into 30 equal temporal segments and selects the best frame from each segment based on face detection and sharpness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping CSV clip IDs.\n",
    "def get_csv_clip_id(video_stem: str) -> str:\n",
    "    base = video_stem.strip()\n",
    "    if base.startswith(\"110001\"):\n",
    "        base = base.replace(\"110001\", \"202614\", 1)\n",
    "    return base\n",
    "\n",
    "# Select best frames using face detection and Laplacian variance.\n",
    "def select_impactful_frames(video_folder: Path, num_frames=30):\n",
    "    frame_files = sorted(video_folder.glob(\"frame_*.jpg\"))\n",
    "    total_frames = len(frame_files)\n",
    "    if total_frames == 0:\n",
    "        return []\n",
    "    if total_frames <= num_frames:\n",
    "        return frame_files\n",
    "    segment_size = total_frames // num_frames\n",
    "    cascade_path = cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    "    face_cascade = cv2.CascadeClassifier(cascade_path)\n",
    "    selected_frames = []\n",
    "    for i in range(num_frames):\n",
    "        start_idx = i * segment_size\n",
    "        end_idx = (i + 1) * segment_size if i < num_frames - 1 else total_frames\n",
    "        best_score = -1\n",
    "        best_frame = None\n",
    "        for fp in frame_files[start_idx:end_idx]:\n",
    "            img = cv2.imread(str(fp))\n",
    "            if img is None:\n",
    "                continue\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "            if len(faces) > 0:\n",
    "                face = max(faces, key=lambda r: r[2]*r[3])\n",
    "                x, y, w, h = face\n",
    "                region = gray[y:y+h, x:x+w]\n",
    "                quality = cv2.Laplacian(region, cv2.CV_64F).var()\n",
    "            else:\n",
    "                quality = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "            if quality > best_score:\n",
    "                best_score = quality\n",
    "                best_frame = fp\n",
    "        if best_frame is not None:\n",
    "            selected_frames.append(best_frame)\n",
    "    return selected_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_best_frames(csv_file: Path, video_root: Path, num_frames=30, transform=None):\n",
    "    \"\"\"\n",
    "    Precompute and cache the best frame paths for each video in the CSV.\n",
    "    The results are saved to a pickle file and returned.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(csv_file, dtype=str)\n",
    "    data.columns = data.columns.str.strip()\n",
    "    split = Path(csv_file).stem.replace(\"Labels\", \"\").strip()\n",
    "    precomputed = []  # list of lists for each video\n",
    "    valid_indices = []  # valid data indices\n",
    "    skipped_count = 0\n",
    "\n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data), desc=\"Precomputing best frames\", dynamic_ncols=True):\n",
    "        clip_id = str(row[\"ClipID\"]).strip()\n",
    "        if clip_id.endswith(('.avi', '.mp4')):\n",
    "            clip_id = clip_id.rsplit('.', 1)[0]\n",
    "        mapped_id = get_csv_clip_id(clip_id)\n",
    "        video_folder = video_root / split / mapped_id\n",
    "        if video_folder.exists():\n",
    "            frame_files = sorted(video_folder.glob(\"frame_*.jpg\"))\n",
    "            if len(frame_files) >= num_frames:\n",
    "                selected_frames = select_impactful_frames(video_folder, num_frames)\n",
    "                precomputed.append(selected_frames)\n",
    "                valid_indices.append(idx)\n",
    "            else:\n",
    "                skipped_count += 1\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "    print(f\"Precomputation: Skipped {skipped_count} videos out of {len(data)}.\")\n",
    "    cache_data = {\"valid_indices\": valid_indices, \"precomputed_frames\": precomputed}\n",
    "    cache_file = CACHE_DIR / f\"precomputed_{Path(csv_file).stem}_frames.pkl\"\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(cache_data, f)\n",
    "    print(f\"Precomputed results saved to {cache_file}\")\n",
    "    return cache_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the custom Dataset for video classification\n",
    "\n",
    "This cell defines a PyTorch Dataset that reads a CSV file with video IDs and 4 labels. For each video, it loads the 30 best frames using the above function, applies transforms, and returns a tensor and its label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, video_root, transform=None, num_frames=30):\n",
    "        self.csv_file = Path(csv_file)\n",
    "        self.data = pd.read_csv(self.csv_file, dtype=str)\n",
    "        self.data.columns = self.data.columns.str.strip()\n",
    "        self.video_root = Path(video_root)\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "        self.split = self.csv_file.stem.replace(\"Labels\", \"\").strip()\n",
    "        cache_file = CACHE_DIR / f\"precomputed_{self.csv_file.stem}_frames.pkl\"\n",
    "        if cache_file.exists():\n",
    "            with open(cache_file, \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            valid_indices = cache_data[\"valid_indices\"]\n",
    "            self.precomputed_frames = cache_data[\"precomputed_frames\"]\n",
    "            self.data = self.data.iloc[valid_indices].reset_index(drop=True)\n",
    "            print(f\"Loaded precomputed frames for {len(self.data)} videos from cache.\")\n",
    "        else:\n",
    "            valid_rows = []\n",
    "            self.precomputed_frames = []\n",
    "            skipped_count = 0\n",
    "            for idx, row in self.data.iterrows():\n",
    "                clip_id = str(row[\"ClipID\"]).strip()\n",
    "                if clip_id.endswith(('.avi', '.mp4')):\n",
    "                    clip_id = clip_id.rsplit('.', 1)[0]\n",
    "                mapped_id = get_csv_clip_id(clip_id)\n",
    "                video_folder = self.video_root / self.split / mapped_id\n",
    "                if video_folder.exists():\n",
    "                    frame_files = sorted(video_folder.glob(\"frame_*.jpg\"))\n",
    "                    if len(frame_files) >= num_frames:\n",
    "                        selected_frames = select_impactful_frames(video_folder, num_frames)\n",
    "                        valid_rows.append(row)\n",
    "                        self.precomputed_frames.append(selected_frames)\n",
    "                    else:\n",
    "                        skipped_count += 1\n",
    "                else:\n",
    "                    skipped_count += 1\n",
    "            self.data = pd.DataFrame(valid_rows)\n",
    "            print(f\"Computed frames on the fly: Skipped {skipped_count} videos.\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        selected_frame_paths = self.precomputed_frames[idx]\n",
    "        frames = []\n",
    "        for fp in selected_frame_paths:\n",
    "            img = Image.open(fp).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            frames.append(img)\n",
    "        frames_tensor = torch.stack(frames)\n",
    "        labels = torch.tensor([\n",
    "            int(row[\"Engagement\"]),\n",
    "            int(row[\"Boredom\"]),\n",
    "            int(row[\"Confusion\"]),\n",
    "            int(row[\"Frustration\"])\n",
    "        ], dtype=torch.long)\n",
    "        return frames_tensor, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the MobileNetV2-TCN model\n",
    "\n",
    "This cell defines the MobileNetV2-TCN model. It processes a sequence of frames by applying MobileNetV2 on each frame, stacking the features, and feeding them to a temporal convolution network (TCN).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetTCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MobileNetTCN, self).__init__()\n",
    "        self.mobilenet = mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "        self.mobilenet.classifier = nn.Identity()\n",
    "        self.tcn = nn.Sequential(\n",
    "            nn.Conv1d(1280, 512, kernel_size=3, dilation=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(512, 16, kernel_size=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, num_frames, C, H, W = x.size()\n",
    "        x_reshaped = x.view(-1, C, H, W)\n",
    "        features_reshaped = self.mobilenet(x_reshaped)\n",
    "        features = features_reshaped.view(batch_size, num_frames, -1).permute(0, 2, 1)\n",
    "        out = self.tcn(features)\n",
    "        out = out[:, :, -1]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training, checkpointing, and evaluation functions\n",
    "\n",
    "This cell defines the training loop which uses mixed precision (`torch.cuda.amp`), shows progress with `tqdm`, and saves checkpoints to resume training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, best_val_loss, checkpoint_path):\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(state, checkpoint_path)\n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path):\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        state = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(state[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
    "        return state[\"epoch\"], state[\"best_val_loss\"]\n",
    "    return 0, float(\"inf\")\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, lr, checkpoint_path):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler = GradScaler()\n",
    "    start_epoch, best_val_loss = load_checkpoint(model, optimizer, checkpoint_path)\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for frames, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\", leave=False, total=len(train_loader), mininterval=1, dynamic_ncols=True):\n",
    "            frames, labels = frames.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast(enabled=True, device_type='cuda'):\n",
    "                outputs = model(frames)\n",
    "                loss_fn = nn.CrossEntropyLoss()\n",
    "                outputs_reshaped = outputs.view(outputs.size(0), 4, 4)\n",
    "                loss = 0.0\n",
    "                for d in range(4):\n",
    "                    loss += loss_fn(outputs_reshaped[:, d, :], labels[:, d])\n",
    "                loss = loss / 4.0\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            running_loss += loss.item() * frames.size(0)\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        for frames, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\", leave=False, total=len(val_loader), mininterval=1, dynamic_ncols=True):\n",
    "            frames, labels = frames.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            with autocast(enabled=True, device_type='cuda'):\n",
    "                outputs = model(frames)\n",
    "                outputs_reshaped = outputs.view(outputs.size(0), 4, 4)\n",
    "                loss = 0.0\n",
    "                for d in range(4):\n",
    "                    loss += loss_fn(outputs_reshaped[:, d, :], labels[:, d])\n",
    "                loss = loss / 4.0\n",
    "            val_loss += loss.item() * frames.size(0)\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_checkpoint(model, optimizer, epoch+1, best_val_loss, checkpoint_path)\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Optuna objective for hyperparameter tuning (using SQLite storage)\n",
    "\n",
    "This cell defines an Optuna objective that trains the MobileNetV2-TCN model for a few epochs using hyperparameters suggested by the trial. The study is configured to use an SQLite database (`tuning.db`) for saving progress so tuning can be resumed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    num_frames = trial.suggest_categorical(\"num_frames\", [30])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 5e-4, log=True)\n",
    "    epochs = trial.suggest_int(\"epochs\", 3, 5)\n",
    "    \n",
    "    from torch.utils.data import DataLoader\n",
    "    train_dataset = VideoDataset(LABELS_DIR / \"TrainLabels.csv\", FRAMES_DIR, transform=train_transform, num_frames=num_frames)\n",
    "    val_dataset = VideoDataset(LABELS_DIR / \"ValidationLabels.csv\", FRAMES_DIR, transform=val_transform, num_frames=num_frames)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    model = MobileNetTCN()\n",
    "    trial_checkpoint = MODEL_DIR / f\"checkpoint_trial_{trial.number}.pth\"\n",
    "    \n",
    "    try:\n",
    "        best_val_loss = train_model(model, train_loader, val_loader, epochs, lr, trial_checkpoint)\n",
    "        return best_val_loss\n",
    "    except Exception as e:\n",
    "        print(f\"Trial {trial.number} failed: {e}\")\n",
    "        return float(\"inf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate and visualize results\n",
    "\n",
    "This cell evaluates the final model on the test set and prints a classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for frames, labels in tqdm(test_loader, desc=\"Evaluating\", dynamic_ncols=True):\n",
    "            frames = frames.to(device)\n",
    "            outputs = model(frames)\n",
    "            outputs_reshaped = outputs.view(outputs.size(0), 4, 4)\n",
    "            preds = torch.argmax(outputs_reshaped, dim=2)\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels)\n",
    "    all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "    \n",
    "    dims = [\"Engagement\", \"Boredom\", \"Confusion\", \"Frustration\"]\n",
    "    for i, dim in enumerate(dims):\n",
    "        print(f\"Classification report for {dim}:\")\n",
    "        print(classification_report(all_labels[:, i], all_preds[:, i], digits=3))\n",
    "        \n",
    "        # Compute confusion matrix and plot it with matplotlib.\n",
    "        cm = confusion_matrix(all_labels[:, i], all_preds[:, i])\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Confusion Matrix for {dim}\")\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(cm.shape[0])\n",
    "        plt.xticks(tick_marks, tick_marks)\n",
    "        plt.yticks(tick_marks, tick_marks)\n",
    "        plt.xlabel(\"Predicted label\")\n",
    "        plt.ylabel(\"True label\")\n",
    "        # Annotate each cell with its value.\n",
    "        thresh = cm.max() / 2.\n",
    "        for j in range(cm.shape[0]):\n",
    "            for k in range(cm.shape[1]):\n",
    "                plt.text(k, j, format(cm[j, k], 'd'),\n",
    "                         horizontalalignment=\"center\",\n",
    "                         color=\"white\" if cm[j, k] > thresh else \"black\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing best frames for training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing best frames:  83%|████████▎ | 4471/5358 [3:23:30<51:52,  3.51s/it]  "
     ]
    }
   ],
   "source": [
    "# Step 1: (Optional) Precompute and cache best frames for training and validation CSVs.\n",
    "train_csv = LABELS_DIR / \"TrainLabels.csv\"\n",
    "val_csv = LABELS_DIR / \"ValidationLabels.csv\"\n",
    "\n",
    "cache_file_train = CACHE_DIR / f\"precomputed_{Path(train_csv).stem}_frames.pkl\"\n",
    "if not cache_file_train.exists():\n",
    "    print(\"Precomputing best frames for training data...\")\n",
    "    precompute_best_frames(train_csv, FRAMES_DIR, num_frames=30)\n",
    "\n",
    "cache_file_val = CACHE_DIR / f\"precomputed_{Path(val_csv).stem}_frames.pkl\"      \n",
    "if not cache_file_val.exists():\n",
    "    print(\"Precomputing best frames for validation data...\")      \n",
    "    precompute_best_frames(val_csv, FRAMES_DIR, num_frames=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Run Optuna tuning with a progress bar.\n",
    "n_trials = 10\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    study_name=\"mobilev2_tcn_study\",\n",
    "    storage=\"sqlite:///tuning.db\",\n",
    "    load_if_exists=True\n",
    ")\n",
    "print(\"Starting Optuna hyperparameter tuning...\")\n",
    "pbar = tqdm(total=n_trials, desc=\"Optuna Trials\", unit=\"trial\", leave=True, dynamic_ncols=True)\n",
    "study.optimize(objective, n_trials=n_trials, catch=(Exception,), callbacks=[lambda study, trial: pbar.update()])\n",
    "pbar.close()\n",
    "print(\"Optuna tuning complete.\")\n",
    "print(\"Best trial:\", study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Final training with best hyperparameters.\n",
    "best_trial = study.best_trial\n",
    "num_frames = best_trial.params[\"num_frames\"]\n",
    "batch_size = best_trial.params[\"batch_size\"]\n",
    "lr = best_trial.params[\"lr\"]\n",
    "epochs = best_trial.params[\"epochs\"]\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataset = VideoDataset(train_csv, FRAMES_DIR, transform=train_transform, num_frames=num_frames)\n",
    "val_dataset = VideoDataset(val_csv, FRAMES_DIR, transform=val_transform, num_frames=num_frames)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "final_model = MobileNetTCN()\n",
    "final_checkpoint = MODEL_DIR / \"final_model_checkpoint.pth\"\n",
    "print(\"Starting final training with best hyperparameters...\")\n",
    "train_model(final_model, train_loader, val_loader, epochs, lr, final_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Evaluate the final model on test data with visualization.\n",
    "test_csv = LABELS_DIR / \"TestLabels.csv\"  # update as needed\n",
    "test_dataset = VideoDataset(test_csv, FRAMES_DIR, transform=val_transform, num_frames=num_frames)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "print(\"Evaluating final model...\")\n",
    "evaluate_model(final_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
