{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries, set up paths, and device\n",
    "\n",
    "This cell imports necessary libraries and defines file paths and device settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import optuna\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm \n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torch.amp import GradScaler, autocast\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define paths\n",
    "BASE_DIR = Path(\"C:/Users/abhis/Downloads/Documents/Learner Engagement Project\")\n",
    "DATA_DIR = BASE_DIR / \"data\" / \"DAiSEE\"\n",
    "FRAMES_DIR = DATA_DIR / \"ExtractedFrames\"\n",
    "LABELS_DIR = DATA_DIR / \"Labels\"\n",
    "MODEL_DIR = BASE_DIR / \"models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Precomputed directory for caching best frames\n",
    "CACHE_DIR = BASE_DIR / \"cache\"\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Set device and CUDA configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Define data transforms for training and validation.\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define helper function for selecting 30 best frames using face detection and sharpness\n",
    "\n",
    "This cell defines a function that, given a folder of extracted frames, divides them into 30 equal temporal segments and selects the best frame from each segment based on face detection and sharpness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping CSV clip IDs.\n",
    "def get_csv_clip_id(video_stem: str) -> str:\n",
    "    base = video_stem.strip()\n",
    "    if base.startswith(\"110001\"):\n",
    "        base = base.replace(\"110001\", \"202614\", 1)\n",
    "    return base\n",
    "\n",
    "# Select best frames using face detection and Laplacian variance.\n",
    "def select_impactful_frames(video_folder: Path, num_frames=30):\n",
    "    frame_files = sorted(video_folder.glob(\"frame_*.jpg\"))\n",
    "    total_frames = len(frame_files)\n",
    "    if total_frames == 0:\n",
    "        return []\n",
    "    if total_frames <= num_frames:\n",
    "        return frame_files\n",
    "    segment_size = total_frames // num_frames\n",
    "    cascade_path = cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    "    face_cascade = cv2.CascadeClassifier(cascade_path)\n",
    "    selected_frames = []\n",
    "    for i in range(num_frames):\n",
    "        start_idx = i * segment_size\n",
    "        end_idx = (i + 1) * segment_size if i < num_frames - 1 else total_frames\n",
    "        best_score = -1\n",
    "        best_frame = None\n",
    "        for fp in frame_files[start_idx:end_idx]:\n",
    "            img = cv2.imread(str(fp))\n",
    "            if img is None:\n",
    "                continue\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "            if len(faces) > 0:\n",
    "                face = max(faces, key=lambda r: r[2]*r[3])\n",
    "                x, y, w, h = face\n",
    "                region = gray[y:y+h, x:x+w]\n",
    "                quality = cv2.Laplacian(region, cv2.CV_64F).var()\n",
    "            else:\n",
    "                quality = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "            if quality > best_score:\n",
    "                best_score = quality\n",
    "                best_frame = fp\n",
    "        if best_frame is not None:\n",
    "            selected_frames.append(best_frame)\n",
    "    return selected_frames\n",
    "\n",
    "def precompute_best_frames(csv_file: Path, video_root: Path, num_frames=30):\n",
    "    \"\"\"\n",
    "    Precompute and cache the best frame paths for each video in the CSV.\n",
    "    The results are saved to a pickle file and returned.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(csv_file, dtype=str)\n",
    "    data.columns = data.columns.str.strip()\n",
    "    split = Path(csv_file).stem.replace(\"Labels\", \"\").strip()\n",
    "    precomputed = []  # list of lists for each video\n",
    "    valid_indices = []  # valid data indices\n",
    "    skipped_count = 0\n",
    "\n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data), desc=\"Precomputing best frames\", dynamic_ncols=True):\n",
    "        clip_id = str(row[\"ClipID\"]).strip()\n",
    "        if clip_id.endswith(('.avi', '.mp4')):\n",
    "            clip_id = clip_id.rsplit('.', 1)[0]\n",
    "        mapped_id = get_csv_clip_id(clip_id)\n",
    "        video_folder = video_root / split / mapped_id\n",
    "        if video_folder.exists():\n",
    "            frame_files = sorted(video_folder.glob(\"frame_*.jpg\"))\n",
    "            if len(frame_files) >= num_frames:\n",
    "                selected_frames = select_impactful_frames(video_folder, num_frames)\n",
    "                precomputed.append(selected_frames)\n",
    "                valid_indices.append(idx)\n",
    "            else:\n",
    "                skipped_count += 1\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "    print(f\"Precomputation: Skipped {skipped_count} videos out of {len(data)}.\")\n",
    "    cache_data = {\"valid_indices\": valid_indices, \"precomputed_frames\": precomputed}\n",
    "    cache_file = CACHE_DIR / f\"precomputed_{Path(csv_file).stem}_frame_{num_frames}.pkl\"\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(cache_data, f)\n",
    "    print(f\"Precomputed results saved to {cache_file}\")\n",
    "    return cache_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the custom Dataset for video classification\n",
    "\n",
    "This cell defines a PyTorch Dataset that reads a CSV file with video IDs and 4 labels. For each video, it loads the 30 best frames using the above function, applies transforms, and returns a tensor and its label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, video_root, transform=None, num_frames=30):\n",
    "        self.csv_file = Path(csv_file)\n",
    "        self.data = pd.read_csv(self.csv_file, dtype=str)\n",
    "        self.data.columns = self.data.columns.str.strip()\n",
    "        self.video_root = Path(video_root)\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "        self.split = self.csv_file.stem.replace(\"Labels\", \"\").strip()\n",
    "        cache_file = CACHE_DIR / f\"precomputed_{Path(csv_file).stem}_frame_{num_frames}.pkl\"\n",
    "        if cache_file.exists():\n",
    "            with open(cache_file, \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            valid_indices = cache_data[\"valid_indices\"]\n",
    "            self.precomputed_frames = cache_data[\"precomputed_frames\"]\n",
    "            self.data = self.data.iloc[valid_indices].reset_index(drop=True)\n",
    "            print(f\"Loaded precomputed frames for {len(self.data)} videos from cache.\")\n",
    "        else:\n",
    "            valid_rows = []\n",
    "            self.precomputed_frames = []\n",
    "            skipped_count = 0\n",
    "            for idx, row in self.data.iterrows():\n",
    "                clip_id = str(row[\"ClipID\"]).strip()\n",
    "                if clip_id.endswith(('.avi', '.mp4')):\n",
    "                    clip_id = clip_id.rsplit('.', 1)[0]\n",
    "                mapped_id = get_csv_clip_id(clip_id)\n",
    "                video_folder = self.video_root / self.split / mapped_id\n",
    "                if video_folder.exists():\n",
    "                    frame_files = sorted(video_folder.glob(\"frame_*.jpg\"))\n",
    "                    if len(frame_files) >= num_frames:\n",
    "                        selected_frames = select_impactful_frames(video_folder, num_frames)\n",
    "                        valid_rows.append(row)\n",
    "                        self.precomputed_frames.append(selected_frames)\n",
    "                    else:\n",
    "                        skipped_count += 1\n",
    "                else:\n",
    "                    skipped_count += 1\n",
    "            self.data = pd.DataFrame(valid_rows)\n",
    "            print(f\"Computed frames on the fly: Skipped {skipped_count} videos.\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        selected_frame_paths = self.precomputed_frames[idx]\n",
    "        frames = []\n",
    "        for fp in selected_frame_paths:\n",
    "            try:\n",
    "                # Attempt to load frame using PIL\n",
    "                img = Image.open(fp).convert(\"RGB\")\n",
    "            except (FileNotFoundError, OSError):\n",
    "                # Create a black PIL image placeholder\n",
    "                img = Image.new('RGB', (224, 224))\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            frames.append(img)\n",
    "        \n",
    "        # Ensure exactly num_frames by adding placeholder images if needed\n",
    "        while len(frames) < self.num_frames:\n",
    "            placeholder_img = Image.new('RGB', (224, 224))\n",
    "            if self.transform:\n",
    "                placeholder_img = self.transform(placeholder_img)\n",
    "            frames.append(placeholder_img)\n",
    "        \n",
    "        frames_tensor = torch.stack(frames)\n",
    "        labels = torch.tensor([\n",
    "            int(row[\"Engagement\"]),\n",
    "            int(row[\"Boredom\"]),\n",
    "            int(row[\"Confusion\"]),\n",
    "            int(row[\"Frustration\"])\n",
    "        ], dtype=torch.long)\n",
    "        return frames_tensor, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the MobileNetV2-TCN model\n",
    "\n",
    "This cell defines the MobileNetV2-TCN model. It processes a sequence of frames by applying MobileNetV2 on each frame, stacking the features, and feeding them to a temporal convolution network (TCN).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetTCN(nn.Module):\n",
    "    def __init__(self, hidden_ch=512, freeze_block=0):\n",
    "        super(MobileNetTCN, self).__init__()\n",
    "        self.mobilenet = mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "        self.freeze_blocks(freeze_block)\n",
    "        self.mobilenet.classifier = nn.Identity()\n",
    "        \n",
    "        # Adjust TCN layers using hyperparameter hidden_ch.\n",
    "        self.tcn = nn.Sequential(\n",
    "            nn.Conv1d(1280, hidden_ch, kernel_size=3, dilation=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_ch, 16, kernel_size=1)\n",
    "        )\n",
    "    \n",
    "    def freeze_blocks(self, freeze_block):\n",
    "        # Freeze the first 'freeze_block' blocks in MobileNetV2 features.\n",
    "        if freeze_block > 0:\n",
    "            for i in range(freeze_block):\n",
    "                if i < len(self.mobilenet.features):\n",
    "                    for param in self.mobilenet.features[i].parameters():\n",
    "                        param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, num_frames, C, H, W = x.size()\n",
    "        x_reshaped = x.view(-1, C, H, W)\n",
    "        features_reshaped = self.mobilenet(x_reshaped)\n",
    "        features = features_reshaped.view(batch_size, num_frames, -1).permute(0, 2, 1)\n",
    "        out = self.tcn(features)\n",
    "        out = out[:, :, -1]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training, checkpointing, and evaluation functions\n",
    "\n",
    "This cell defines the training loop which uses mixed precision (`torch.cuda.amp`), shows progress with `tqdm`, and saves checkpoints to resume training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, best_val_loss, checkpoint_path):\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(state, checkpoint_path)\n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path):\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        state = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(state[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
    "        return state[\"epoch\"], state[\"best_val_loss\"]\n",
    "    return 0, float(\"inf\")\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, lr, checkpoint_path, patience=5, gradient_accum_steps=1):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler = GradScaler()\n",
    "    start_epoch, best_val_loss = load_checkpoint(model, optimizer, checkpoint_path)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    # Outer tqdm for epochs\n",
    "    epoch_pbar = tqdm(\n",
    "        range(start_epoch, epochs),\n",
    "        desc=\"Epochs\",\n",
    "        total=epochs - start_epoch,\n",
    "        position=0,\n",
    "        leave=True,\n",
    "        dynamic_ncols=True\n",
    "    )\n",
    "\n",
    "    for epoch in epoch_pbar:\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Inner tqdm for batch-level progress within each epoch\n",
    "        train_iter = tqdm(\n",
    "            enumerate(train_loader),\n",
    "            total=len(train_loader),\n",
    "            desc=f\"Epoch {epoch+1} [Train]\",\n",
    "            position=1,\n",
    "            leave=False,\n",
    "            dynamic_ncols=True\n",
    "        )\n",
    "\n",
    "        for i, (frames, labels) in train_iter:\n",
    "            frames, labels = frames.to(device), labels.to(device)\n",
    "\n",
    "            with autocast(enabled=True, dtype=torch.float16, device_type='cuda'):\n",
    "                outputs = model(frames)\n",
    "                # outputs_reshaped: [batch_size, 4 (dimensions), 4 (classes each dimension)]\n",
    "                outputs_reshaped = outputs.view(outputs.size(0), 4, 4)\n",
    "                # Summation of cross-entropy across the 4 dimensions\n",
    "                loss = sum(loss_fn(outputs_reshaped[:, d], labels[:, d]) for d in range(4)) / 4.0\n",
    "\n",
    "            # Gradient scaling / accumulation\n",
    "            scaler.scale(loss / gradient_accum_steps).backward()\n",
    "            if (i + 1) % gradient_accum_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item() * frames.size(0)\n",
    "\n",
    "        # Compute average training loss over the entire train set\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for frames, labels in val_loader:\n",
    "                frames, labels = frames.to(device), labels.to(device)\n",
    "                with autocast(enabled=True, dtype=torch.float16, device_type='cuda'):\n",
    "                    outputs = model(frames)\n",
    "                    outputs_reshaped = outputs.view(outputs.size(0), 4, 4)\n",
    "                    loss = sum(loss_fn(outputs_reshaped[:, d], labels[:, d]) for d in range(4)) / 4.0\n",
    "                    val_loss += loss.item() * frames.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        # Update the epoch-level progress bar with train & val losses\n",
    "        epoch_pbar.set_postfix({\"train_loss\": f\"{train_loss:.4f}\", \"val_loss\": f\"{val_loss:.4f}\"})\n",
    "\n",
    "        # Print to console as well if desired\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_checkpoint(model, optimizer, epoch + 1, best_val_loss, checkpoint_path)\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. Best loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Optuna objective for hyperparameter tuning (using SQLite storage)\n",
    "\n",
    "This cell defines an Optuna objective that trains the MobileNetV2-TCN model for a few epochs using hyperparameters suggested by the trial. The study is configured to use an SQLite database (`tuning.db`) for saving progress so tuning can be resumed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    num_frames = trial.suggest_categorical(\"num_frames\", [30])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 5e-4, log=True)\n",
    "    epochs = trial.suggest_int(\"epochs\", 3, 5)\n",
    "    hidden_ch = trial.suggest_categorical(\"hidden_ch\", [64, 128, 256])\n",
    "    freeze_block = trial.suggest_int(\"freeze_block\", 0, 4)\n",
    "\n",
    "    from torch.utils.data import DataLoader\n",
    "    train_dataset = VideoDataset(LABELS_DIR / \"TrainLabels.csv\", FRAMES_DIR, transform=train_transform, num_frames=num_frames)\n",
    "    val_dataset = VideoDataset(LABELS_DIR / \"ValidationLabels.csv\", FRAMES_DIR, transform=val_transform, num_frames=num_frames)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    model = MobileNetTCN(hidden_ch=hidden_ch, freeze_block=freeze_block)\n",
    "    trial_checkpoint = MODEL_DIR / f\"checkpoint_trial_{trial.number}.pth\"\n",
    "\n",
    "    try:\n",
    "        best_val_loss = train_model(model, train_loader, val_loader, epochs, lr, trial_checkpoint, patience=3)\n",
    "        return best_val_loss\n",
    "    except Exception as e:\n",
    "        print(f\"Trial {trial.number} failed: {e}\")\n",
    "        return float(\"inf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate and visualize results\n",
    "\n",
    "This cell evaluates the final model on the test set and prints a classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for frames, labels in tqdm(test_loader, desc=\"Evaluating\", dynamic_ncols=True):\n",
    "            frames = frames.to(device)\n",
    "            outputs = model(frames)\n",
    "            outputs_reshaped = outputs.view(outputs.size(0), 4, 4)\n",
    "            preds = torch.argmax(outputs_reshaped, dim=2)\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels)\n",
    "    all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "    \n",
    "    dims = [\"Engagement\", \"Boredom\", \"Confusion\", \"Frustration\"]\n",
    "    for i, dim in enumerate(dims):\n",
    "        print(f\"Classification report for {dim}:\")\n",
    "        print(classification_report(all_labels[:, i], all_preds[:, i], digits=3))\n",
    "        \n",
    "        cm = confusion_matrix(all_labels[:, i], all_preds[:, i])\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Confusion Matrix for {dim}\")\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(cm.shape[0])\n",
    "        plt.xticks(tick_marks, tick_marks)\n",
    "        plt.yticks(tick_marks, tick_marks)\n",
    "        plt.xlabel(\"Predicted label\")\n",
    "        plt.ylabel(\"True label\")\n",
    "        thresh = cm.max() / 2.\n",
    "        for j in range(cm.shape[0]):\n",
    "            for k in range(cm.shape[1]):\n",
    "                plt.text(k, j, format(cm[j, k], 'd'),\n",
    "                         horizontalalignment=\"center\",\n",
    "                         color=\"white\" if cm[j, k] > thresh else \"black\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-17 12:33:36,448] A new study created in RDB with name: mobilev2_tcn_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna hyperparameter tuning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f23a18de3b64bf794fa73d7f346bdad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optuna Trials:   0%|          | 0/10 [00:00<?, ?trial/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded precomputed frames for 4851 videos from cache.\n",
      "Loaded precomputed frames for 1429 videos from cache.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337a7dafeb074d9d948679cecd60813d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1560464154594279b95dcdc8b819eb3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [Train]:   0%|          | 0/304 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 0.8689 | Val Loss: 0.9948\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2afb2b7cd8b0466bbc64cc9a6fcfc908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 [Train]:   0%|          | 0/304 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Step 1: (Optional) Precompute and cache best frames\n",
    "    train_csv = LABELS_DIR / \"TrainLabels.csv\"\n",
    "    val_csv = LABELS_DIR / \"ValidationLabels.csv\"\n",
    "    \n",
    "    cache_file_train = CACHE_DIR / f\"precomputed_{Path(train_csv).stem}_frame_30.pkl\"\n",
    "    if not cache_file_train.exists():\n",
    "        print(\"Precomputing best frames for training data...\")\n",
    "        precompute_best_frames(train_csv, FRAMES_DIR, num_frames=30)\n",
    "    \n",
    "    cache_file_val = CACHE_DIR / f\"precomputed_{Path(val_csv).stem}_frame_30.pkl\"      \n",
    "    if not cache_file_val.exists():\n",
    "        print(\"Precomputing best frames for validation data...\")      \n",
    "        precompute_best_frames(val_csv, FRAMES_DIR, num_frames=30)\n",
    "    \n",
    "    # ----------------------\n",
    "    # Step 2: Run Optuna tuning with early stopping\n",
    "    # ----------------------\n",
    "     # Optuna hyperparameter tuning\n",
    "    n_trials = 10\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        study_name=\"mobilev2_tcn_study\",\n",
    "        storage=\"sqlite:///tuning.db\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    print(\"Starting Optuna hyperparameter tuning...\")\n",
    "\n",
    "    pbar = tqdm(total=n_trials, desc=\"Optuna Trials\", unit=\"trial\", dynamic_ncols=True)\n",
    "    def update(study, trial):\n",
    "        pbar.update()\n",
    "\n",
    "    study.optimize(objective, n_trials=n_trials, catch=(Exception,), callbacks=[update])\n",
    "    pbar.close()\n",
    "    print(f\"Optuna tuning complete.\\nBest trial: {study.best_trial}\")\n",
    "    \n",
    "    # ----------------------\n",
    "    # Step 3: Final training with best hyperparameters and early stopping.\n",
    "    # ----------------------\n",
    "    best_trial = study.best_trial\n",
    "    num_frames = best_trial.params[\"num_frames\"]\n",
    "    batch_size = best_trial.params[\"batch_size\"]\n",
    "    lr = best_trial.params[\"lr\"]\n",
    "    epochs = best_trial.params[\"epochs\"]\n",
    "    hidden_ch = best_trial.params[\"hidden_ch\"]\n",
    "    freeze_block = best_trial.params[\"freeze_block\"]\n",
    "    \n",
    "    # Use num_workers=0 to avoid Windows spawn delays.\n",
    "    train_dataset = VideoDataset(train_csv, FRAMES_DIR, transform=train_transform, num_frames=num_frames)\n",
    "    val_dataset = VideoDataset(val_csv, FRAMES_DIR, transform=val_transform, num_frames=num_frames)\n",
    "    \n",
    "    from torch.utils.data import DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    final_model = MobileNetTCN(hidden_ch=hidden_ch, freeze_block=freeze_block)\n",
    "    final_checkpoint = MODEL_DIR / \"final_model_checkpoint.pth\"\n",
    "    patience = 5  # Set patience for final training\n",
    "    print(\"Starting final training with best hyperparameters...\")\n",
    "    train_model(final_model, train_loader, val_loader, epochs, lr, final_checkpoint, patience=patience)\n",
    "    \n",
    "    # ----------------------\n",
    "    # Step 4: Evaluate final model on test data.\n",
    "    # ----------------------\n",
    "    test_csv = LABELS_DIR / \"TestLabels.csv\"  # Update if needed\n",
    "    test_dataset = VideoDataset(test_csv, FRAMES_DIR, transform=val_transform, num_frames=num_frames)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    print(\"Evaluating final model...\")\n",
    "    evaluate_model(final_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
