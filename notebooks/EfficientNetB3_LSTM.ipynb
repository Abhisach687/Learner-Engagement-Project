{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import torch\n",
    "import optuna\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import io\n",
    "import lmdb  # pip install lmdb\n",
    "from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights, mobilenet_v2, MobileNet_V2_Weights\n",
    "from torch.amp import GradScaler, autocast\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import sqlite3\n",
    "from optuna.pruners import MedianPruner\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.checkpoint import checkpoint  # For gradient checkpointing\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# CONSTANTS & HYPERPARAMETERS\n",
    "# ------------------------------\n",
    "GRADIENT_ACCUM_STEPS = 4      # Accumulate gradients over 4 mini-batches (simulate effective batch size = mini_batch * 4)\n",
    "NUM_FRAMES = 30               # Use 30 frames per video\n",
    "\n",
    "# ------------------------------\n",
    "# Environment & Paths\n",
    "# ------------------------------\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "BASE_DIR = Path(\"C:/Users/abhis/Downloads/Documents/Learner Engagement Project\")\n",
    "DATA_DIR = BASE_DIR / \"data\" / \"DAiSEE\"\n",
    "FRAMES_DIR = DATA_DIR / \"ExtractedFrames\"\n",
    "LABELS_DIR = DATA_DIR / \"Labels\"  # Expect subfolders: Train, Validation, Test\n",
    "MODEL_DIR = BASE_DIR / \"models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR = BASE_DIR / \"cache\"\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Models directory exists:\", os.path.exists(MODEL_DIR))\n",
    "print(\"Checkpoint path writable:\", os.access(MODEL_DIR, os.W_OK))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ------------------------------\n",
    "# Data Transforms\n",
    "# ------------------------------\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Helper Functions\n",
    "# ------------------------------\n",
    "def get_csv_clip_id(video_stem: str) -> str:\n",
    "    base = video_stem.strip()\n",
    "    return base.replace(\"110001\", \"202614\", 1) if base.startswith(\"110001\") else base\n",
    "\n",
    "def select_impactful_frames(video_folder: Path, num_frames=30):\n",
    "    frame_files = sorted(video_folder.glob(\"frame_*.jpg\"))\n",
    "    total_frames = len(frame_files)\n",
    "    if total_frames == 0:\n",
    "        return []\n",
    "    if total_frames <= num_frames:\n",
    "        return frame_files\n",
    "    # For speed, select evenly spaced frames.\n",
    "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    return [frame_files[i] for i in indices]\n",
    "\n",
    "def precompute_best_frames(csv_file: Path, video_root: Path, num_frames=30):\n",
    "    \"\"\"\n",
    "    Precompute and cache the frame file paths for each video.\n",
    "    Saves a pickle file with keys \"valid_indices\" and \"precomputed_frames\".\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(csv_file, dtype=str)\n",
    "    data.columns = data.columns.str.strip()\n",
    "    split = csv_file.stem.replace(\"Labels\", \"\").strip()\n",
    "    valid_indices = []\n",
    "    precomputed = []\n",
    "    skipped = 0\n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data), desc=\"Precomputing frames\"):\n",
    "        clip_id = get_csv_clip_id(row[\"ClipID\"].split('.')[0])\n",
    "        video_folder = video_root / split / clip_id\n",
    "        if video_folder.exists():\n",
    "            frames = select_impactful_frames(video_folder, num_frames)\n",
    "            if len(frames) >= num_frames:\n",
    "                precomputed.append(frames[:num_frames])\n",
    "                valid_indices.append(idx)\n",
    "            else:\n",
    "                skipped += 1\n",
    "        else:\n",
    "            skipped += 1\n",
    "    print(f\"Precomputation: Skipped {skipped} videos out of {len(data)}.\")\n",
    "    cache_data = {\"valid_indices\": valid_indices, \"precomputed_frames\": precomputed}\n",
    "    cache_file = CACHE_DIR / f\"precomputed_{csv_file.stem}_frame_{num_frames}.pkl\"\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(cache_data, f)\n",
    "    print(f\"Precomputed results saved to {cache_file}\")\n",
    "    return cache_data\n",
    "\n",
    "def convert_pkl_to_lmdb(csv_file: Path, num_frames=30, transform=train_transform, lmdb_map_size=1 * 1024**3):\n",
    "    \"\"\"\n",
    "    Convert the precomputed .pkl file into an LMDB database containing\n",
    "    preprocessed feature tensors (in half precision) extracted by EfficientNet-B3.\n",
    "    If the LMDB folder already exists, it is reused.\n",
    "    \"\"\"\n",
    "    pkl_file = CACHE_DIR / f\"precomputed_{csv_file.stem}_frame_{num_frames}.pkl\"\n",
    "    lmdb_path = CACHE_DIR / f\"lmdb_{csv_file.stem}_frame_{num_frames}\"\n",
    "    # If the LMDB folder exists, assume it is valid and return its path.\n",
    "    if lmdb_path.exists():\n",
    "        print(f\"LMDB database already exists at {lmdb_path}\")\n",
    "        return lmdb_path\n",
    "\n",
    "    env = lmdb.open(str(lmdb_path), map_size=lmdb_map_size)\n",
    "    try:\n",
    "        with env.begin(write=False) as txn:\n",
    "            if txn.stat()['entries'] > 0:\n",
    "                print(f\"LMDB database already exists at {lmdb_path}\")\n",
    "                env.close()\n",
    "                return lmdb_path\n",
    "    except lmdb.Error as e:\n",
    "        print(\"Error checking LMDB database:\", e)\n",
    "    \n",
    "    if not pkl_file.exists():\n",
    "        precompute_best_frames(csv_file, FRAMES_DIR, num_frames=num_frames)\n",
    "    with open(pkl_file, \"rb\") as f:\n",
    "        cache = pickle.load(f)\n",
    "    valid_indices = cache[\"valid_indices\"]\n",
    "    file_paths_list = cache[\"precomputed_frames\"]\n",
    "\n",
    "    # Prepare a frozen EfficientNet-B3 feature extractor.\n",
    "    feature_extractor = efficientnet_b3(weights=EfficientNet_B3_Weights.IMAGENET1K_V1).features\n",
    "    feature_extractor.eval()\n",
    "    feature_extractor.to(device)\n",
    "    for param in feature_extractor.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    print(\"Converting file paths to LMDB preprocessed feature tensors...\")\n",
    "    with env.begin(write=True) as txn:\n",
    "        for idx, paths in tqdm(enumerate(file_paths_list), total=len(file_paths_list)):\n",
    "            video_features = []\n",
    "            for fp in paths:\n",
    "                try:\n",
    "                    img = Image.open(fp).convert(\"RGB\")\n",
    "                except Exception:\n",
    "                    img = Image.new('RGB', (224, 224))\n",
    "                tensor = transform(img).unsqueeze(0).to(device)\n",
    "                with torch.no_grad(), autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    feat = feature_extractor(tensor)\n",
    "                    feat = nn.functional.adaptive_avg_pool2d(feat, (1, 1)).view(-1).cpu().half()\n",
    "                video_features.append(feat)\n",
    "            if video_features:\n",
    "                video_features = torch.stack(video_features)\n",
    "                key = f\"video_{valid_indices[idx]}\".encode(\"utf-8\")\n",
    "                txn.put(key, pickle.dumps(video_features))\n",
    "    env.close()\n",
    "    print(f\"LMDB database created at {lmdb_path}\")\n",
    "    return lmdb_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# LMDB Dataset Class (Corrected)\n",
    "# ------------------------------\n",
    "class VideoDatasetLMDB(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, lmdb_path, num_frames=30):\n",
    "        self.data = pd.read_csv(csv_file, dtype=str)\n",
    "        self.data.columns = self.data.columns.str.strip()\n",
    "        pkl_file = CACHE_DIR / f\"precomputed_{csv_file.stem}_frame_{num_frames}.pkl\"\n",
    "        with open(pkl_file, \"rb\") as f:\n",
    "            cache = pickle.load(f)\n",
    "        # Save the original valid indices for key lookup.\n",
    "        self.valid_indices = cache[\"valid_indices\"]\n",
    "        self.data = self.data.iloc[self.valid_indices].reset_index(drop=True)\n",
    "        self.num_frames = num_frames\n",
    "        self.lmdb_path = str(lmdb_path)\n",
    "        self.env = None  # Will be opened per worker\n",
    "\n",
    "    def _init_env(self):\n",
    "        if self.env is None:\n",
    "            self.env = lmdb.open(self.lmdb_path, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "        return self.env\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        env = self._init_env()\n",
    "        # Use the original index from valid_indices for key formation.\n",
    "        original_idx = self.valid_indices[idx]\n",
    "        key = f\"video_{original_idx}\".encode(\"utf-8\")\n",
    "        with env.begin(write=False) as txn:\n",
    "            data_bytes = txn.get(key)\n",
    "            if data_bytes is None:\n",
    "                raise IndexError(f\"Key {key} not found in LMDB\")\n",
    "            features = pickle.loads(data_bytes)\n",
    "        labels = self.data.iloc[idx][[\"Engagement\", \"Boredom\", \"Confusion\", \"Frustration\"]].astype(int)\n",
    "        return features, torch.tensor(labels.values, dtype=torch.long)\n",
    "\n",
    "# ------------------------------\n",
    "# LSTM Model for Precomputed Features\n",
    "# ------------------------------\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, feature_dim=1536, hidden_size=128, num_lstm_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(feature_dim, hidden_size, num_lstm_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 4 * 4)\n",
    "    def forward(self, x):\n",
    "        lstm_out, (h_n, _) = self.lstm(x)\n",
    "        out = self.fc(h_n[-1])\n",
    "        return out.view(-1, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Checkpointing Functions\n",
    "# ------------------------------\n",
    "def save_checkpoint(model, optimizer, epoch, best_val_loss, checkpoint_path):\n",
    "    try:\n",
    "        print(f\"Saving checkpoint to {checkpoint_path} ...\")\n",
    "        state = {\n",
    "            \"epoch\": epoch,\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict()\n",
    "        }\n",
    "        temp_path = checkpoint_path.with_suffix(\".tmp\")\n",
    "        torch.save(state, temp_path, _use_new_zipfile_serialization=False)\n",
    "        if checkpoint_path.exists():\n",
    "            checkpoint_path.unlink()\n",
    "        temp_path.rename(checkpoint_path)\n",
    "        print(f\"Checkpoint saved successfully to {checkpoint_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving checkpoint: {e}\")\n",
    "        if temp_path.exists():\n",
    "            temp_path.unlink()\n",
    "        raise\n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path):\n",
    "    if checkpoint_path.exists():\n",
    "        try:\n",
    "            state = torch.load(checkpoint_path, map_location=device)\n",
    "            model.load_state_dict(state[\"model_state_dict\"])\n",
    "            optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
    "            return state[\"epoch\"], state[\"best_val_loss\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint {checkpoint_path}: {e}. Starting from scratch.\")\n",
    "            return 0, float(\"inf\")\n",
    "    return 0, float(\"inf\")\n",
    "\n",
    "# ------------------------------\n",
    "# Training Function with Gradient Accumulation\n",
    "# ------------------------------\n",
    "def train_model(model, train_loader, val_loader, epochs, lr, checkpoint_path, patience=5, gradient_accum_steps=GRADIENT_ACCUM_STEPS):\n",
    "    model.to(device, non_blocking=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scaler = GradScaler()\n",
    "    if checkpoint_path.exists():\n",
    "        state = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(state[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
    "        start_epoch = state[\"epoch\"]\n",
    "        best_val_loss = state[\"best_val_loss\"]\n",
    "    else:\n",
    "        start_epoch, best_val_loss = 0, float('inf')\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    early_stop_counter = 0\n",
    "    checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (features, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\")):\n",
    "            features = features.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(features)\n",
    "                outputs = outputs.view(outputs.size(0), 4, 4)\n",
    "                loss = sum(loss_fn(outputs[:, d], labels[:, d]) for d in range(4)) / 4.0\n",
    "            scaler.scale(loss / gradient_accum_steps).backward()\n",
    "            if (i + 1) % gradient_accum_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            running_loss += loss.item() * features.size(0)\n",
    "            del features, labels, outputs, loss\n",
    "            if (i + 1) % 30 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad(), autocast(device_type='cuda', dtype=torch.float16):\n",
    "            for features, labels in val_loader:\n",
    "                features = features.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "                outputs = model(features)\n",
    "                outputs = outputs.view(outputs.size(0), 4, 4)\n",
    "                loss = sum(loss_fn(outputs[:, d], labels[:, d]) for d in range(4)) / 4.0\n",
    "                val_loss += loss.item() * features.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            state = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"best_val_loss\": best_val_loss,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict()\n",
    "            }\n",
    "            temp_path = checkpoint_path.with_suffix(\".tmp\")\n",
    "            torch.save(state, temp_path, _use_new_zipfile_serialization=False)\n",
    "            if checkpoint_path.exists():\n",
    "                checkpoint_path.unlink()\n",
    "            temp_path.rename(checkpoint_path)\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. Best loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Hyperparameter Tuning with Optuna (LMDB Dataset)\n",
    "# ------------------------------\n",
    "def objective(trial):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    num_frames = trial.suggest_categorical(\"num_frames\", [NUM_FRAMES])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [2, 4, 8, 16])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 5e-4, log=True)\n",
    "    epochs = trial.suggest_int(\"epochs\", 3, 5)\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [64, 128])\n",
    "    num_lstm_layers = trial.suggest_categorical(\"num_lstm_layers\", [1, 2])\n",
    "    \n",
    "    train_set = VideoDatasetLMDB(LABELS_DIR / \"TrainLabels.csv\",\n",
    "                                 CACHE_DIR / f\"lmdb_{(LABELS_DIR / 'TrainLabels.csv').stem}_frame_{NUM_FRAMES}\",\n",
    "                                 num_frames=num_frames)\n",
    "    val_set = VideoDatasetLMDB(LABELS_DIR / \"ValidationLabels.csv\",\n",
    "                               CACHE_DIR / f\"lmdb_{(LABELS_DIR / 'ValidationLabels.csv').stem}_frame_{NUM_FRAMES}\",\n",
    "                               num_frames=num_frames)\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=2, pin_memory=False, prefetch_factor=1)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=2, pin_memory=False, prefetch_factor=1)\n",
    "    \n",
    "    model = LSTMModel(feature_dim=1536, hidden_size=hidden_size, num_lstm_layers=num_lstm_layers).to(device)\n",
    "    trial_checkpoint = MODEL_DIR / f\"lmdb_trial_eff_{trial.number}_checkpoint.pth\"\n",
    "    trial_checkpoint.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        best_loss = train_model(model, train_loader, val_loader, epochs, lr, trial_checkpoint,\n",
    "                                patience=3, gradient_accum_steps=GRADIENT_ACCUM_STEPS)\n",
    "    except Exception as e:\n",
    "        if trial_checkpoint.exists():\n",
    "            trial_checkpoint.unlink()\n",
    "        print(f\"Trial {trial.number} failed: {e}\")\n",
    "        best_loss = float(\"inf\")\n",
    "    del model, train_loader, val_loader, train_set, val_set\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return best_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Evaluation Function (LMDB Dataset)\n",
    "# ------------------------------\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad(), autocast(device_type='cuda', dtype=torch.float16):\n",
    "        for features, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            features = features.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model(features)\n",
    "            outputs = outputs.view(outputs.size(0), 4, 4)\n",
    "            preds = torch.argmax(outputs, dim=2)\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "    for i, dim in enumerate([\"Engagement\", \"Boredom\", \"Confusion\", \"Frustration\"]):\n",
    "        print(f\"Classification report for {dim}:\")\n",
    "        print(classification_report(all_labels[:, i], all_preds[:, i], digits=3))\n",
    "        cm = confusion_matrix(all_labels[:, i], all_preds[:, i])\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Confusion Matrix for {dim}\")\n",
    "        plt.colorbar()\n",
    "        plt.xticks(np.arange(cm.shape[0]), np.arange(cm.shape[0]))\n",
    "        plt.yticks(np.arange(cm.shape[1]), np.arange(cm.shape[1]))\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Main Execution Flow\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    torch.multiprocessing.set_start_method('spawn')\n",
    "    import io  # required for LMDB conversion\n",
    "    \n",
    "    # CSV file paths\n",
    "    train_csv = LABELS_DIR / \"TrainLabels.csv\"\n",
    "    val_csv = LABELS_DIR / \"ValidationLabels.csv\"\n",
    "    test_csv = LABELS_DIR / \"TestLabels.csv\"\n",
    "    \n",
    "    # Precompute frame paths and convert to LMDB for each CSV if not already done.\n",
    "    for csv in [train_csv, val_csv, test_csv]:\n",
    "        precompute_best_frames(csv, FRAMES_DIR, NUM_FRAMES)\n",
    "        convert_pkl_to_lmdb(csv, NUM_FRAMES, transform=train_transform, lmdb_map_size=1 * 1024**3)\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Hyperparameter Tuning with Optuna (LMDB Dataset)\n",
    "    # ------------------------------\n",
    "    db_path = BASE_DIR / \"tuning_eff.db\"\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        pruner=MedianPruner(n_startup_trials=2, n_warmup_steps=10),\n",
    "        storage=f\"sqlite:///{db_path}\",\n",
    "        study_name=\"efficientnetb3_lstm_study\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    TARGET_TRIALS = 10\n",
    "    while True:\n",
    "        successful_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE and np.isfinite(t.value)]\n",
    "        remaining = TARGET_TRIALS - len(successful_trials)\n",
    "        if remaining <= 0:\n",
    "            break\n",
    "        print(f\"Running {remaining} additional trial(s) to reach {TARGET_TRIALS} successful trials...\")\n",
    "        study.optimize(objective, n_trials=remaining, catch=(Exception,))\n",
    "    print(f\"Optuna tuning complete. Total successful trials: {len(successful_trials)}\")\n",
    "    best_trial = min(successful_trials, key=lambda t: t.value)\n",
    "    print(\"Best trial parameters:\", best_trial.params)\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Final Training with Best Hyperparameters (LMDB Dataset)\n",
    "    # ------------------------------\n",
    "    final_checkpoint = MODEL_DIR / \"final_model_eff_checkpoint.pth\"\n",
    "    if not final_checkpoint.exists():\n",
    "        print(\"\\n--- Starting Final Training ---\")\n",
    "        best_params = best_trial.params\n",
    "        num_frames = best_params.get(\"num_frames\", NUM_FRAMES)\n",
    "        batch_size = best_params.get(\"batch_size\", 8)\n",
    "        # For safety on an 8GB GPU, cap batch size at 8.\n",
    "        if batch_size > 8:\n",
    "            print(\"Reducing final training batch size to 8 for safety.\")\n",
    "            batch_size = 8\n",
    "        lr = best_params.get(\"lr\", 1e-4)\n",
    "        epochs = best_params.get(\"epochs\", 5)\n",
    "        hidden_size = best_params.get(\"hidden_size\", 128)\n",
    "        num_lstm_layers = best_params.get(\"num_lstm_layers\", 1)\n",
    "        \n",
    "        train_set = VideoDatasetLMDB(train_csv, CACHE_DIR / f\"lmdb_{train_csv.stem}_frame_{NUM_FRAMES}\", num_frames=num_frames)\n",
    "        val_set = VideoDatasetLMDB(val_csv, CACHE_DIR / f\"lmdb_{val_csv.stem}_frame_{NUM_FRAMES}\", num_frames=num_frames)\n",
    "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        \n",
    "        final_model = LSTMModel(feature_dim=1536, hidden_size=hidden_size, num_lstm_layers=num_lstm_layers).to(device)\n",
    "        print(f\"Final Training: batch_size={batch_size}, lr={lr:.0e}, hidden_size={hidden_size}\")\n",
    "        try:\n",
    "            train_model(final_model, train_loader, val_loader, epochs, lr, final_checkpoint,\n",
    "                        patience=5, gradient_accum_steps=GRADIENT_ACCUM_STEPS)\n",
    "        except RuntimeError as e:\n",
    "            print(\"Final Training failed:\", e)\n",
    "            exit(1)\n",
    "        if not final_checkpoint.exists():\n",
    "            print(\"Final checkpoint not created; exiting.\")\n",
    "            exit(1)\n",
    "    else:\n",
    "        print(\"\\n--- Skipping Final Training (Checkpoint Exists) ---\")\n",
    "        print(f\"Using model from: {final_checkpoint}\")\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Evaluation on Test Set (LMDB Dataset)\n",
    "    # ------------------------------\n",
    "    # For evaluation, use the best tuned batch size \n",
    "    best_params = study.best_trial.params\n",
    "    eval_batch_size = best_params.get(\"batch_size\", 8)\n",
    "    test_set = VideoDatasetLMDB(test_csv, CACHE_DIR / f\"lmdb_{test_csv.stem}_frame_{NUM_FRAMES}\", num_frames=NUM_FRAMES)\n",
    "    test_loader = DataLoader(test_set, batch_size=eval_batch_size, shuffle=False, num_workers=2)\n",
    "    eval_model = LSTMModel(feature_dim=1536, hidden_size=best_params.get(\"hidden_size\", 128),\n",
    "                           num_lstm_layers=best_params.get(\"num_lstm_layers\", 1)).to(device)\n",
    "    if final_checkpoint.exists():\n",
    "        state = torch.load(final_checkpoint, map_location=device)\n",
    "        eval_model.load_state_dict(state['model_state_dict'])\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Final checkpoint not found!\")\n",
    "    eval_model.to(device)\n",
    "    try:\n",
    "        evaluate_model(eval_model, test_loader)\n",
    "    except RuntimeError as e:\n",
    "        print(\"Evaluation failed due to CUDA memory issues:\", e)\n",
    "        exit(1)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"\\n--- Evaluation Complete ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
