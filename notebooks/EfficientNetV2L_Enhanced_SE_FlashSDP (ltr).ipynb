{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models directory exists: True\n",
      "Checkpoint path writable: True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import torch\n",
    "import optuna\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import lmdb  # pip install lmdb\n",
    "import logging\n",
    "import sqlite3\n",
    "from optuna.pruners import MedianPruner\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "import matplotlib.pyplot as plt\n",
    "import timm  # pip install timm\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Enable PyTorch built-in Flash SDP (which uses FlashAttention when conditions are met)\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# ------------------------------\n",
    "# CONSTANTS & HYPERPARAMETERS\n",
    "# ------------------------------\n",
    "GRADIENT_ACCUM_STEPS = 8\n",
    "NUM_FRAMES = 50\n",
    "# Progressive schedule: first train at 224x224 then fine-tune at 300x300.\n",
    "PROG_SCHEDULE = [(224, 15), (300, 10)]\n",
    "FOCAL_ALPHA = 0.25\n",
    "FOCAL_GAMMA = 2.0\n",
    "\n",
    "# ------------------------------\n",
    "# Environment & Paths\n",
    "# ------------------------------\n",
    "BASE_DIR = Path(\"C:/Users/abhis/Downloads/Documents/Learner Engagement Project\")\n",
    "DATA_DIR = BASE_DIR / \"data\" / \"DAiSEE\"\n",
    "FRAMES_DIR = DATA_DIR / \"ExtractedFrames\"\n",
    "LABELS_DIR = DATA_DIR / \"Labels\"\n",
    "MODEL_DIR = BASE_DIR / \"models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR = BASE_DIR / \"cache\"\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Models directory exists:\", os.path.exists(MODEL_DIR))\n",
    "print(\"Checkpoint path writable:\", os.access(MODEL_DIR, os.W_OK))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ------------------------------\n",
    "# Custom Collate Function\n",
    "# ------------------------------\n",
    "def custom_collate_fn(batch):\n",
    "    features, labels = zip(*batch)\n",
    "    features = torch.stack(features, dim=0)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    return features, labels\n",
    "\n",
    "# ------------------------------\n",
    "# Image Transformations\n",
    "# ------------------------------\n",
    "def get_transform(resolution):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Utility Functions\n",
    "# ------------------------------\n",
    "def get_csv_clip_id(video_stem: str) -> str:\n",
    "    base = video_stem.strip()\n",
    "    return base.replace(\"110001\", \"202614\", 1) if base.startswith(\"110001\") else base\n",
    "\n",
    "def select_impactful_frames(video_folder: Path, num_frames=50):\n",
    "    frame_files = sorted(video_folder.glob(\"frame_*.jpg\"))\n",
    "    total = len(frame_files)\n",
    "    if total == 0:\n",
    "        return []\n",
    "    if total <= num_frames:\n",
    "        return frame_files\n",
    "    indices = np.linspace(0, total - 1, num_frames, dtype=int)\n",
    "    return [frame_files[i] for i in indices]\n",
    "\n",
    "# ------------------------------\n",
    "# Precomputation & LMDB Caching Functions\n",
    "# ------------------------------\n",
    "def precompute_best_frames(csv_file: Path, video_root: Path, num_frames=50, resolution=224):\n",
    "    data = pd.read_csv(csv_file, dtype=str)\n",
    "    data.columns = data.columns.str.strip()\n",
    "    split = csv_file.stem.replace(\"Labels\", \"\").strip()\n",
    "    valid_indices = []\n",
    "    precomputed = []\n",
    "    skipped = 0\n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data),\n",
    "                         desc=f\"Precomputing frames for {csv_file.stem} at {resolution}x{resolution}\"):\n",
    "        clip_id = get_csv_clip_id(row[\"ClipID\"].split('.')[0])\n",
    "        video_folder = video_root / split / clip_id\n",
    "        if video_folder.exists():\n",
    "            frames = select_impactful_frames(video_folder, num_frames)\n",
    "            if len(frames) >= num_frames:\n",
    "                precomputed.append(frames[:num_frames])\n",
    "                valid_indices.append(idx)\n",
    "            else:\n",
    "                skipped += 1\n",
    "        else:\n",
    "            skipped += 1\n",
    "    print(f\"Precomputation: Skipped {skipped} videos out of {len(data)}.\")\n",
    "    cache_data = {\"valid_indices\": valid_indices, \"precomputed_frames\": precomputed}\n",
    "    cache_file = CACHE_DIR / f\"precomputed_{csv_file.stem}_frame_{num_frames}_{resolution}.pkl\"\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(cache_data, f)\n",
    "    print(f\"Precomputed results saved to {cache_file}\")\n",
    "    return cache_data\n",
    "\n",
    "def convert_pkl_to_lmdb(csv_file: Path, num_frames=50, resolution=224,\n",
    "                          transform=None, lmdb_map_size=1 * 1024**3):\n",
    "    if transform is None:\n",
    "        transform = get_transform(resolution)\n",
    "    pkl_file = CACHE_DIR / f\"precomputed_{csv_file.stem}_frame_{num_frames}_{resolution}.pkl\"\n",
    "    lmdb_path = CACHE_DIR / f\"lmdb_{csv_file.stem}_frame_{num_frames}_{resolution}\"\n",
    "    if (lmdb_path / \"data.mdb\").exists():\n",
    "        print(f\"LMDB database already exists at {lmdb_path}\")\n",
    "        return lmdb_path\n",
    "    env = lmdb.open(str(lmdb_path), map_size=lmdb_map_size)\n",
    "    if not pkl_file.exists():\n",
    "        precompute_best_frames(csv_file, FRAMES_DIR, num_frames=num_frames, resolution=resolution)\n",
    "    with open(pkl_file, \"rb\") as f:\n",
    "        cache = pickle.load(f)\n",
    "    valid_indices = cache[\"valid_indices\"]\n",
    "    file_paths_list = cache[\"precomputed_frames\"]\n",
    "    backbone = timm.create_model(\"tf_efficientnetv2_l\", pretrained=True)\n",
    "    backbone.reset_classifier(0)\n",
    "    backbone.eval()\n",
    "    backbone.to(device)\n",
    "    for param in backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(f\"Converting frame paths to LMDB features for {csv_file.stem} at {resolution}x{resolution} ...\")\n",
    "    with env.begin(write=True) as txn:\n",
    "        for idx, paths in tqdm(enumerate(file_paths_list), total=len(file_paths_list)):\n",
    "            video_features = []\n",
    "            for fp in paths:\n",
    "                try:\n",
    "                    img = Image.open(fp).convert(\"RGB\")\n",
    "                except Exception:\n",
    "                    img = Image.new('RGB', (resolution, resolution))\n",
    "                tensor = transform(img).unsqueeze(0).to(device)\n",
    "                with torch.no_grad(), autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    feat = backbone(tensor)\n",
    "                    feat = feat.squeeze(0).cpu().half().detach()\n",
    "                video_features.append(feat.numpy())\n",
    "            if video_features:\n",
    "                video_features_np = np.stack(video_features)\n",
    "                key = f\"video_{valid_indices[idx]}\".encode(\"utf-8\")\n",
    "                txn.put(key, pickle.dumps(video_features_np))\n",
    "    env.close()\n",
    "    print(f\"LMDB database created at {lmdb_path}\")\n",
    "    return lmdb_path\n",
    "\n",
    "# ------------------------------\n",
    "# LMDB Dataset Classes\n",
    "# ------------------------------\n",
    "class VideoDatasetLMDB(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, lmdb_path, num_frames=50, resolution=224):\n",
    "        self.data = pd.read_csv(csv_file, dtype=str)\n",
    "        self.data.columns = self.data.columns.str.strip()\n",
    "        self.resolution = resolution\n",
    "        pkl_file = CACHE_DIR / f\"precomputed_{csv_file.stem}_frame_{num_frames}_{resolution}.pkl\"\n",
    "        with open(pkl_file, \"rb\") as f:\n",
    "            cache = pickle.load(f)\n",
    "        self.valid_indices = cache[\"valid_indices\"]\n",
    "        self.data = self.data.iloc[self.valid_indices].reset_index(drop=True)\n",
    "        self.num_frames = num_frames\n",
    "        self.lmdb_path = str(lmdb_path)\n",
    "        self.env = None\n",
    "    def _init_env(self):\n",
    "        if self.env is None:\n",
    "            self.env = lmdb.open(self.lmdb_path, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "        return self.env\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        env = self._init_env()\n",
    "        original_idx = self.valid_indices[idx]\n",
    "        key = f\"video_{original_idx}\".encode(\"utf-8\")\n",
    "        with env.begin(write=False) as txn:\n",
    "            data_bytes = txn.get(key)\n",
    "            if data_bytes is None:\n",
    "                raise IndexError(f\"Key {key} not found in LMDB\")\n",
    "            features_np = pickle.loads(data_bytes)\n",
    "            features = torch.from_numpy(features_np)\n",
    "        labels_array = np.array(self.data.iloc[idx][[\"Engagement\", \"Boredom\", \"Confusion\", \"Frustration\"]].tolist(), dtype=np.int64)\n",
    "        labels_tensor = torch.tensor(labels_array, dtype=torch.long)\n",
    "        return features, labels_tensor\n",
    "\n",
    "class VideoDatasetRaw(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, video_root, num_frames=50, transform=None, split=\"train\"):\n",
    "        self.data = pd.read_csv(csv_file, dtype=str)\n",
    "        self.data.columns = self.data.columns.str.strip()\n",
    "        self.split = split\n",
    "        pkl_file = CACHE_DIR / f\"precomputed_{csv_file.stem}_frame_{num_frames}_raw.pkl\"\n",
    "        if not pkl_file.exists():\n",
    "            cache = precompute_best_frames(csv_file, video_root, num_frames=num_frames)\n",
    "            with open(pkl_file, \"wb\") as f:\n",
    "                pickle.dump(cache, f)\n",
    "        else:\n",
    "            with open(pkl_file, \"rb\") as f:\n",
    "                cache = pickle.load(f)\n",
    "        self.valid_indices = cache[\"valid_indices\"]\n",
    "        self.file_paths = cache[\"precomputed_frames\"]\n",
    "        self.data = self.data.iloc[self.valid_indices].reset_index(drop=True)\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        paths = self.file_paths[idx]\n",
    "        frames = []\n",
    "        for fp in paths:\n",
    "            try:\n",
    "                img = Image.open(fp).convert(\"RGB\")\n",
    "            except Exception:\n",
    "                img = Image.new('RGB', (self.transform.transforms[0].size, self.transform.transforms[0].size))\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            frames.append(img)\n",
    "        video_tensor = torch.stack(frames)\n",
    "        labels_array = np.array(self.data.iloc[idx][[\"Engagement\", \"Boredom\", \"Confusion\", \"Frustration\"]].tolist(), dtype=np.int64)\n",
    "        labels_tensor = torch.tensor(labels_array, dtype=torch.long)\n",
    "        return video_tensor, labels_tensor\n",
    "\n",
    "# ------------------------------\n",
    "# Class-Specific Focal Loss\n",
    "# ------------------------------\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        if alpha is not None:\n",
    "            self.alpha = torch.tensor(alpha, dtype=torch.float32)\n",
    "        else:\n",
    "            self.alpha = None\n",
    "        self.gamma = torch.tensor(gamma, dtype=torch.float32)\n",
    "        self.reduction = reduction\n",
    "        self.ce = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    def forward(self, logits, target):\n",
    "        num_classes = logits.size(1)\n",
    "        target_onehot = F.one_hot(target, num_classes=num_classes).float()\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pt = (probs * target_onehot).sum(dim=1)\n",
    "        logpt = torch.log(pt + 1e-8)\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = (self.alpha.to(logits.device) * target_onehot).sum(dim=1)\n",
    "        else:\n",
    "            alpha_t = 1.0\n",
    "        if self.gamma.numel() > 1:\n",
    "            gamma_t = (self.gamma.to(logits.device) * target_onehot).sum(dim=1)\n",
    "        else:\n",
    "            gamma_t = self.gamma.to(logits.device)\n",
    "        loss = -alpha_t * ((1 - pt) ** gamma_t) * logpt\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "# Create per-head focal loss functions\n",
    "loss_engagement  = FocalLoss(alpha=[1.0, 1.0, 1.0, 1.0], gamma=2.5)\n",
    "loss_boredom     = FocalLoss(alpha=[1.0, 1.0, 1.0, 1.0], gamma=2.0)\n",
    "loss_confusion   = FocalLoss(alpha=[1.0, 1.0, 1.0, 1.0], gamma=1.5)\n",
    "loss_frustration = FocalLoss(alpha=[1.0, 1.0, 1.0, 1.0], gamma=2.0)\n",
    "\n",
    "# ------------------------------\n",
    "# Temporal CutMix Data Augmentation\n",
    "# ------------------------------\n",
    "class TemporalCutMix(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def forward(self, video_batch, labels=None):\n",
    "        if torch.rand(1).item() > self.p:\n",
    "            return video_batch, labels\n",
    "        B, T, C, H, W = video_batch.shape\n",
    "        rand_indices = torch.randperm(B, device=video_batch.device)\n",
    "        t_start = torch.randint(0, T-10, (B,), device=video_batch.device)\n",
    "        t_end = t_start + 10\n",
    "        for i in range(B):\n",
    "            video_batch[i, t_start[i]:t_end[i]] = video_batch[rand_indices[i], t_start[i]:t_end[i]]\n",
    "            if labels is not None:\n",
    "                labels[i, t_start[i]:t_end[i]] = labels[rand_indices[i], t_start[i]:t_end[i]]\n",
    "        return video_batch, labels\n",
    "\n",
    "# ------------------------------\n",
    "# Spatial SE Module (Replacing CBAM)\n",
    "# ------------------------------\n",
    "class SpatialSELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x).transpose(1, 2)\n",
    "        y = self.fc(y).transpose(1, 2)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "# ------------------------------\n",
    "# Temporal Module using PyTorch’s Built-In Flash Attention\n",
    "# ------------------------------\n",
    "class TemporalFlashSDPModule(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        super(TemporalFlashSDPModule, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=2,\n",
    "                            bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "        self.layer_norm = nn.LayerNorm(2 * hidden_size)\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  # (B, T, 2*hidden_size)\n",
    "        attn_out = F.scaled_dot_product_attention(lstm_out, lstm_out, lstm_out, dropout_p=0.0, is_causal=False)\n",
    "        out = self.layer_norm(lstm_out + attn_out)\n",
    "        return out\n",
    "\n",
    "# ------------------------------\n",
    "# Enhanced Model Architecture using PyTorch Built-In Flash Attention\n",
    "# ------------------------------\n",
    "class EfficientNetV2L_Enhanced(nn.Module):\n",
    "    def __init__(self, lstm_hidden=512, dropout_rate=0.5, n_heads=4):\n",
    "        super(EfficientNetV2L_Enhanced, self).__init__()\n",
    "        # Backbone: EfficientNetV2-L with minimal freezing\n",
    "        self.backbone = timm.create_model(\"tf_efficientnetv2_l\", pretrained=True)\n",
    "        self.backbone.reset_classifier(0)\n",
    "        if hasattr(self.backbone, \"blocks\"):\n",
    "            for i, block in enumerate(self.backbone.blocks):\n",
    "                if i < 6:\n",
    "                    for param in block.parameters():\n",
    "                        param.requires_grad = False\n",
    "        if hasattr(self.backbone, \"num_features\"):\n",
    "            self.feature_dim = self.backbone.num_features\n",
    "        else:\n",
    "            self.feature_dim = 1536\n",
    "        # Use SE for spatial recalibration\n",
    "        self.se = SpatialSELayer(channel=self.feature_dim, reduction=16)\n",
    "        # Temporal module: BiLSTM + Flash-based attention\n",
    "        self.temporal_module = TemporalFlashSDPModule(input_dim=self.feature_dim, hidden_size=lstm_hidden)\n",
    "        self.d_model = 2 * lstm_hidden\n",
    "        # Project global spatial feature to d_model\n",
    "        self.spatial_proj = nn.Linear(self.feature_dim, self.d_model)\n",
    "        # Multi-head attention pooling\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=self.d_model, num_heads=n_heads, batch_first=True)\n",
    "        self.ln_fusion = nn.LayerNorm(self.d_model * 2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(self.d_model * 2, 16)  # 16 logits, to be reshaped into (4,4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.dim() == 5:\n",
    "            B, T, C, H, W = x.size()\n",
    "            x = x.view(-1, C, H, W)\n",
    "            features = self.backbone(x)\n",
    "            features = features.view(B, T, self.feature_dim)\n",
    "        elif x.dim() == 3:\n",
    "            features = x\n",
    "            B, T, _ = features.size()\n",
    "        else:\n",
    "            raise ValueError(\"Input tensor must have 3 or 5 dimensions.\")\n",
    "        # Apply SE: permute to (B, C, T)\n",
    "        features_se = self.se(features.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        global_spatial = features_se.mean(dim=1)  # (B, feature_dim)\n",
    "        global_spatial_proj = self.spatial_proj(global_spatial)  # (B, d_model)\n",
    "        temporal_features = self.temporal_module(features_se)  # (B, T, d_model)\n",
    "        # Use global spatial as query for attention pooling\n",
    "        query = global_spatial_proj.unsqueeze(1)  # (B, 1, d_model)\n",
    "        attn_out, _ = self.cross_attn(query, temporal_features, temporal_features)\n",
    "        attn_out = attn_out.squeeze(1)  # (B, d_model)\n",
    "        fusion = torch.cat((global_spatial_proj, attn_out), dim=1)  # (B, 2*d_model)\n",
    "        fusion = self.ln_fusion(fusion)\n",
    "        fusion = self.dropout(fusion)\n",
    "        logits = self.classifier(fusion)  # (B, 16)\n",
    "        # Reshape logits to (B, 4, 4) and return as 4 separate outputs\n",
    "        B = logits.size(0)\n",
    "        logits = logits.view(B, 4, 4)\n",
    "        # Return separate outputs for each affective state\n",
    "        return logits[:, 0], logits[:, 1], logits[:, 2], logits[:, 3]\n",
    "\n",
    "# ------------------------------\n",
    "# Training Function (Progressive Resolution, Mixed Precision, Grad Accumulation)\n",
    "# ------------------------------\n",
    "def progressive_train_model(model, total_epochs, lr, checkpoint_path, batch_size,\n",
    "                            patience=5, gradient_accum_steps=GRADIENT_ACCUM_STEPS):\n",
    "    backbone_params = list(model.backbone.parameters())\n",
    "    backbone_param_ids = {id(p) for p in backbone_params}\n",
    "    other_params = [p for p in model.parameters() if id(p) not in backbone_param_ids]\n",
    "    optimizer = optim.AdamW([\n",
    "        {\"params\": backbone_params, \"lr\": 1e-5},\n",
    "        {\"params\": other_params, \"lr\": lr}\n",
    "    ], weight_decay=1e-4)\n",
    "    scaler = GradScaler()\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    current_epoch = 0\n",
    "    for res, ep in PROG_SCHEDULE:\n",
    "        transform = get_transform(res)\n",
    "        train_lmdb = convert_pkl_to_lmdb(train_csv, num_frames=NUM_FRAMES, resolution=res, transform=transform)\n",
    "        val_lmdb = convert_pkl_to_lmdb(val_csv, num_frames=NUM_FRAMES, resolution=res, transform=transform)\n",
    "        train_set = VideoDatasetLMDB(train_csv, train_lmdb, num_frames=NUM_FRAMES, resolution=res)\n",
    "        val_set = VideoDatasetLMDB(val_csv, val_lmdb, num_frames=NUM_FRAMES, resolution=res)\n",
    "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, collate_fn=custom_collate_fn)\n",
    "        val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True, collate_fn=custom_collate_fn)\n",
    "        for epoch in range(ep):\n",
    "            print(f\"Progressive Training: Epoch {current_epoch+1}/{total_epochs} at resolution {res}x{res}\")\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for i, (features, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "                features = features.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "                if labels.dim() == 1:\n",
    "                    labels = labels.unsqueeze(0)\n",
    "                with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    out_eng, out_bor, out_conf, out_frust = model(features)\n",
    "                    loss = (loss_engagement(out_eng, labels[:, 0]) +\n",
    "                            loss_boredom(out_bor, labels[:, 1]) +\n",
    "                            loss_confusion(out_conf, labels[:, 2]) +\n",
    "                            loss_frustration(out_frust, labels[:, 3])) / 4.0\n",
    "                scaler.scale(loss / gradient_accum_steps).backward()\n",
    "                if (i + 1) % gradient_accum_steps == 0:\n",
    "                    scaler.step(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "                running_loss += loss.item() * features.size(0)\n",
    "                del features, labels, out_eng, out_bor, out_conf, out_frust, loss\n",
    "                if (i + 1) % 30 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "            train_loss = running_loss / len(train_loader.dataset)\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad(), autocast(device_type='cuda', dtype=torch.float16):\n",
    "                for features, labels in val_loader:\n",
    "                    features = features.to(device, non_blocking=True)\n",
    "                    labels = labels.to(device, non_blocking=True)\n",
    "                    if labels.dim() == 1:\n",
    "                        labels = labels.unsqueeze(0)\n",
    "                    out_eng, out_bor, out_conf, out_frust = model(features)\n",
    "                    loss = (loss_engagement(out_eng, labels[:, 0]) +\n",
    "                            loss_boredom(out_bor, labels[:, 1]) +\n",
    "                            loss_confusion(out_conf, labels[:, 2]) +\n",
    "                            loss_frustration(out_frust, labels[:, 3])) / 4.0\n",
    "                    val_loss += loss.item() * features.size(0)\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            print(f\"Epoch {current_epoch+1}/{total_epochs} at {res}x{res} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                state = {\n",
    "                    \"epoch\": current_epoch + 1,\n",
    "                    \"best_val_loss\": best_val_loss,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict()\n",
    "                }\n",
    "                temp_path = checkpoint_path.with_suffix(\".tmp\")\n",
    "                torch.save(state, temp_path, _use_new_zipfile_serialization=False)\n",
    "                if checkpoint_path.exists():\n",
    "                    checkpoint_path.unlink()\n",
    "                temp_path.rename(checkpoint_path)\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {current_epoch+1}. Best val loss: {best_val_loss:.4f}\")\n",
    "                return best_val_loss\n",
    "            current_epoch += 1\n",
    "    return best_val_loss\n",
    "\n",
    "# ------------------------------\n",
    "# Evaluation Function\n",
    "# ------------------------------\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = {0:[], 1:[], 2:[], 3:[]}\n",
    "    all_labels = {0:[], 1:[], 2:[], 3:[]}\n",
    "    with torch.no_grad(), autocast(device_type='cuda', dtype=torch.float16):\n",
    "        for frames, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            frames = frames.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            if labels.dim() == 1:\n",
    "                labels = labels.unsqueeze(0)\n",
    "            out_eng, out_bor, out_conf, out_frust = model(frames)\n",
    "            pred_eng = torch.argmax(out_eng, dim=1)\n",
    "            pred_bor = torch.argmax(out_bor, dim=1)\n",
    "            pred_conf = torch.argmax(out_conf, dim=1)\n",
    "            pred_frust = torch.argmax(out_frust, dim=1)\n",
    "            all_preds[0].append(pred_eng.cpu())\n",
    "            all_preds[1].append(pred_bor.cpu())\n",
    "            all_preds[2].append(pred_conf.cpu())\n",
    "            all_preds[3].append(pred_frust.cpu())\n",
    "            all_labels[0].append(labels[:, 0].cpu())\n",
    "            all_labels[1].append(labels[:, 1].cpu())\n",
    "            all_labels[2].append(labels[:, 2].cpu())\n",
    "            all_labels[3].append(labels[:, 3].cpu())\n",
    "    for k in all_preds.keys():\n",
    "        all_preds[k] = torch.cat(all_preds[k]).numpy()\n",
    "        all_labels[k] = torch.cat(all_labels[k]).numpy()\n",
    "    for i, state in enumerate([\"Engagement\", \"Boredom\", \"Confusion\", \"Frustration\"]):\n",
    "        print(f\"Classification report for {state}:\")\n",
    "        print(classification_report(all_labels[i], all_preds[i], digits=3))\n",
    "        cm = confusion_matrix(all_labels[i], all_preds[i])\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Confusion Matrix for {state}\")\n",
    "        plt.colorbar()\n",
    "        plt.xticks(np.arange(cm.shape[0]), np.arange(cm.shape[0]))\n",
    "        plt.yticks(np.arange(cm.shape[1]), np.arange(cm.shape[1]))\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing frames for TrainLabels at 224x224: 100%|██████████| 5358/5358 [00:18<00:00, 295.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputation: Skipped 507 videos out of 5358.\n",
      "Precomputed results saved to C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\precomputed_TrainLabels_frame_50_224.pkl\n",
      "LMDB database already exists at C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\lmdb_TrainLabels_frame_50_224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing frames for TrainLabels at 300x300: 100%|██████████| 5358/5358 [00:17<00:00, 312.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputation: Skipped 507 videos out of 5358.\n",
      "Precomputed results saved to C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\precomputed_TrainLabels_frame_50_300.pkl\n",
      "LMDB database already exists at C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\lmdb_TrainLabels_frame_50_300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing frames for ValidationLabels at 224x224: 100%|██████████| 1429/1429 [00:04<00:00, 288.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputation: Skipped 0 videos out of 1429.\n",
      "Precomputed results saved to C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\precomputed_ValidationLabels_frame_50_224.pkl\n",
      "LMDB database already exists at C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\lmdb_ValidationLabels_frame_50_224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing frames for ValidationLabels at 300x300: 100%|██████████| 1429/1429 [00:05<00:00, 278.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputation: Skipped 0 videos out of 1429.\n",
      "Precomputed results saved to C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\precomputed_ValidationLabels_frame_50_300.pkl\n",
      "LMDB database already exists at C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\lmdb_ValidationLabels_frame_50_300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing frames for TestLabels at 224x224: 100%|██████████| 1784/1784 [00:05<00:00, 323.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputation: Skipped 146 videos out of 1784.\n",
      "Precomputed results saved to C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\precomputed_TestLabels_frame_50_224.pkl\n",
      "LMDB database already exists at C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\lmdb_TestLabels_frame_50_224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing frames for TestLabels at 300x300: 100%|██████████| 1784/1784 [00:05<00:00, 306.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputation: Skipped 146 videos out of 1784.\n",
      "Precomputed results saved to C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\precomputed_TestLabels_frame_50_300.pkl\n",
      "LMDB database already exists at C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\cache\\lmdb_TestLabels_frame_50_300\n",
      "Database created/connected at: C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\notebooks\\tuning_eff_v2l_enhanced_flashSDP.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-02 12:26:49,182] Using an existing study with name 'efficientnetv2l_enhanced_study' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna tuning complete. Total successful trials: 30\n",
      "Best trial parameters: {'batch_size': 8, 'lr': 0.00019026855481898958, 'lstm_hidden': 512, 'n_heads': 4, 'dropout_rate': 0.6}\n",
      "\n",
      "--- Skipping Final Training (Checkpoint Exists) ---\n",
      "Using existing model from: C:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\models\\final_model_eff_v2l_enhanced_flashSDP_checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/tf_efficientnetv2_l.in21k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/tf_efficientnetv2_l.in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Evaluating:   1%|          | 4/410 [00:53<1:33:38, 13.84s/it]"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Main Execution\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    torch.multiprocessing.set_start_method('spawn')\n",
    "    # CSV file paths\n",
    "    train_csv = LABELS_DIR / \"TrainLabels.csv\"\n",
    "    val_csv = LABELS_DIR / \"ValidationLabels.csv\"\n",
    "    test_csv = LABELS_DIR / \"TestLabels.csv\"\n",
    "    \n",
    "    # Precompute caches and LMDB databases for resolutions 224 and 300\n",
    "    resolutions = [224, 300]\n",
    "    for csv in [train_csv, val_csv, test_csv]:\n",
    "        for res in resolutions:\n",
    "            precompute_best_frames(csv, FRAMES_DIR, num_frames=NUM_FRAMES, resolution=res)\n",
    "            convert_pkl_to_lmdb(csv, num_frames=NUM_FRAMES, resolution=res,\n",
    "                                transform=get_transform(res), lmdb_map_size=1 * 1024**3)\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Hyperparameter Tuning using Progressive Training\n",
    "    # ------------------------------\n",
    "    def objective(trial):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [4, 8])\n",
    "        lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "        lstm_hidden = trial.suggest_categorical(\"lstm_hidden\", [512, 768])\n",
    "        n_heads = trial.suggest_categorical(\"n_heads\", [4, 8])\n",
    "        dropout_rate = trial.suggest_categorical(\"dropout_rate\", [0.5, 0.6])\n",
    "        total_epochs = sum(eps for _, eps in PROG_SCHEDULE)\n",
    "        model = EfficientNetV2L_Enhanced(lstm_hidden=lstm_hidden, n_heads=n_heads, dropout_rate=dropout_rate).to(device)\n",
    "        trial_checkpoint = MODEL_DIR / f\"trial_eff_v2l_{trial.number}_enhanced_flashSDP_checkpoint.pth\"\n",
    "        trial_checkpoint.parent.mkdir(parents=True, exist_ok=True)\n",
    "        loss = progressive_train_model(model, total_epochs, lr, trial_checkpoint, batch_size,\n",
    "                                       patience=3, gradient_accum_steps=GRADIENT_ACCUM_STEPS)\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        return loss\n",
    "\n",
    "    db_path = BASE_DIR / \"notebooks\" / \"tuning_eff_v2l_enhanced_flashSDP.db\"\n",
    "    db_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        print(f\"Database created/connected at: {db_path}\")\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"DB Error: {e}\")\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        pruner=MedianPruner(n_startup_trials=2, n_warmup_steps=10),\n",
    "        study_name=\"efficientnetv2l_enhanced_study\",\n",
    "        storage=f\"sqlite:///{db_path}\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    target_trials = 30\n",
    "    while True:\n",
    "        successes = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE and np.isfinite(t.value)]\n",
    "        remaining = target_trials - len(successes)\n",
    "        if remaining <= 0:\n",
    "            break\n",
    "        print(f\"Running {remaining} additional trial(s) to reach {target_trials} successful trials...\")\n",
    "        study.optimize(objective, n_trials=remaining, catch=(Exception,))\n",
    "    print(f\"Optuna tuning complete. Total successful trials: {len(successes)}\")\n",
    "    best_trial = min(successes, key=lambda t: t.value)\n",
    "    print(f\"Best trial parameters: {best_trial.params}\")\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Final Training (using raw images for end-to-end fine-tuning)\n",
    "    # ------------------------------\n",
    "    total_epochs = sum(eps for _, eps in PROG_SCHEDULE)\n",
    "    final_checkpoint = MODEL_DIR / \"final_model_eff_v2l_enhanced_flashSDP_checkpoint.pth\"\n",
    "    if not final_checkpoint.exists():\n",
    "        print(\"\\n--- Starting Final Training ---\")\n",
    "        params = best_trial.params\n",
    "        batch_size = params.get(\"batch_size\", 4)\n",
    "        lr = params.get(\"lr\", 1e-4)\n",
    "        lstm_hidden = params.get(\"lstm_hidden\", 512)\n",
    "        n_heads = params.get(\"n_heads\", 4)\n",
    "        dropout_rate = params.get(\"dropout_rate\", 0.5)\n",
    "        final_model = EfficientNetV2L_Enhanced(lstm_hidden=lstm_hidden, n_heads=n_heads, dropout_rate=dropout_rate).to(device)\n",
    "        # Unfreeze additional backbone layers for fine-tuning\n",
    "        if hasattr(final_model.backbone, \"blocks\"):\n",
    "            for i, block in enumerate(final_model.backbone.blocks):\n",
    "                if i >= 6:\n",
    "                    for param in block.parameters():\n",
    "                        param.requires_grad = True\n",
    "        final_checkpoint.parent.mkdir(parents=True, exist_ok=True)\n",
    "        final_loss = progressive_train_model(final_model, total_epochs, lr, final_checkpoint, batch_size,\n",
    "                                             patience=5, gradient_accum_steps=GRADIENT_ACCUM_STEPS)\n",
    "    else:\n",
    "        print(\"\\n--- Skipping Final Training (Checkpoint Exists) ---\")\n",
    "        print(f\"Using existing model from: {final_checkpoint}\")\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Evaluation on Test Set (300x300)\n",
    "    # ------------------------------\n",
    "    test_transform = get_transform(300)\n",
    "    test_set = VideoDatasetRaw(test_csv, FRAMES_DIR, num_frames=NUM_FRAMES, transform=test_transform)\n",
    "    test_loader = DataLoader(test_set, batch_size=4, shuffle=False, num_workers=0, pin_memory=True, collate_fn=custom_collate_fn)\n",
    "    eval_model = EfficientNetV2L_Enhanced(lstm_hidden=best_trial.params.get(\"lstm_hidden\", 512),\n",
    "                                           n_heads=best_trial.params.get(\"n_heads\", 4),\n",
    "                                           dropout_rate=best_trial.params.get(\"dropout_rate\", 0.5)).to(device)\n",
    "    state = torch.load(final_checkpoint, map_location=device)\n",
    "    eval_model.load_state_dict(state[\"model_state_dict\"])\n",
    "    eval_model.to(device)\n",
    "    evaluate_model(eval_model, test_loader)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"\\n--- Evaluation Complete ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
