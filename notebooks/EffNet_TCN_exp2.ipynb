{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import torch\n",
    "import optuna\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import io\n",
    "import lmdb\n",
    "import logging\n",
    "import sqlite3\n",
    "from optuna.pruners import MedianPruner\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch.amp import GradScaler, autocast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from datetime import datetime\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import torch.serialization\n",
    "import math\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "# Make sure certain numpy serialization is safe\n",
    "torch.serialization.add_safe_globals(['numpy.core.multiarray.scalar'])\n",
    "\n",
    "# ------------------------------\n",
    "# CONSTANTS & HYPERPARAMETERS\n",
    "# ------------------------------\n",
    "NUM_FRAMES = 50  # Match the existing LMDB databases\n",
    "FOCAL_ALPHA = 0.25\n",
    "FOCAL_GAMMA = 2.0\n",
    "LABEL_SMOOTHING = 0.1\n",
    "GRADIENT_ACCUM_STEPS = 4  \n",
    "\n",
    "# ------------------------------\n",
    "# Environment & Paths\n",
    "# ------------------------------\n",
    "BASE_DIR = Path(\"C:/Users/abhis/Downloads/Documents/Learner Engagement Project\")\n",
    "DATA_DIR = BASE_DIR / \"data\" / \"DAiSEE\"\n",
    "FRAMES_DIR = DATA_DIR / \"ExtractedFrames\"\n",
    "LABELS_DIR = DATA_DIR / \"Labels\"\n",
    "MODEL_DIR = BASE_DIR / \"models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR = BASE_DIR / \"cache\"\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Models directory exists:\", os.path.exists(MODEL_DIR))\n",
    "print(\"Checkpoint path writable:\", os.access(MODEL_DIR, os.W_OK))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# CSV file paths\n",
    "train_csv = LABELS_DIR / \"TrainLabels.csv\"\n",
    "val_csv = LABELS_DIR / \"ValidationLabels.csv\"\n",
    "test_csv = LABELS_DIR / \"TestLabels.csv\"\n",
    "\n",
    "# ------------------------------\n",
    "# LMDB Dataset Class\n",
    "# ------------------------------\n",
    "class VideoDatasetLMDB(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, lmdb_path, num_frames=50, resolution=224):\n",
    "        self.data = pd.read_csv(csv_file, dtype=str)\n",
    "        self.data.columns = self.data.columns.str.strip()\n",
    "        self.resolution = resolution\n",
    "        pkl_file = CACHE_DIR / f\"precomputed_{Path(csv_file).stem}_frame_{num_frames}_{resolution}.pkl\"\n",
    "        with open(pkl_file, \"rb\") as f:\n",
    "            cache = pickle.load(f)\n",
    "        self.valid_indices = cache[\"valid_indices\"]\n",
    "        self.data = self.data.iloc[self.valid_indices].reset_index(drop=True)\n",
    "        self.num_frames = num_frames\n",
    "        self.lmdb_path = str(lmdb_path)\n",
    "        # Don't initialize env in __init__\n",
    "        self.env = None\n",
    "        self._pid = None  # Track process ID\n",
    "\n",
    "    def _init_env(self):\n",
    "        # Only initialize if not initialized or if we're in a different process\n",
    "        current_pid = os.getpid()\n",
    "        if self.env is None or self._pid != current_pid:\n",
    "            if self.env is not None:\n",
    "                self.env.close()\n",
    "            self.env = lmdb.open(self.lmdb_path, readonly=True, lock=False, \n",
    "                                 readahead=False, meminit=False)\n",
    "            self._pid = current_pid\n",
    "        return self.env\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            env = self._init_env()\n",
    "            original_idx = self.valid_indices[idx]\n",
    "            key = f\"video_{original_idx}\".encode(\"utf-8\")\n",
    "            with env.begin(write=False) as txn:\n",
    "                data_bytes = txn.get(key)\n",
    "                if data_bytes is None:\n",
    "                    raise IndexError(f\"Key {key} not found in LMDB\")\n",
    "                features_np = pickle.loads(data_bytes)\n",
    "                features = torch.from_numpy(features_np)\n",
    "            labels = self.data.iloc[idx][[\"Engagement\", \"Boredom\", \"Confusion\", \"Frustration\"]].astype(int)\n",
    "            return features, torch.tensor(labels.values, dtype=torch.long)\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing LMDB in __getitem__: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'env') and self.env is not None:\n",
    "            self.env.close()\n",
    "            self.env = None\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# SIMPLE BiGRU MODEL\n",
    "# ------------------------------\n",
    "class EfficientNetBasicBiGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    Very simple single-layer BiGRU-based model with smaller hidden dimension,\n",
    "    an input projection, plus separate classifiers for each emotion.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels=1280, hidden_dim=64, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(input_channels, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Single-layer BiGRU\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim // 2,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.classifiers = nn.ModuleDict({\n",
    "            'engagement': self._make_classifier(hidden_dim, dropout_rate),\n",
    "            'boredom': self._make_classifier(hidden_dim, dropout_rate),\n",
    "            'confusion': self._make_classifier(hidden_dim, dropout_rate),\n",
    "            'frustration': self._make_classifier(hidden_dim, dropout_rate)\n",
    "        })\n",
    "    \n",
    "    def _make_classifier(self, hidden_dim, dropout_rate):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 4),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)   # [batch_size, num_frames, hidden_dim]\n",
    "        x, _ = self.gru(x)       # [batch_size, num_frames, hidden_dim]\n",
    "        \n",
    "        # Combine last hidden state + max pooling\n",
    "        last_hidden = x[:, -1]\n",
    "        max_pool, _ = torch.max(x, dim=1)\n",
    "        combined = last_hidden + max_pool\n",
    "        \n",
    "        outputs = []\n",
    "        for emotion in ['engagement', 'boredom', 'confusion', 'frustration']:\n",
    "            outputs.append(self.classifiers[emotion](combined))\n",
    "        \n",
    "        return torch.stack(outputs, dim=1)  # [batch_size, 4, 4]\n",
    "    \n",
    "\n",
    "# ------------------------------\n",
    "# FAME Framework Components\n",
    "# ------------------------------\n",
    "class FeaturePipeline(nn.Module):\n",
    "    def __init__(self, input_dim=1280, feature_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Global feature enhancement\n",
    "        self.global_transform = nn.Sequential(\n",
    "            nn.Linear(input_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Emotion-specific mapping networks (one per emotion)\n",
    "        self.emotion_maps = nn.ModuleDict({\n",
    "            'engagement': nn.Sequential(\n",
    "                nn.Linear(input_dim, feature_dim // 2),\n",
    "                nn.LayerNorm(feature_dim // 2),\n",
    "                nn.GELU()\n",
    "            ),\n",
    "            'boredom': nn.Sequential(\n",
    "                nn.Linear(input_dim, feature_dim // 2),\n",
    "                nn.LayerNorm(feature_dim // 2),\n",
    "                nn.GELU()\n",
    "            ),\n",
    "            'confusion': nn.Sequential(\n",
    "                nn.Linear(input_dim, feature_dim // 2),\n",
    "                nn.LayerNorm(feature_dim // 2),\n",
    "                nn.GELU()\n",
    "            ),\n",
    "            'frustration': nn.Sequential(\n",
    "                nn.Linear(input_dim, feature_dim // 2),\n",
    "                nn.LayerNorm(feature_dim // 2),\n",
    "                nn.GELU()\n",
    "            )\n",
    "        })\n",
    "        \n",
    "    def forward(self, x, emotion=None):\n",
    "        # x shape: [batch_size, seq_len, input_dim]\n",
    "        # Global features\n",
    "        global_features = self.global_transform(x)\n",
    "        \n",
    "        # If specific emotion requested, return just that emotion's features\n",
    "        if emotion is not None:\n",
    "            emotion_features = self.emotion_maps[emotion](x)\n",
    "            return torch.cat([global_features, emotion_features], dim=-1)\n",
    "        \n",
    "        # Otherwise return features for all emotions\n",
    "        emotion_features = []\n",
    "        for emotion_name in ['engagement', 'boredom', 'confusion', 'frustration']:\n",
    "            emotion_feat = self.emotion_maps[emotion_name](x)\n",
    "            emotion_features.append(emotion_feat)\n",
    "            \n",
    "        return global_features, emotion_features\n",
    "\n",
    "\n",
    "class TemporalCrossAttention(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.query_proj = nn.Linear(feature_dim, feature_dim)\n",
    "        self.key_proj = nn.Linear(feature_dim, feature_dim)\n",
    "        self.value_proj = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "        self.scale = feature_dim ** -0.5\n",
    "        self.layer_norm = nn.LayerNorm(feature_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, feature_dim]\n",
    "        batch_size, seq_len, feature_dim = x.shape\n",
    "        \n",
    "        # Project to queries, keys, values\n",
    "        queries = self.query_proj(x)\n",
    "        keys = self.key_proj(x)\n",
    "        values = self.value_proj(x)\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.bmm(queries, keys.transpose(1, 2)) * self.scale\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Apply attention weights\n",
    "        context = torch.bmm(attn_weights, values)\n",
    "        \n",
    "        # Residual connection and layer norm\n",
    "        return self.layer_norm(x + context)\n",
    "\n",
    "\n",
    "class HierarchicalClassifier(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.coarse_classifiers = nn.ModuleDict({\n",
    "            'engagement': nn.Linear(feature_dim, 2),  # Low vs High\n",
    "            'boredom': nn.Linear(feature_dim, 2),\n",
    "            'confusion': nn.Linear(feature_dim, 2),\n",
    "            'frustration': nn.Linear(feature_dim, 2)\n",
    "        })\n",
    "        \n",
    "        self.fine_classifiers_low = nn.ModuleDict({\n",
    "            'engagement': nn.Linear(feature_dim, 2),  # Class 0 vs 1\n",
    "            'boredom': nn.Linear(feature_dim, 2),\n",
    "            'confusion': nn.Linear(feature_dim, 2),\n",
    "            'frustration': nn.Linear(feature_dim, 2)\n",
    "        })\n",
    "        \n",
    "        self.fine_classifiers_high = nn.ModuleDict({\n",
    "            'engagement': nn.Linear(feature_dim, 2),  # Class 2 vs 3\n",
    "            'boredom': nn.Linear(feature_dim, 2),\n",
    "            'confusion': nn.Linear(feature_dim, 2),\n",
    "            'frustration': nn.Linear(feature_dim, 2)\n",
    "        })\n",
    "        \n",
    "        # Class boosting factors for minority classes\n",
    "        self.class_boost = {\n",
    "            'engagement': torch.tensor([4.0, 1.0, 5.0, 5.0]),  # Strong boost to underrepresented classes\n",
    "            'boredom': torch.tensor([5.0, 4.0, 1.0, 1.0]),     # Strong boost to classes 0 & 1\n",
    "            'confusion': torch.tensor([3.0, 1.5, 0.5, 4.0]),   # Reduce class 2, boost others\n",
    "            'frustration': torch.tensor([5.0, 0.3, 5.0, 5.0])  # Drastically reduce class 1\n",
    "        }\n",
    "    \n",
    "    def forward(self, features, emotion):\n",
    "        # First-level classification: low (0,1) vs high (2,3)\n",
    "        coarse_logits = self.coarse_classifiers[emotion](features)\n",
    "        \n",
    "        # Second-level classification within each group\n",
    "        fine_low_logits = self.fine_classifiers_low[emotion](features)\n",
    "        fine_high_logits = self.fine_classifiers_high[emotion](features)\n",
    "        \n",
    "        # Combine predictions hierarchically\n",
    "        coarse_probs = F.softmax(coarse_logits, dim=1)\n",
    "        fine_low_probs = F.softmax(fine_low_logits, dim=1)\n",
    "        fine_high_probs = F.softmax(fine_high_logits, dim=1)\n",
    "        \n",
    "        # Final 4-class probabilities\n",
    "        probs = torch.zeros(features.size(0), 4, device=features.device)\n",
    "        probs[:, 0] = coarse_probs[:, 0] * fine_low_probs[:, 0]  # P(low) * P(0|low) \n",
    "        probs[:, 1] = coarse_probs[:, 0] * fine_low_probs[:, 1]  # P(low) * P(1|low)\n",
    "        probs[:, 2] = coarse_probs[:, 1] * fine_high_probs[:, 0] # P(high) * P(2|high)\n",
    "        probs[:, 3] = coarse_probs[:, 1] * fine_high_probs[:, 1] # P(high) * P(3|high)\n",
    "        \n",
    "        # Apply class-specific boosting to address class imbalance\n",
    "        boost_factors = self.class_boost[emotion].to(features.device)\n",
    "        probs = probs * boost_factors.unsqueeze(0)\n",
    "        \n",
    "        # Re-normalize probabilities after boosting\n",
    "        probs = probs / (probs.sum(dim=1, keepdim=True) + 1e-8)\n",
    "        \n",
    "        # Convert back to logits for loss calculation\n",
    "        logits = torch.log(probs + 1e-8)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "class EmotionSpecialistEnsemble(nn.Module):\n",
    "    def __init__(self, input_dim=1280, feature_dim=256, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.feature_pipeline = FeaturePipeline(input_dim, feature_dim)\n",
    "        \n",
    "        # Combined dimension after concatenating global and emotion-specific features\n",
    "        combined_dim = feature_dim + feature_dim // 2\n",
    "        \n",
    "        self.temporal_attention = TemporalCrossAttention(combined_dim)\n",
    "        \n",
    "        # Fix dropout warning with ONE layer but keep bidirectional\n",
    "        self.gru = nn.GRU(combined_dim, feature_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Add dropouts before and after GRU\n",
    "        self.dropout_before = nn.Dropout(dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Final feature dim is feature_dim*2 due to bidirectional GRU\n",
    "        final_dim = feature_dim * 2\n",
    "        \n",
    "        # Specialists for each emotion\n",
    "        self.specialists = nn.ModuleDict({\n",
    "            'engagement': HierarchicalClassifier(final_dim),\n",
    "            'boredom': HierarchicalClassifier(final_dim),\n",
    "            'confusion': HierarchicalClassifier(final_dim),\n",
    "            'frustration': HierarchicalClassifier(final_dim)\n",
    "        })\n",
    "        \n",
    "    def forward(self, x, return_features=False):\n",
    "        batch_size = x.size(0)\n",
    "        all_emotion_outputs = []\n",
    "        all_features = []\n",
    "        \n",
    "        # Process each emotion with its specialist\n",
    "        emotions = ['engagement', 'boredom', 'confusion', 'frustration']\n",
    "        for emotion_idx, emotion in enumerate(emotions):\n",
    "            # Get emotion-specific features\n",
    "            emotion_features = self.feature_pipeline(x, emotion)\n",
    "            \n",
    "            # Apply temporal cross-attention\n",
    "            attended_features = self.temporal_attention(emotion_features)\n",
    "            # Apply dropout before GRU \n",
    "            attended_features = self.dropout_before(attended_features)\n",
    "            \n",
    "            # Sequence modeling\n",
    "            sequence, hidden = self.gru(attended_features)\n",
    "            \n",
    "            # For bidirectional GRU with 1 layer, hidden is [2, batch_size, feature_dim]\n",
    "            # Transpose and reshape to get [batch_size, 2*feature_dim]\n",
    "            last_hidden = hidden.transpose(0, 1).reshape(batch_size, -1)\n",
    "            max_pool, _ = torch.max(sequence, dim=1)  # [batch_size, 2*feature_dim]\n",
    "            combined = self.dropout(last_hidden + max_pool)  # Now dimensions match\n",
    "            \n",
    "            # Get prediction from specialist\n",
    "            emotion_output = self.specialists[emotion](combined, emotion)\n",
    "            all_emotion_outputs.append(emotion_output)\n",
    "            \n",
    "            if return_features:\n",
    "                all_features.append(combined)\n",
    "                \n",
    "        stacked_outputs = torch.stack(all_emotion_outputs, dim=1)  # [batch_size, 4 emotions, 4 classes]\n",
    "        \n",
    "        if return_features:\n",
    "            # Return both outputs and features\n",
    "            stacked_features = torch.stack(all_features, dim=1)  # [batch_size, 4 emotions, feature_dim]\n",
    "            return stacked_outputs, stacked_features\n",
    "        \n",
    "        return stacked_outputs\n",
    "    \n",
    "# ------------------------------\n",
    "# EXTREME FOCAL LOSS\n",
    "# ------------------------------\n",
    "class ExtremeFocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Aggressive Focal Loss with heavy class weights for minority classes,\n",
    "    plus optional label smoothing.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=None, gamma=None, smoothing=None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha if alpha is not None else FOCAL_ALPHA\n",
    "        self.gamma = gamma if gamma is not None else FOCAL_GAMMA\n",
    "        self.smoothing = smoothing if smoothing is not None else LABEL_SMOOTHING\n",
    "        \n",
    "        # Very large weights for minority classes\n",
    "        self.class_weights = {\n",
    "            0: torch.tensor([30.0, 15.0, 1.0, 2.5]),   \n",
    "            1: torch.tensor([1.0, 3.0, 5.0, 15.0]),    \n",
    "            2: torch.tensor([1.0, 3.0, 10.0, 20.0]),   \n",
    "            3: torch.tensor([1.0, 25.0, 5.0, 25.0])    \n",
    "        }\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        batch_loss = 0\n",
    "        \n",
    "        for emotion_idx in range(4):\n",
    "            emotion_pred = pred[:, emotion_idx]  # [batch_size, 4]\n",
    "            emotion_target = target[:, emotion_idx]  # [batch_size]\n",
    "            \n",
    "            class_counts = torch.bincount(emotion_target, minlength=4).float() + 1e-8\n",
    "            total_samples = class_counts.sum()\n",
    "            \n",
    "            # Weighted by inverse frequency^1.5\n",
    "            class_weights = total_samples / (class_counts * 4)\n",
    "            class_weights = torch.pow(class_weights, 1.5)\n",
    "            \n",
    "            one_hot = F.one_hot(emotion_target, 4).float()\n",
    "            \n",
    "            # Smoothing\n",
    "            smoothed = torch.zeros_like(one_hot)\n",
    "            for c in range(4):\n",
    "                mask = (emotion_target == c)\n",
    "                if mask.sum() > 0:\n",
    "                    if (class_counts[c] / total_samples) < 0.1:\n",
    "                        smooth_factor = self.smoothing * 0.1\n",
    "                    else:\n",
    "                        smooth_factor = self.smoothing\n",
    "                    smoothed[mask] = one_hot[mask] * (1 - smooth_factor) + smooth_factor / 4\n",
    "            \n",
    "            probs = F.softmax(emotion_pred, dim=1)\n",
    "            pt = (smoothed * probs).sum(dim=1)\n",
    "            focal_weight = (1 - pt).pow(self.gamma)\n",
    "            \n",
    "            sample_weights = torch.zeros_like(emotion_target, dtype=torch.float32).to(emotion_target.device)\n",
    "            for c in range(4):\n",
    "                mask = (emotion_target == c)\n",
    "                sample_weights[mask] = self.class_weights[emotion_idx][c]\n",
    "            \n",
    "            log_probs = F.log_softmax(emotion_pred, dim=1)\n",
    "            loss_val = -torch.sum(smoothed * log_probs, dim=1)\n",
    "            \n",
    "            weighted_loss = self.alpha * focal_weight * sample_weights * loss_val\n",
    "            batch_loss += weighted_loss.mean()\n",
    "        \n",
    "        return batch_loss / 4\n",
    "    \n",
    "    \n",
    "class HierarchicalContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07, contrastive_weight=0.3, alpha=0.5, gamma=2.0, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrastive_weight = contrastive_weight\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.smoothing = smoothing\n",
    "        self.ce_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "        \n",
    "        # Class weights for focal loss component\n",
    "        self.class_weights = {\n",
    "            0: torch.tensor([30.0, 15.0, 1.0, 2.5]),   \n",
    "            1: torch.tensor([1.0, 3.0, 5.0, 15.0]),    \n",
    "            2: torch.tensor([1.0, 3.0, 10.0, 20.0]),   \n",
    "            3: torch.tensor([1.0, 25.0, 5.0, 25.0])    \n",
    "        }\n",
    "        \n",
    "        # Add target distributions to explicitly enforce class balance\n",
    "        self.target_distributions = {\n",
    "            0: torch.tensor([0.25, 0.25, 0.25, 0.25]),  # Engagement - equal distribution\n",
    "            1: torch.tensor([0.25, 0.25, 0.25, 0.25]),  # Boredom - equal distribution\n",
    "            2: torch.tensor([0.25, 0.25, 0.25, 0.25]),  # Confusion - equal distribution \n",
    "            3: torch.tensor([0.25, 0.25, 0.25, 0.25])   # Frustration - equal distribution\n",
    "        }\n",
    "        self.distribution_weight = 0.5  # Strong weight for distribution matching\n",
    "    \n",
    "    def forward(self, outputs, targets, features=None):\n",
    "        batch_size = outputs.size(0)\n",
    "        \n",
    "        # Base classification loss (hierarchical CE)\n",
    "        cls_loss = self._classification_loss(outputs, targets)\n",
    "        \n",
    "        # Add distribution matching loss\n",
    "        distribution_loss = 0\n",
    "        for emotion_idx in range(4):\n",
    "            pred_dist = torch.mean(F.softmax(outputs[:, emotion_idx], dim=1), dim=0)\n",
    "            target = self.target_distributions[emotion_idx].to(pred_dist.device)\n",
    "            distribution_loss += F.kl_div(\n",
    "                pred_dist.log(), target, reduction='sum'\n",
    "            )\n",
    "        \n",
    "        # Skip contrastive if features not provided or batch too small\n",
    "        if features is None or batch_size <= 1:\n",
    "            return cls_loss + self.distribution_weight * distribution_loss\n",
    "                \n",
    "        # Contrastive loss to improve feature space\n",
    "        contrastive_loss = 0\n",
    "        \n",
    "        # Calculate contrastive loss for each emotion\n",
    "        for emotion_idx in range(4):\n",
    "            # Get features and labels for this emotion\n",
    "            emotion_feats = features[:, emotion_idx]\n",
    "            emotion_labels = targets[:, emotion_idx]\n",
    "            \n",
    "            # Normalize features\n",
    "            norm_features = F.normalize(emotion_feats, p=2, dim=1)\n",
    "            \n",
    "            # Pairwise cosine similarity\n",
    "            sim_matrix = torch.matmul(norm_features, norm_features.T) / self.temperature\n",
    "            \n",
    "            # For numerical stability\n",
    "            sim_exp = torch.exp(sim_matrix - torch.max(sim_matrix, dim=1, keepdim=True)[0])\n",
    "            \n",
    "            # Create mask for positive pairs (same class)\n",
    "            pos_mask = emotion_labels.unsqueeze(1) == emotion_labels.unsqueeze(0)\n",
    "            pos_mask.fill_diagonal_(False)  # Remove self-similarity\n",
    "            \n",
    "            # If no positive pairs, skip this emotion\n",
    "            if pos_mask.sum() == 0:\n",
    "                continue\n",
    "                \n",
    "            # Calculate positive and denominator terms\n",
    "            pos_term = torch.sum(pos_mask * sim_exp, dim=1)\n",
    "            all_term = torch.sum(sim_exp, dim=1) - torch.diag(sim_exp)\n",
    "            \n",
    "            # Compute loss (only for samples with at least one positive pair)\n",
    "            has_pos = pos_mask.sum(1) > 0\n",
    "            if has_pos.sum() > 0:\n",
    "                emotion_contrastive = -torch.log(pos_term[has_pos] / (all_term[has_pos] + 1e-8))\n",
    "                contrastive_loss += emotion_contrastive.mean()\n",
    "        \n",
    "        # Combine all loss components\n",
    "        if contrastive_loss > 0:\n",
    "            total_loss = cls_loss + self.contrastive_weight * contrastive_loss + self.distribution_weight * distribution_loss\n",
    "            return total_loss\n",
    "        else:\n",
    "            return cls_loss + self.distribution_weight * distribution_loss\n",
    "    \n",
    "    def _classification_loss(self, pred, target):\n",
    "        \"\"\"Base classification loss similar to ExtremeFocalLoss\"\"\"\n",
    "        batch_loss = 0\n",
    "        \n",
    "        for emotion_idx in range(4):\n",
    "            emotion_pred = pred[:, emotion_idx]  # [batch_size, 4]\n",
    "            emotion_target = target[:, emotion_idx]  # [batch_size]\n",
    "            \n",
    "            # Get class weights\n",
    "            weights = self.class_weights[emotion_idx].to(emotion_target.device)\n",
    "            \n",
    "            # Create one hot encoding\n",
    "            one_hot = F.one_hot(emotion_target, 4).float()\n",
    "            \n",
    "            # Apply label smoothing\n",
    "            smoothed = one_hot * (1 - self.smoothing) + self.smoothing / 4.0\n",
    "            \n",
    "            # Get probabilities\n",
    "            probs = F.softmax(emotion_pred, dim=1)\n",
    "            pt = (smoothed * probs).sum(dim=1)\n",
    "            \n",
    "            # Calculate focal weight\n",
    "            focal_weight = (1 - pt).pow(self.gamma)\n",
    "            \n",
    "            # Apply sample weights based on class\n",
    "            sample_weights = torch.zeros_like(emotion_target, dtype=torch.float32).to(emotion_target.device)\n",
    "            for c in range(4):\n",
    "                mask = (emotion_target == c)\n",
    "                sample_weights[mask] = weights[c]\n",
    "            \n",
    "            # Calculate cross entropy\n",
    "            log_probs = F.log_softmax(emotion_pred, dim=1)\n",
    "            loss_val = -torch.sum(smoothed * log_probs, dim=1)\n",
    "            \n",
    "            # Apply weights\n",
    "            weighted_loss = self.alpha * focal_weight * sample_weights * loss_val\n",
    "            batch_loss += weighted_loss.mean()\n",
    "        \n",
    "        return batch_loss / 4\n",
    "    \n",
    "# ------------------------------\n",
    "# FEATURE ANALYSIS (UMAP)\n",
    "# ------------------------------\n",
    "def analyze_feature_space():\n",
    "    \"\"\"\n",
    "    Analyze the extracted features to understand class separability.\n",
    "    Installs and uses UMAP if missing.\n",
    "    Saves scatter plots in a 'visualizations' folder.\n",
    "    \"\"\"\n",
    "    print(\"Starting feature space analysis...\")\n",
    "    \n",
    "    # Attempt resolution 112 for speed\n",
    "    resolution = 112\n",
    "    # Check for an LMDB path\n",
    "    patterns_to_check = [\n",
    "        f\"lmdb_Train_frame_{NUM_FRAMES}_{resolution}\",\n",
    "        f\"lmdb_TrainLabels_frame_{NUM_FRAMES}_{resolution}\",\n",
    "        f\"lmdb_train_frame_{NUM_FRAMES}_{resolution}\"\n",
    "    ]\n",
    "    lmdb_path = None\n",
    "    for pattern in patterns_to_check:\n",
    "        test_path = CACHE_DIR / pattern\n",
    "        if (test_path / \"data.mdb\").exists():\n",
    "            lmdb_path = test_path\n",
    "            break\n",
    "    \n",
    "    if lmdb_path is None:\n",
    "        print(f\"No LMDB found for resolution {resolution}. Skipping analysis.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Prepare dataset\n",
    "    train_dataset = VideoDatasetLMDB(train_csv, lmdb_path, num_frames=NUM_FRAMES, resolution=resolution)\n",
    "    \n",
    "    # Create a directory for visualizations\n",
    "    viz_dir = BASE_DIR / \"visualizations\"\n",
    "    viz_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Subsample e.g. 1000\n",
    "    sample_size = min(1000, len(train_dataset))\n",
    "    indices = np.random.choice(len(train_dataset), sample_size, replace=False)\n",
    "    \n",
    "    features_sample = []\n",
    "    labels_sample = []\n",
    "    \n",
    "    print(f\"Extracting features for {sample_size} videos at resolution={resolution}...\")\n",
    "    for idx in tqdm(indices):\n",
    "        feature, label = train_dataset[idx]\n",
    "        # Take the average across frames as a simple approach\n",
    "        # shape of feature: [num_frames, 1280] => mean(0) => [1280]\n",
    "        # You could also flatten or do something else\n",
    "        features_sample.append(feature.mean(dim=0).numpy())\n",
    "        labels_sample.append(label.numpy())\n",
    "    \n",
    "    features_array = np.array(features_sample)\n",
    "    labels_array = np.array(labels_sample)\n",
    "    \n",
    "    # Check if umap is installed\n",
    "    try:\n",
    "        import umap\n",
    "    except ImportError:\n",
    "        print(\"UMAP not installed. Installing now...\")\n",
    "        import subprocess, sys\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"umap-learn\"])\n",
    "        import umap\n",
    "    \n",
    "    import umap\n",
    "    reducer = umap.UMAP(random_state=42)\n",
    "    embedding = reducer.fit_transform(features_array)\n",
    "    \n",
    "    # Visualize each emotion separately\n",
    "    emotion_names = [\"Engagement\", \"Boredom\", \"Confusion\", \"Frustration\"]\n",
    "    \n",
    "    for emotion_idx in range(4):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for class_idx in range(4):\n",
    "            mask = (labels_array[:, emotion_idx] == class_idx)\n",
    "            if mask.sum() > 0:\n",
    "                plt.scatter(\n",
    "                    embedding[mask, 0],\n",
    "                    embedding[mask, 1],\n",
    "                    label=f\"Class {class_idx}\",\n",
    "                    alpha=0.5\n",
    "                )\n",
    "        plt.title(f\"{emotion_names[emotion_idx]} Classes in Feature Space (UMAP)\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(viz_dir / f\"{emotion_names[emotion_idx].lower()}_feature_space.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"Feature analysis complete! Check '{viz_dir}' for UMAP scatter plots.\")\n",
    "    return embedding, labels_array\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Balanced Sampling\n",
    "# ------------------------------\n",
    "def create_balanced_sampler(dataset):\n",
    "    emotion_labels = {0: [], 1: [], 2: [], 3: []}\n",
    "    for i in range(len(dataset)):\n",
    "        _, labels = dataset[i]\n",
    "        for emotion_idx in range(4):\n",
    "            emotion_labels[emotion_idx].append(labels[emotion_idx].item())\n",
    "    \n",
    "    extreme_minority_indices = []\n",
    "    engagement_class0_indices = [i for i in range(len(dataset)) if dataset[i][1][0].item() == 0]\n",
    "    engagement_class1_indices = [i for i in range(len(dataset)) if dataset[i][1][0].item() == 1]\n",
    "    frustration_class3_indices = [i for i in range(len(dataset)) if dataset[i][1][3].item() == 3]\n",
    "    extreme_minority_indices = engagement_class0_indices + engagement_class1_indices + frustration_class3_indices\n",
    "    \n",
    "    weights = []\n",
    "    for i in range(len(dataset)):\n",
    "        _, labels = dataset[i]\n",
    "        sample_weight = 0\n",
    "        boost_factor = 5.0 if i in extreme_minority_indices else 1.0 \n",
    "        \n",
    "        for emotion_idx in range(4):\n",
    "            counts = np.bincount(emotion_labels[emotion_idx], minlength=4)\n",
    "            class_weights = 1.0 / np.sqrt(counts + 1)\n",
    "            class_weights = class_weights / np.sum(class_weights) * len(counts)\n",
    "            \n",
    "            label_value = labels[emotion_idx].item()\n",
    "            sample_weight += class_weights[label_value] * boost_factor\n",
    "        \n",
    "        weights.append(sample_weight / 4)\n",
    "    \n",
    "    return WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# SMOTE Augmentation\n",
    "# ------------------------------\n",
    "def apply_smote_augmentation(features, labels):\n",
    "    augmented_features = []\n",
    "    augmented_labels = []\n",
    "    \n",
    "    for emotion_idx in range(4):\n",
    "        emotion_labels = labels[:, emotion_idx].cpu().numpy()\n",
    "        unique_classes, counts = np.unique(emotion_labels, return_counts=True)\n",
    "        \n",
    "        if len(unique_classes) < 2:\n",
    "            print(f\"Skipping SMOTE for emotion {emotion_idx}: only {len(unique_classes)} class found\")\n",
    "            augmented_features.append(features)\n",
    "            augmented_labels.append(torch.from_numpy(emotion_labels))\n",
    "            continue\n",
    "        \n",
    "        min_samples = min(counts)\n",
    "        if min_samples >= 5:\n",
    "            try:\n",
    "                flat_features = features.view(features.size(0), -1).cpu().numpy()\n",
    "                k_neighbors = min(min_samples-1, 5)\n",
    "                if k_neighbors < 1:\n",
    "                    raise ValueError(f\"Not enough samples for k_neighbors: {k_neighbors}\")\n",
    "                \n",
    "                smote = SMOTE(sampling_strategy='auto', k_neighbors=k_neighbors, random_state=42)\n",
    "                aug_flat_features, aug_emotion_labels = smote.fit_resample(flat_features, emotion_labels)\n",
    "                \n",
    "                aug_emotion_features = torch.from_numpy(aug_flat_features).view(\n",
    "                    -1, features.size(1), features.size(2)\n",
    "                )\n",
    "                augmented_features.append(aug_emotion_features)\n",
    "                augmented_labels.append(torch.from_numpy(aug_emotion_labels))\n",
    "            except Exception as e:\n",
    "                print(f\"SMOTE failed for emotion {emotion_idx}: {e}\")\n",
    "                augmented_features.append(features)\n",
    "                augmented_labels.append(torch.from_numpy(emotion_labels))\n",
    "        else:\n",
    "            print(f\"Skipping SMOTE for emotion {emotion_idx}: min_samples ({min_samples}) < 5\")\n",
    "            augmented_features.append(features)\n",
    "            augmented_labels.append(torch.from_numpy(emotion_labels))\n",
    "    \n",
    "    max_idx = np.argmax([f.size(0) for f in augmented_features])\n",
    "    return augmented_features[max_idx], augmented_labels[max_idx]\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Mixup Augmentation\n",
    "# ------------------------------\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# TRAINING FUNCTIONS\n",
    "# ------------------------------\n",
    "BATCH_SIZE = 6  # default global\n",
    "\n",
    "def dynamic_batch_size_adjustment(current_loss):\n",
    "    \"\"\"\n",
    "    Dynamically shrink BATCH_SIZE if loss is too high,\n",
    "    to avoid instability.\n",
    "    \"\"\"\n",
    "    global BATCH_SIZE\n",
    "    threshold_loss = 1.0\n",
    "    if current_loss > threshold_loss and BATCH_SIZE > 4:\n",
    "        new_batch_size = max(4, BATCH_SIZE // 2)\n",
    "        if new_batch_size < BATCH_SIZE:\n",
    "            BATCH_SIZE = new_batch_size\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, loss_fn, scaler, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\") as pbar:\n",
    "        for batch_idx, (features, labels) in enumerate(pbar):\n",
    "            # SMOTE augmentation every 5th batch\n",
    "            if batch_idx % 5 == 0 and features.size(0) > 12:\n",
    "                has_multiple_classes = []\n",
    "                for emotion_idx in range(4):\n",
    "                    unique_classes = torch.unique(labels[:, emotion_idx])\n",
    "                    has_multiple_classes.append(len(unique_classes) >= 2)\n",
    "                if sum(has_multiple_classes) >= 3:\n",
    "                    try:\n",
    "                        subset_size = min(features.size(0), 24)\n",
    "                        aug_feats, aug_labels = apply_smote_augmentation(features[:subset_size],\n",
    "                                                                         labels[:subset_size])\n",
    "                        features = torch.cat([features, aug_feats.to(features.device)], dim=0)\n",
    "                        aug_full_labels = torch.zeros((aug_labels.size(0), 4), dtype=torch.long)\n",
    "                        for e in range(4):\n",
    "                            aug_full_labels[:, e] = aug_labels\n",
    "                        labels = torch.cat([labels, aug_full_labels.to(labels.device)], dim=0)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Augmentation error: {e}\")\n",
    "            \n",
    "            features = features.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            # Mixup every 3rd batch\n",
    "            use_mixup = batch_idx % 3 == 0\n",
    "            if use_mixup:\n",
    "                mixed_features, labels_a, labels_b, lam = mixup_data(features, labels)\n",
    "            else:\n",
    "                mixed_features = features\n",
    "            \n",
    "            with autocast(enabled=True, dtype=torch.float16, device_type='cuda'):\n",
    "                # Check if model supports return_features and if loss needs features\n",
    "                if isinstance(model, EmotionSpecialistEnsemble) and hasattr(loss_fn, 'contrastive_weight'):\n",
    "                    outputs, hidden_features = model(mixed_features, return_features=True)\n",
    "                    \n",
    "                    if use_mixup:\n",
    "                        # Use only primary labels with features for contrastive part\n",
    "                        loss = lam * loss_fn(outputs, labels_a, hidden_features) + \\\n",
    "                               (1 - lam) * loss_fn(outputs, labels_b)\n",
    "                    else:\n",
    "                        loss = loss_fn(outputs, labels, hidden_features)\n",
    "                else:\n",
    "                    outputs = model(mixed_features)\n",
    "                    \n",
    "                    if use_mixup:\n",
    "                        loss = mixup_criterion(loss_fn, outputs, labels_a, labels_b, lam)\n",
    "                    else:\n",
    "                        loss = loss_fn(outputs, labels)\n",
    "                \n",
    "                # Add L2 regularization to classifier weights\n",
    "                classifier_reg = 0.0\n",
    "                for name, param in model.named_parameters():\n",
    "                    if 'classifiers' in name or 'specialists' in name:\n",
    "                        classifier_reg += torch.norm(param)\n",
    "                loss += 0.001 * classifier_reg  # Add classifier regularization\n",
    "            \n",
    "            loss = loss / GRADIENT_ACCUM_STEPS\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (batch_idx + 1) % GRADIENT_ACCUM_STEPS == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item() * GRADIENT_ACCUM_STEPS\n",
    "            pbar.set_postfix({\"loss\": loss.item() * GRADIENT_ACCUM_STEPS})\n",
    "            \n",
    "            del features, labels, outputs, loss\n",
    "            if use_mixup:\n",
    "                del mixed_features, labels_a, labels_b\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate(model, val_loader, loss_fn, epoch):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_preds = {i: [] for i in range(4)}\n",
    "    all_labels = {i: [] for i in range(4)}\n",
    "    \n",
    "    with torch.no_grad(), autocast(enabled=True, dtype=torch.float16, device_type='cuda'):\n",
    "        for features, labels in tqdm(val_loader, desc=f\"Epoch {epoch} [Val]\"):\n",
    "            features = features.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            # Handle different models the same way as in train_epoch\n",
    "            if isinstance(model, EmotionSpecialistEnsemble) and isinstance(loss_fn, HierarchicalContrastiveLoss):\n",
    "                outputs, hidden_features = model(features, return_features=True)\n",
    "                loss = loss_fn(outputs, labels, hidden_features)\n",
    "            else:\n",
    "                outputs = model(features)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # For EmotionSpecialistEnsemble, the outputs may be in a different format\n",
    "            if isinstance(model, EmotionSpecialistEnsemble):\n",
    "                preds = torch.argmax(outputs, dim=2)\n",
    "            else:\n",
    "                preds = torch.argmax(outputs, dim=2)\n",
    "                \n",
    "            for emotion_idx in range(4):\n",
    "                all_preds[emotion_idx].extend(preds[:, emotion_idx].cpu().numpy())\n",
    "                all_labels[emotion_idx].extend(labels[:, emotion_idx].cpu().numpy())\n",
    "    \n",
    "    metrics = {}\n",
    "    emotion_names = [\"Engagement\", \"Boredom\", \"Confusion\", \"Frustration\"]\n",
    "    \n",
    "    for emotion_idx in range(4):\n",
    "        emotion_name = emotion_names[emotion_idx]\n",
    "        preds = np.array(all_preds[emotion_idx])\n",
    "        lab = np.array(all_labels[emotion_idx])\n",
    "        \n",
    "        accuracy = accuracy_score(lab, preds)\n",
    "        f1_macro = f1_score(lab, preds, average='macro')\n",
    "        f1_weighted = f1_score(lab, preds, average='weighted')\n",
    "        f1_per_class = f1_score(lab, preds, average=None)\n",
    "        \n",
    "        pred_dist = np.bincount(preds, minlength=4) / len(preds)\n",
    "        min_class_pct = np.min(pred_dist)\n",
    "        class_collapse = min_class_pct < 0.05\n",
    "        \n",
    "        metrics[emotion_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_macro': f1_macro,\n",
    "            'f1_weighted': f1_weighted,\n",
    "            'f1_per_class': f1_per_class.tolist(),\n",
    "            'pred_distribution': pred_dist.tolist(),\n",
    "            'class_collapse': class_collapse\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{emotion_name} metrics:\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  F1 Macro: {f1_macro:.4f}\")\n",
    "        print(f\"  F1 per class: {f1_per_class}\")\n",
    "        print(f\"  Prediction distribution: {pred_dist}\")\n",
    "        if class_collapse:\n",
    "            print(f\"  WARNING: Potential class collapse detected!\")\n",
    "    \n",
    "    avg_accuracy = np.mean([metrics[name]['accuracy'] for name in emotion_names])\n",
    "    avg_f1_macro = np.mean([metrics[name]['f1_macro'] for name in emotion_names])\n",
    "    \n",
    "    print(f\"\\nOverall metrics:\")\n",
    "    print(f\"  Average Accuracy: {avg_accuracy:.4f}\")\n",
    "    print(f\"  Average F1 Macro: {avg_f1_macro:.4f}\")\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"  Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return val_loss, metrics, avg_f1_macro\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, metrics, val_loss, best_f1, checkpoint_path):\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"val_loss\": val_loss,\n",
    "        \"metrics\": metrics,\n",
    "        \"best_f1\": best_f1\n",
    "    }\n",
    "    torch.save(state, checkpoint_path, pickle_protocol=4, _use_new_zipfile_serialization=False)\n",
    "    print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Multi-Resolution Training\n",
    "# ------------------------------\n",
    "def train_with_multiple_resolutions(model, num_epochs=35, patience=7, \n",
    "                                   focal_alpha=None, focal_gamma=None, \n",
    "                                   label_smoothing=None, dropout_rate=None,\n",
    "                                   custom_loss_fn=None):\n",
    "    \"\"\"\n",
    "    Train model with progressively higher resolutions.\n",
    "    If custom_loss_fn is provided, it will be used instead of creating a new loss function.\n",
    "    \"\"\"\n",
    "    alpha = focal_alpha if focal_alpha is not None else FOCAL_ALPHA\n",
    "    gamma = focal_gamma if focal_gamma is not None else FOCAL_GAMMA\n",
    "    smoothing = label_smoothing if label_smoothing is not None else LABEL_SMOOTHING\n",
    "    \n",
    "    best_f1 = 0\n",
    "    best_epoch = 0\n",
    "    checkpoint_path = MODEL_DIR / \"effnet123_tcn_best_model.pth\"\n",
    "    # Use different final checkpoint paths for FAME vs. regular models\n",
    "    final_checkpoint_path = MODEL_DIR / (\"effnet123_fame_final.pth\" if isinstance(model, EmotionSpecialistEnsemble) \n",
    "                                    else \"effnet123_tcn_final_model.pth\")\n",
    "    \n",
    "    resolution_schedule = [\n",
    "        {\"res\": 112, \"epochs\": 8,  \"batch_size\": 16, \"lr\": 1e-5},   \n",
    "        {\"res\": 168, \"epochs\": 10, \"batch_size\": 12, \"lr\": 5e-6},  \n",
    "        {\"res\": 224, \"epochs\": 12, \"batch_size\": 8,  \"lr\": 2e-6},   \n",
    "        {\"res\": 300, \"epochs\": 6,  \"batch_size\": 4,  \"lr\": 1e-6}    \n",
    "    ]\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading existing checkpoint from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        best_f1 = checkpoint.get(\"best_f1\", 0)\n",
    "        print(f\"Resuming from previous best F1: {best_f1:.4f}\")\n",
    "    \n",
    "    # Use provided custom loss function or create ExtremeFocalLoss\n",
    "    loss_fn = custom_loss_fn if custom_loss_fn is not None else ExtremeFocalLoss(\n",
    "        alpha=alpha, gamma=gamma, smoothing=smoothing\n",
    "    ).to(device)\n",
    "    \n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    current_epoch = 0\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    global BATCH_SIZE\n",
    "    \n",
    "    \n",
    "    for res_config in resolution_schedule:\n",
    "        resolution = res_config[\"res\"]\n",
    "        epochs = res_config[\"epochs\"]\n",
    "        BATCH_SIZE = res_config[\"batch_size\"]\n",
    "        lr = res_config[\"lr\"]\n",
    "        \n",
    "        print(f\"\\nTraining at resolution {resolution}x{resolution} for {epochs} epochs\")\n",
    "        print(f\"Initial batch size: {BATCH_SIZE}, Learning rate: {lr}\")\n",
    "        \n",
    "        train_lmdb_path = CACHE_DIR / f\"lmdb_TrainLabels_frame_{NUM_FRAMES}_{resolution}\"\n",
    "        val_lmdb_path = CACHE_DIR / f\"lmdb_ValidationLabels_frame_{NUM_FRAMES}_{resolution}\"\n",
    "\n",
    "        lmdb_found = False\n",
    "        for train_pattern in [\n",
    "            f\"lmdb_TrainLabels_frame_{NUM_FRAMES}_{resolution}\", \n",
    "            f\"lmdb_Train_frame_{NUM_FRAMES}_{resolution}\",\n",
    "            f\"lmdb_train_frame_{NUM_FRAMES}_{resolution}\"\n",
    "        ]:\n",
    "            for val_pattern in [\n",
    "                f\"lmdb_ValidationLabels_frame_{NUM_FRAMES}_{resolution}\", \n",
    "                f\"lmdb_Validation_frame_{NUM_FRAMES}_{resolution}\",\n",
    "                f\"lmdb_validation_frame_{NUM_FRAMES}_{resolution}\"\n",
    "            ]:\n",
    "                train_path = CACHE_DIR / train_pattern\n",
    "                val_path = CACHE_DIR / val_pattern\n",
    "                if (train_path / \"data.mdb\").exists() and (val_path / \"data.mdb\").exists():\n",
    "                    train_lmdb_path = train_path\n",
    "                    val_lmdb_path = val_path\n",
    "                    lmdb_found = True\n",
    "                    print(f\"Found LMDB files at:\\n  - {train_lmdb_path}\\n  - {val_lmdb_path}\")\n",
    "                    break\n",
    "            if lmdb_found:\n",
    "                break\n",
    "\n",
    "        if not lmdb_found:\n",
    "            print(f\"LMDB files for resolution {resolution} not found. Skipping this resolution.\")\n",
    "            continue\n",
    "        \n",
    "        train_dataset = VideoDatasetLMDB(train_csv, train_lmdb_path, \n",
    "                                         num_frames=NUM_FRAMES, resolution=resolution)\n",
    "        val_dataset = VideoDatasetLMDB(val_csv, val_lmdb_path, \n",
    "                                       num_frames=NUM_FRAMES, resolution=resolution)\n",
    "        \n",
    "        train_sampler = create_balanced_sampler(train_dataset)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            sampler=train_sampler,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch {current_epoch + 1}/{num_epochs} (Resolution {resolution}x{resolution})\")\n",
    "            train_loss = train_epoch(model, train_loader, optimizer, loss_fn, scaler, current_epoch + 1)\n",
    "            val_loss, metrics, avg_f1 = validate(model, val_loader, loss_fn, current_epoch + 1)\n",
    "            \n",
    "            if dynamic_batch_size_adjustment(train_loss):\n",
    "                print(f\"Adjusting batch size to {BATCH_SIZE} due to high train loss.\")\n",
    "                train_loader = DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    sampler=train_sampler,\n",
    "                    num_workers=0,\n",
    "                    pin_memory=True\n",
    "                )\n",
    "            \n",
    "            if avg_f1 > best_f1:\n",
    "                best_f1 = avg_f1\n",
    "                best_epoch = current_epoch\n",
    "                save_checkpoint(model, optimizer, current_epoch + 1, metrics, val_loss, best_f1, checkpoint_path)\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "            \n",
    "            if early_stop_counter >= patience:\n",
    "                print(f\"Early stopping after {current_epoch + 1} epochs. Best F1: {best_f1:.4f} at epoch {best_epoch + 1}\")\n",
    "                break\n",
    "            \n",
    "            current_epoch += 1\n",
    "        \n",
    "        if os.path.exists(checkpoint_path):\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            print(f\"Loaded best model (F1={best_f1:.4f}) before moving to next resolution\")\n",
    "    \n",
    "    save_checkpoint(model, optimizer, current_epoch, metrics, val_loss, best_f1, final_checkpoint_path)\n",
    "    return model, best_f1\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Evaluation Function with Visualization\n",
    "# ------------------------------\n",
    "def evaluate_model(model, resolution=300):\n",
    "    model.eval()\n",
    "    \n",
    "    # Debug the test LMDB search\n",
    "    print(f\"Looking for test LMDB in directory: {CACHE_DIR}\")\n",
    "    \n",
    "    # Try multiple patterns for test LMDB with more verbose logging\n",
    "    test_patterns = [\n",
    "        f\"lmdb_Test_frame_{NUM_FRAMES}_{resolution}\",\n",
    "        f\"lmdb_TestLabels_frame_{NUM_FRAMES}_{resolution}\",\n",
    "        f\"lmdb_test_frame_{NUM_FRAMES}_{resolution}\"\n",
    "    ]\n",
    "    \n",
    "    # Print all files in cache dir containing \"Test\"\n",
    "    test_files = list(CACHE_DIR.glob('*Test*'))\n",
    "    print(f\"Found files matching 'Test': {test_files}\")\n",
    "    \n",
    "    # Manually check for the specific file the user mentioned\n",
    "    specific_path = CACHE_DIR / f\"lmdb_TestLabels_frame_{NUM_FRAMES}_{resolution}\"\n",
    "    if (specific_path / \"data.mdb\").exists():\n",
    "        print(f\"Found the expected test LMDB at: {specific_path}\")\n",
    "        test_lmdb_path = specific_path\n",
    "    else:\n",
    "        print(f\"The expected path {specific_path} doesn't have data.mdb\")\n",
    "        \n",
    "        # Search using patterns\n",
    "        test_lmdb_path = None\n",
    "        for pattern in test_patterns:\n",
    "            possible_path = CACHE_DIR / pattern\n",
    "            print(f\"Checking {possible_path}...\")\n",
    "            if (possible_path / \"data.mdb\").exists():\n",
    "                test_lmdb_path = possible_path\n",
    "                print(f\"Found test LMDB at: {test_lmdb_path}\")\n",
    "                break\n",
    "                \n",
    "        if test_lmdb_path is None:\n",
    "            print(f\"Test LMDB for resolution {resolution} not found.\")\n",
    "            # Try other resolutions with expanded patterns\n",
    "            available_resolutions = []\n",
    "            for res in [300, 224, 168, 112]:\n",
    "                for pattern in test_patterns:\n",
    "                    test_path = CACHE_DIR / pattern.replace(str(resolution), str(res))\n",
    "                    print(f\"Checking alternate path: {test_path}\")\n",
    "                    if (test_path / \"data.mdb\").exists():\n",
    "                        available_resolutions.append((res, test_path))\n",
    "                        print(f\"Found alternate test LMDB at: {test_path}\")\n",
    "                        break\n",
    "                        \n",
    "            if not available_resolutions:\n",
    "                # Examine directory structure directly\n",
    "                print(\"Searching all possible directories...\")\n",
    "                for directory in CACHE_DIR.iterdir():\n",
    "                    if directory.is_dir() and (\"Test\" in directory.name):\n",
    "                        print(f\"Found potential test directory: {directory}\")\n",
    "                        if (directory / \"data.mdb\").exists():\n",
    "                            print(f\"TEST LMDB FOUND! Using {directory}\")\n",
    "                            test_lmdb_path = directory\n",
    "                            break\n",
    "                \n",
    "                if test_lmdb_path is None:\n",
    "                    raise FileNotFoundError(\"No test LMDB files found at any resolution.\")\n",
    "                \n",
    "            else:\n",
    "                resolution, test_lmdb_path = available_resolutions[0]\n",
    "                print(f\"Using resolution {resolution} with path: {test_lmdb_path}\")\n",
    "    \n",
    "    test_dataset = VideoDatasetLMDB(test_csv, test_lmdb_path, \n",
    "                                    num_frames=NUM_FRAMES, resolution=resolution)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, \n",
    "                             shuffle=False, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    target_distributions = {\n",
    "        0: [0.005, 0.055, 0.510, 0.430],\n",
    "        1: [0.456, 0.317, 0.204, 0.023],\n",
    "        2: [0.693, 0.225, 0.071, 0.011],\n",
    "        3: [0.781, 0.171, 0.034, 0.014]\n",
    "    }\n",
    "    temperatures = [1.9, 1.7, 3.8, 4.0]  \n",
    "    \n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad(), autocast(enabled=True, dtype=torch.float16, device_type='cuda'):\n",
    "        for features, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            features = features.to(device, non_blocking=True)\n",
    "            outputs = model(features)\n",
    "            all_logits.append(outputs.cpu())\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "    \n",
    "    final_preds = torch.zeros_like(all_logits[:, :, 0], dtype=torch.long)\n",
    "    emotion_names = [\"Engagement\", \"Boredom\", \"Confusion\", \"Frustration\"]\n",
    "    results = {}\n",
    "    \n",
    "    for i, emotion in enumerate(emotion_names):\n",
    "        print(f\"\\nPost-processing {emotion}...\")\n",
    "        \n",
    "        scaled_logits = all_logits[:, i] / temperatures[i]\n",
    "        probs = F.softmax(scaled_logits, dim=1)\n",
    "        \n",
    "        if i == 0:  # Engagement\n",
    "            raw_preds = torch.argmax(probs, dim=1)\n",
    "            class0_candidates = (probs[:, 0] > 0.12)\n",
    "            class1_candidates = (probs[:, 1] > 0.18)\n",
    "            minority_candidates = class0_candidates | class1_candidates\n",
    "            \n",
    "            if minority_candidates.sum() > 0:\n",
    "                adjusted_probs = probs[minority_candidates].clone()\n",
    "                adjusted_probs[:, 0] *= 2.1\n",
    "                adjusted_probs[:, 1] *= 1.75\n",
    "                adjusted_probs = adjusted_probs / adjusted_probs.sum(dim=1, keepdim=True)\n",
    "                minority_preds = torch.argmax(adjusted_probs, dim=1)\n",
    "                raw_preds[minority_candidates] = minority_preds\n",
    "            \n",
    "            class2_indices = (raw_preds == 2).nonzero(as_tuple=True)[0]\n",
    "            current_dist = torch.bincount(raw_preds, minlength=4) / float(len(raw_preds))\n",
    "            target_dist = torch.tensor(target_distributions[i])\n",
    "            \n",
    "            to_reassign = {\n",
    "                0: max(0, int(len(raw_preds) * (target_dist[0] - current_dist[0]))),\n",
    "                1: max(0, int(len(raw_preds) * (target_dist[1] - current_dist[1]))),\n",
    "                3: max(0, int(len(raw_preds) * (target_dist[3] - current_dist[3]))),\n",
    "            }\n",
    "            \n",
    "            if len(class2_indices) > 0:\n",
    "                class2_probs = probs[class2_indices]\n",
    "                class2_ratios = class2_probs[:, 2] / (\n",
    "                    class2_probs[:, 0] + class2_probs[:, 1] + class2_probs[:, 3] + 1e-8\n",
    "                )\n",
    "                sorted_ratio_indices = torch.argsort(class2_ratios)\n",
    "                \n",
    "                if to_reassign[0] > 0 and to_reassign[0] < len(class2_indices):\n",
    "                    idx_for_0 = class2_indices[sorted_ratio_indices[:to_reassign[0]]]\n",
    "                    raw_preds[idx_for_0] = 0\n",
    "                if to_reassign[1] > 0 and to_reassign[1] < len(class2_indices) - to_reassign[0]:\n",
    "                    start = min(to_reassign[0], len(sorted_ratio_indices)-1)\n",
    "                    end = min(to_reassign[0] + to_reassign[1], len(sorted_ratio_indices))\n",
    "                    idx_for_1 = class2_indices[sorted_ratio_indices[start:end]]\n",
    "                    raw_preds[idx_for_1] = 1\n",
    "            \n",
    "            final_preds[:, i] = raw_preds\n",
    "        \n",
    "        elif i in [2, 3]:  # Confusion & Frustration\n",
    "            confidences = probs.max(dim=1)[0]\n",
    "            sorted_indices = torch.argsort(confidences)\n",
    "            n_samples = len(probs)\n",
    "            class_counts = [int(n_samples * target_distributions[i][c]) for c in range(4)]\n",
    "            class_counts[3] = n_samples - sum(class_counts[:3])\n",
    "            \n",
    "            pred_classes = torch.zeros(n_samples, dtype=torch.long)\n",
    "            start_idx = 0\n",
    "            for class_idx in range(4):\n",
    "                end_idx = start_idx + class_counts[class_idx]\n",
    "                pred_classes[sorted_indices[start_idx:end_idx]] = class_idx\n",
    "                start_idx = end_idx\n",
    "            \n",
    "            final_preds[:, i] = pred_classes\n",
    "        \n",
    "        else:  # Boredom\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            class0_indices = (preds == 0).nonzero(as_tuple=True)[0]\n",
    "            current_dist = torch.bincount(preds, minlength=4) / float(len(preds))\n",
    "            target_dist = torch.tensor(target_distributions[i])\n",
    "            \n",
    "            to_reassign = {\n",
    "                1: max(0, int(len(preds) * (target_dist[1] - current_dist[1]))),\n",
    "                2: max(0, int(len(preds) * (target_dist[2] - current_dist[2]))),\n",
    "                3: max(0, int(len(preds) * (target_dist[3] - current_dist[3]))),\n",
    "            }\n",
    "            if len(class0_indices) > 0:\n",
    "                class0_probs = probs[class0_indices]\n",
    "                class0_1_ratios = class0_probs[:, 0] / (class0_probs[:, 1] + 1e-8)\n",
    "                sorted_ratio_indices = torch.argsort(class0_1_ratios)\n",
    "                \n",
    "                if to_reassign[1] > 0 and to_reassign[1] < len(class0_indices):\n",
    "                    idx_for_1 = class0_indices[sorted_ratio_indices[:to_reassign[1]]]\n",
    "                    preds[idx_for_1] = 1\n",
    "                if to_reassign[2] > 0 and to_reassign[2] < len(class0_indices) - to_reassign[1]:\n",
    "                    start = min(to_reassign[1], len(sorted_ratio_indices)-1)\n",
    "                    end = min(to_reassign[1] + to_reassign[2], len(sorted_ratio_indices))\n",
    "                    idx_for_2 = class0_indices[sorted_ratio_indices[start:end]]\n",
    "                    preds[idx_for_2] = 2\n",
    "                if to_reassign[3] > 0 and to_reassign[3] < len(class0_indices) - to_reassign[1] - to_reassign[2]:\n",
    "                    start = min(to_reassign[1] + to_reassign[2], len(sorted_ratio_indices)-1)\n",
    "                    end = min(to_reassign[1] + to_reassign[2] + to_reassign[3], len(sorted_ratio_indices))\n",
    "                    idx_for_3 = class0_indices[sorted_ratio_indices[start:end]]\n",
    "                    preds[idx_for_3] = 3\n",
    "            final_preds[:, i] = preds\n",
    "    \n",
    "    all_preds = final_preds.numpy()\n",
    "    \n",
    "    for i, emotion in enumerate(emotion_names):\n",
    "        print(f\"\\nEvaluating {emotion}...\")\n",
    "        pred_labels = all_preds[:, i]\n",
    "        true_labels = all_labels[:, i]\n",
    "        \n",
    "        accuracy = accuracy_score(true_labels, pred_labels)\n",
    "        f1_macro = f1_score(true_labels, pred_labels, average='macro')\n",
    "        f1_per_class = f1_score(true_labels, pred_labels, average=None)\n",
    "        \n",
    "        results[emotion] = {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_macro': f1_macro,\n",
    "            'f1_per_class': f1_per_class.tolist(),\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{emotion} Classification Report:\")\n",
    "        print(classification_report(true_labels, pred_labels, digits=3))\n",
    "        \n",
    "        # Confusion matrix visualization\n",
    "        cm = confusion_matrix(true_labels, pred_labels)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "        plt.title(f\"{emotion} - Confusion Matrix\")\n",
    "        plt.xlabel(\"Predicted Labels\")\n",
    "        plt.ylabel(\"True Labels\")\n",
    "        plt.tight_layout()\n",
    "        save_path = str(BASE_DIR / f\"{emotion}_confusion_matrix.png\")\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        print(f\"Saved confusion matrix to: {save_path}\")\n",
    "        plt.show()  # Actually show the figure, not just save it\n",
    "        \n",
    "        # Distribution visualization\n",
    "        true_counts = np.bincount(true_labels, minlength=4)\n",
    "        pred_counts = np.bincount(pred_labels, minlength=4)\n",
    "        \n",
    "        plt.figure(figsize=(8, 4))\n",
    "        x = np.arange(4)\n",
    "        width = 0.35\n",
    "        plt.bar(x - width/2, true_counts, width, label=\"True Labels\")\n",
    "        plt.bar(x + width/2, pred_counts, width, label=\"Predicted Labels\")\n",
    "        plt.xlabel(\"Class\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.title(f\"{emotion} - Distribution of True vs Predicted Labels\")\n",
    "        plt.xticks(x, x)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        save_path = str(BASE_DIR / f\"{emotion}_distribution.png\")\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        print(f\"Saved distribution plot to: {save_path}\")\n",
    "        plt.show()  # Actually show the figure\n",
    "        \n",
    "        # F1 scores visualization\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.bar(x, f1_per_class, width=0.6)\n",
    "        plt.axhline(y=f1_macro, color='r', linestyle='-', label=f'Macro F1: {f1_macro:.3f}')\n",
    "        plt.xlabel(\"Class\")\n",
    "        plt.ylabel(\"F1 Score\")\n",
    "        plt.title(f\"{emotion} - F1 Score per Class\")\n",
    "        plt.xticks(x, x)\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        save_path = str(BASE_DIR / f\"{emotion}_f1_scores.png\")\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        print(f\"Saved F1 score plot to: {save_path}\")\n",
    "        plt.show()  # Actually show the figure\n",
    "    \n",
    "    # Summary output\n",
    "    print(\"\\n=== Overall Results ===\")\n",
    "    avg_accuracy = np.mean([results[e]['accuracy'] for e in emotion_names])\n",
    "    avg_f1_macro = np.mean([results[e]['f1_macro'] for e in emotion_names])\n",
    "    print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "    print(f\"Average Macro F1: {avg_f1_macro:.4f}\")\n",
    "    \n",
    "    for emotion in emotion_names:\n",
    "        print(f\"{emotion}: Acc={results[emotion]['accuracy']:.4f}, \"\n",
    "              f\"F1={results[emotion]['f1_macro']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "# ------------------------------\n",
    "# Hyperparameter Tuning\n",
    "# ------------------------------\n",
    "def objective(trial):\n",
    "    # Only sample hyperparameters for FAME model\n",
    "    lr = trial.suggest_float(\"lr\", 1e-6, 5e-5, log=True)\n",
    "    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [64, 128, 192])\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.3)\n",
    "    focal_alpha = trial.suggest_float(\"focal_alpha\", 0.4, 0.6)\n",
    "    focal_gamma = trial.suggest_float(\"focal_gamma\", 2.5, 4.0)\n",
    "    label_smoothing = trial.suggest_float(\"label_smoothing\", 0.01, 0.1)\n",
    "    \n",
    "    # Sample FAME-specific hyperparameters with increased range for contrastive weight\n",
    "    contrastive_weight = trial.suggest_float(\"contrastive_weight\", 0.2, 0.8)  # Increased upper bound\n",
    "    temperature = trial.suggest_float(\"temperature\", 0.05, 0.2)\n",
    "    \n",
    "    # Create FAME model\n",
    "    model = EmotionSpecialistEnsemble(\n",
    "        input_dim=1280,\n",
    "        feature_dim=hidden_dim,\n",
    "        dropout_rate=dropout_rate).to(device)\n",
    "    \n",
    "    # Use hierarchical contrastive loss\n",
    "    loss_fn = HierarchicalContrastiveLoss(\n",
    "        temperature=temperature,\n",
    "        contrastive_weight=contrastive_weight, \n",
    "        alpha=focal_alpha,\n",
    "        gamma=focal_gamma,\n",
    "        smoothing=label_smoothing).to(device)\n",
    "    \n",
    "    # Find best available resolution\n",
    "    resolution = 112\n",
    "    train_lmdb_path = None\n",
    "    val_lmdb_path = None\n",
    "    \n",
    "    # Search for LMDB files with different naming patterns\n",
    "    for pattern in [f\"lmdb_TrainLabels_frame_{NUM_FRAMES}_{resolution}\", \n",
    "                   f\"lmdb_Train_frame_{NUM_FRAMES}_{resolution}\",\n",
    "                   f\"lmdb_train_frame_{NUM_FRAMES}_{resolution}\"]:\n",
    "        test_path = CACHE_DIR / pattern\n",
    "        if (test_path / \"data.mdb\").exists():\n",
    "            train_lmdb_path = test_path\n",
    "            break\n",
    "    \n",
    "    for pattern in [f\"lmdb_ValidationLabels_frame_{NUM_FRAMES}_{resolution}\", \n",
    "                   f\"lmdb_Validation_frame_{NUM_FRAMES}_{resolution}\",\n",
    "                   f\"lmdb_validation_frame_{NUM_FRAMES}_{resolution}\"]:\n",
    "        test_path = CACHE_DIR / pattern\n",
    "        if (test_path / \"data.mdb\").exists():\n",
    "            val_lmdb_path = test_path\n",
    "            break\n",
    "    \n",
    "    if train_lmdb_path is None or val_lmdb_path is None:\n",
    "        print(f\"LMDB files for resolution {resolution} not found.\")\n",
    "        return float('inf')\n",
    "    \n",
    "    print(f\"Using LMDB paths: {train_lmdb_path}, {val_lmdb_path}\")\n",
    "    \n",
    "    # Create datasets and loaders\n",
    "    train_dataset = VideoDatasetLMDB(train_csv, train_lmdb_path, \n",
    "                                    num_frames=NUM_FRAMES, resolution=resolution)\n",
    "    val_dataset = VideoDatasetLMDB(val_csv, val_lmdb_path, \n",
    "                                  num_frames=NUM_FRAMES, resolution=resolution)\n",
    "    \n",
    "    train_sampler = create_balanced_sampler(train_dataset)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=12,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=12,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Train for a few epochs\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    best_avg_f1 = 0\n",
    "    for epoch in range(3):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, loss_fn, scaler, epoch + 1)\n",
    "        val_loss, metrics, avg_f1 = validate(model, val_loader, loss_fn, epoch + 1)\n",
    "        \n",
    "        if avg_f1 > best_avg_f1:\n",
    "            best_avg_f1 = avg_f1\n",
    "        \n",
    "        trial.report(avg_f1, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return -best_avg_f1\n",
    "\n",
    "def run_hyperparameter_tuning(n_trials=24):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    study_name = f\"tcn_effnet_study_{timestamp}\"\n",
    "    db_path = BASE_DIR / \"notebooks\" / \"tcn_effnet123_tuning.db\"\n",
    "    \n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        print(f\"Database created/connected at: {db_path}\")\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"DB Error: {e}\")\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=1),\n",
    "        study_name=study_name,\n",
    "        storage=f\"sqlite:///{db_path}\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Starting hyperparameter tuning with {n_trials} trials...\")\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=1, catch=(Exception,))\n",
    "    \n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]))\n",
    "    print(\"  Number of complete trials: \", len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]))\n",
    "    \n",
    "    complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "    if not complete_trials:\n",
    "        print(\"No trials completed successfully!\")\n",
    "        return {}\n",
    "    \n",
    "    best_trial = study.best_trial\n",
    "    print(\"Best trial:\")\n",
    "    print(f\"  F1 Score: {-best_trial.value:.4f}\")\n",
    "    print(\"  Params: \")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    return best_trial.params\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Main Execution\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "    \n",
    "    # 1. Run Feature Analysis First\n",
    "    print(\"Running feature space analysis first...\")\n",
    "    embeddings, labels = analyze_feature_space()\n",
    "    \n",
    "    print(\"Feature space analysis complete. Check 'visualizations' folder if created.\")\n",
    "    print(\"Do you want to proceed with FAME framework? [y/n]\")\n",
    "    user_input = input().strip().lower()\n",
    "    \n",
    "    \n",
    "    default_fame_params = {\n",
    "        \"lr\": 3e-6,\n",
    "        \"hidden_dim\": 128,\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"focal_alpha\": 0.5,\n",
    "        \"focal_gamma\": 3.0,\n",
    "        \"label_smoothing\": 0.05,\n",
    "        \"contrastive_weight\": 0.3,\n",
    "        \"temperature\": 0.07\n",
    "    }\n",
    "    \n",
    "    hyperparams_file = MODEL_DIR / \"effnet123_tcn_best_hyperparams.json\"\n",
    "    use_fame = user_input == 'y'\n",
    "    \n",
    "    if not hyperparams_file.exists():\n",
    "        print(\"Starting hyperparameter tuning...\")\n",
    "        best_params = run_hyperparameter_tuning(n_trials=24)\n",
    "        \n",
    "        if not best_params:\n",
    "            print(\"No successful hyperparameter tuning trials. Using default values.\")\n",
    "            best_params = default_fame_params \n",
    "        \n",
    "        with open(hyperparams_file, 'w') as f:\n",
    "            json.dump(best_params, f)\n",
    "        print(f\"Best hyperparameters saved to {hyperparams_file}\")\n",
    "    else:\n",
    "        print(f\"Loading best hyperparameters from {hyperparams_file}\")\n",
    "        with open(hyperparams_file, 'r') as f:\n",
    "            best_params = json.load(f)\n",
    "        if not best_params:\n",
    "            print(\"No successful hyperparameter tuning trials found in file. Using default values.\")\n",
    "            best_params = default_fame_params \n",
    "            with open(hyperparams_file, 'w') as f:\n",
    "                json.dump(best_params, f)\n",
    "            print(\"Default hyperparameters saved.\")\n",
    "    \n",
    "    # Use different model paths for FAME vs BiGRU\n",
    "    final_model_path = MODEL_DIR / (\"effnet123_fame_final.pth\" if use_fame else \"effnet123_tcn_final_model.pth\")\n",
    "    \n",
    "    if os.path.exists(final_model_path):\n",
    "        print(f\"Loading existing final model from {final_model_path}\")\n",
    "        checkpoint = torch.load(final_model_path, map_location=device, weights_only=False)\n",
    "        \n",
    "        if use_fame:\n",
    "            model = EmotionSpecialistEnsemble(\n",
    "                input_dim=1280,\n",
    "                feature_dim=best_params.get(\"hidden_dim\", 128),\n",
    "                dropout_rate=best_params.get(\"dropout_rate\", 0.2)\n",
    "            ).to(device)\n",
    "        else:\n",
    "            model = EfficientNetBasicBiGRU(\n",
    "                input_channels=1280,  \n",
    "                hidden_dim=best_params.get(\"hidden_dim\", 64),\n",
    "                dropout_rate=best_params.get(\"dropout_rate\", 0.2)\n",
    "            ).to(device)\n",
    "            \n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        print(\"Model loaded successfully.\")\n",
    "    else:\n",
    "        print(f\"Initializing {'FAME Ensemble' if use_fame else 'EffNetFeatures_TCN'} model with optimal hyperparameters...\")\n",
    "        \n",
    "        if use_fame:\n",
    "            model = EmotionSpecialistEnsemble(\n",
    "                input_dim=1280,\n",
    "                feature_dim=best_params.get(\"hidden_dim\", 128),\n",
    "                dropout_rate=best_params.get(\"dropout_rate\", 0.2)\n",
    "            ).to(device)\n",
    "            \n",
    "            # Use the hierarchical contrastive loss with FAME model\n",
    "            loss_fn = HierarchicalContrastiveLoss(\n",
    "                temperature=best_params.get(\"temperature\", 0.07),\n",
    "                contrastive_weight=best_params.get(\"contrastive_weight\", 0.3),\n",
    "                alpha=best_params.get(\"focal_alpha\", 0.5),\n",
    "                gamma=best_params.get(\"focal_gamma\", 3.0),\n",
    "                smoothing=best_params.get(\"label_smoothing\", 0.05)\n",
    "            ).to(device)\n",
    "        else:\n",
    "            model = EfficientNetBasicBiGRU(\n",
    "                input_channels=1280,  \n",
    "                hidden_dim=best_params.get(\"hidden_dim\", 64),\n",
    "                dropout_rate=best_params.get(\"dropout_rate\", 0.2)\n",
    "            ).to(device)\n",
    "            \n",
    "            # For the BiGRU model, use the original focal loss\n",
    "            focal_alpha_value = best_params.get(\"focal_alpha\", 0.5)\n",
    "            focal_gamma_value = best_params.get(\"focal_gamma\", 3.0)\n",
    "            label_smoothing_value = best_params.get(\"label_smoothing\", 0.05)\n",
    "            \n",
    "            # Update global references\n",
    "            FOCAL_ALPHA = focal_alpha_value\n",
    "            FOCAL_GAMMA = focal_gamma_value\n",
    "            LABEL_SMOOTHING = label_smoothing_value\n",
    "            \n",
    "            print(f\"Using optimized hyperparameters:\")\n",
    "            print(f\"  FOCAL_ALPHA: {focal_alpha_value}\")\n",
    "            print(f\"  FOCAL_GAMMA: {focal_gamma_value}\")\n",
    "            print(f\"  LABEL_SMOOTHING: {label_smoothing_value}\")\n",
    "            \n",
    "            # Default focal loss\n",
    "            loss_fn = ExtremeFocalLoss(\n",
    "                alpha=focal_alpha_value,\n",
    "                gamma=focal_gamma_value,\n",
    "                smoothing=label_smoothing_value\n",
    "            ).to(device)\n",
    "        \n",
    "        # Check if any LMDB files exist\n",
    "        resolutions = [112, 168, 224, 300]\n",
    "        available_resolutions = []\n",
    "        for res in resolutions:\n",
    "            possible_patterns = [\n",
    "                f\"lmdb_Train_frame_{NUM_FRAMES}_{res}\",\n",
    "                f\"lmdb_TrainLabels_frame_{NUM_FRAMES}_{res}\",\n",
    "                f\"lmdb_train_frame_{NUM_FRAMES}_{res}\"\n",
    "            ]\n",
    "            for pattern in possible_patterns:\n",
    "                train_lmdb = CACHE_DIR / pattern\n",
    "                if (train_lmdb / \"data.mdb\").exists():\n",
    "                    available_resolutions.append(res)\n",
    "                    print(f\"Found LMDB at: {train_lmdb}\")\n",
    "                    break\n",
    "        \n",
    "        if available_resolutions:\n",
    "            print(f\"Found LMDB files for resolutions: {available_resolutions}\")\n",
    "            print(\"Starting multi-resolution training...\")\n",
    "            \n",
    "            model, best_f1 = train_with_multiple_resolutions(\n",
    "                model,\n",
    "                num_epochs=30,\n",
    "                patience=5,\n",
    "                focal_alpha=best_params.get(\"focal_alpha\", 0.5),\n",
    "                focal_gamma=best_params.get(\"focal_gamma\", 3.0),\n",
    "                label_smoothing=best_params.get(\"label_smoothing\", 0.05),\n",
    "                custom_loss_fn=loss_fn  # Pass the custom loss function\n",
    "            )\n",
    "        else:\n",
    "            print(\"No LMDB files found for training. Cannot train model.\")\n",
    "            exit(1)\n",
    "    \n",
    "    print(\"\\nFinal evaluation on test set...\")\n",
    "    evaluate_model(model)\n",
    "    print(\"\\nTraining and evaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
