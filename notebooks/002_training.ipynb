{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Required Libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.6.0+cu126\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image\n",
    "from torch.amp import GradScaler, autocast\n",
    "import optuna\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1) Logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.FileHandler('training_classification.log')\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] No 'optuna_trials_backup.txt' found. Proceeding with empty tested set.\n"
     ]
    }
   ],
   "source": [
    "# Parse old file & build 'tested_params' set\n",
    "import ast\n",
    "\n",
    "def load_old_trials(file_path):\n",
    "    \"\"\"\n",
    "    Reads lines in format:\n",
    "      Trial 0 | Val Loss: 0.977... | Params: {...}\n",
    "    Returns a list of (trial_number, val_loss, param_dict).\n",
    "    Skips lines with val_loss=None or invalid format.\n",
    "    \"\"\"\n",
    "    import ast\n",
    "\n",
    "    old_trials = []  # We'll store (trial_num, val_loss, params_dict)\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            # Must start with \"Trial \"\n",
    "            if not line.startswith(\"Trial \"):\n",
    "                continue\n",
    "\n",
    "            # We expect exactly 3 parts => \"Trial X\", \"Val Loss: Y\", \"Params: Z\"\n",
    "            parts = line.split(\" | \", maxsplit=2)\n",
    "            if len(parts) != 3:\n",
    "                print(f\"[SKIP] Unexpected format: {line}\")\n",
    "                continue\n",
    "\n",
    "            # 1) parse \"Trial X\"\n",
    "            trial_str = parts[0].strip()  # e.g. \"Trial 0\"\n",
    "            # e.g. trial_str.split(\" \")[1] => \"0\"\n",
    "            try:\n",
    "                trial_num = int(trial_str.split(\" \")[1])\n",
    "            except:\n",
    "                print(f\"[SKIP] Could not parse trial number: {trial_str}\")\n",
    "                continue\n",
    "\n",
    "            # 2) parse \"Val Loss: Y\"\n",
    "            val_part = parts[1].strip()\n",
    "            if not val_part.startswith(\"Val Loss: \"):\n",
    "                print(f\"[SKIP] val_part doesn't start with 'Val Loss: ': {val_part}\")\n",
    "                continue\n",
    "            val_str = val_part.split(\"Val Loss: \", 1)[1]  # e.g. \"0.977...\" or \"None\"\n",
    "            if val_str == \"None\":\n",
    "                # Means pruned or failed => skip\n",
    "                continue\n",
    "            else:\n",
    "                try:\n",
    "                    val_loss = float(val_str)\n",
    "                except:\n",
    "                    print(f\"[SKIP] could not parse val_loss as float: {val_str}\")\n",
    "                    continue\n",
    "\n",
    "            # 3) parse \"Params: {...}\"\n",
    "            params_part = parts[2].strip()\n",
    "            if not params_part.startswith(\"Params: \"):\n",
    "                print(f\"[SKIP] params_part doesn't start with 'Params: ': {params_part}\")\n",
    "                continue\n",
    "            param_str = params_part.split(\"Params: \", 1)[1]\n",
    "            try:\n",
    "                param_dict = ast.literal_eval(param_str)\n",
    "                if not isinstance(param_dict, dict):\n",
    "                    print(f\"[SKIP] Param part not a dict: {param_dict}\")\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"[SKIP] Could not parse param dict: {param_str} => {e}\")\n",
    "                continue\n",
    "\n",
    "            # Append the 3-element tuple\n",
    "            old_trials.append((trial_num, val_loss, param_dict))\n",
    "\n",
    "    return old_trials\n",
    "\n",
    "\n",
    "TESTED_PARAMS = set()\n",
    "try:\n",
    "    TESTED_PARAMS = load_old_trials(\"optuna_trials_backup.txt\")\n",
    "    print(f\"[INFO] Found {len(TESTED_PARAMS)} tested combos from the old file.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"[WARN] No 'optuna_trials_backup.txt' found. Proceeding with empty tested set.\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] Could not parse old backup file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths \n",
    "DATASET_ROOT = Path(\"C:/Users/abhis/Downloads/Documents/Learner Engagement Project/data/DAiSEE/DataSet\").resolve()\n",
    "FRAMES_ROOT  = Path(\"C:/Users/abhis/Downloads/Documents/Learner Engagement Project/data/DAiSEE/ExtractedFrames\").resolve()\n",
    "\n",
    "#Helper function\n",
    "def get_csv_clip_id(video_stem: str) -> str:\n",
    "    \"\"\"\n",
    "    Maps old filenames to new ones if needed (like 110001 -> 202614).\n",
    "    \"\"\"\n",
    "    base = video_stem.strip()\n",
    "    if base.startswith(\"110001\"):\n",
    "        base = base.replace(\"110001\", \"202614\", 1)\n",
    "    return base\n",
    "\n",
    "#numercial sort key for frames\n",
    "import re\n",
    "def numeric_sort_key(path):\n",
    "    match = re.search(r'frame_(\\d+)\\.jpg', path.name)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return 999999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the DAiSEEDataset Class\n",
    "\n",
    "This class loads video sequences and pairs them with engagement metrics.\n",
    "\n",
    "- **Features**: Uses precomputed features for faster training.\n",
    "- **Error Handling**: Skips missing video directories and logs errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAiSEEDataset(Dataset):\n",
    "    def __init__(self, root, csv_path, transform=None, seq_len=15):\n",
    "        self.root = Path(root)\n",
    "        self.transform = transform\n",
    "        self.seq_len = seq_len\n",
    "        self.video_paths = []\n",
    "        self.labels = []\n",
    "        self.missing_videos = 0\n",
    "        self.total_videos = 0\n",
    "\n",
    "        df = pd.read_csv(csv_path, dtype=str)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        split = Path(csv_path).stem.replace(\"Labels\", \"\").strip()\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            self.total_videos += 1\n",
    "            try:\n",
    "                clip_id = row['ClipID'].strip()\n",
    "                filename = clip_id.split('/')[-1] if '/' in clip_id else clip_id\n",
    "                video_stem = filename.rsplit('.', 1)[0]\n",
    "                mapped_id = get_csv_clip_id(video_stem)\n",
    "\n",
    "                video_dir = self.root / split / mapped_id\n",
    "                if not video_dir.exists():\n",
    "                    self.missing_videos += 1\n",
    "                    continue\n",
    "                \n",
    "                frames = list(video_dir.glob('frame_*.jpg'))\n",
    "                if len(frames) < self.seq_len:\n",
    "                    self.missing_videos += 1\n",
    "                    continue\n",
    "\n",
    "                boredom    = int(row['Boredom'])\n",
    "                engagement = int(row['Engagement'])\n",
    "                confusion  = int(row['Confusion'])\n",
    "                frustrate  = int(row['Frustration'])\n",
    "\n",
    "                self.video_paths.append(video_dir)\n",
    "                self.labels.append([boredom, engagement, confusion, frustrate])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {idx}: {e}\")\n",
    "\n",
    "        if not self.video_paths:\n",
    "            raise ValueError(\"No valid videos found for classification dataset.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_dir = self.video_paths[idx]\n",
    "        label_list = self.labels[idx]\n",
    "        frames_list = sorted(video_dir.glob('frame_*.jpg'), key=numeric_sort_key)[:self.seq_len]\n",
    "\n",
    "        frame_tensors = []\n",
    "        for path in frames_list:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            else:\n",
    "                from torchvision import transforms as T\n",
    "                img = T.ToTensor()(img)\n",
    "            frame_tensors.append(img)\n",
    "\n",
    "        sequence = torch.stack(frame_tensors)            # [seq_len, 3, H, W]\n",
    "        label_tensor = torch.tensor(label_list, dtype=torch.long)  # [4]\n",
    "        return sequence, label_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define the CNN-LSTM Model**\n",
    "\n",
    "**CNN-LSTM Model**:\n",
    "\n",
    "1.  ResNet50 extracts features from each frame.\n",
    "2.  LSTM processes temporal dependencies in sequences.\n",
    "3.  Outputs four-dimensional regression values (engagement metrics).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM_Classification(nn.Module):\n",
    "    def __init__(self, freeze_until='layer3'):\n",
    "        \"\"\"\n",
    "        We'll produce 16 logits total => 4 states × 4 classes each = 16.\n",
    "        Then we do a custom cross entropy for each dimension.\n",
    "        \"\"\"\n",
    "        super(CNN_LSTM_Classification, self).__init__()\n",
    "\n",
    "        # Use official weights to avoid deprecation warnings\n",
    "        self.resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        # Optionally freeze everything at first\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze from 'layer3' forward (this is partial unfreezing)\n",
    "        unfreeze = False\n",
    "        for name, child in self.resnet.named_children():\n",
    "            if name == freeze_until:\n",
    "                unfreeze = True\n",
    "            if unfreeze:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        # We'll skip the final fc layer from resnet, we do LSTM + custom FC\n",
    "        # The output of resnet.avgpool is 2048-d\n",
    "        self.lstm_hidden = 512\n",
    "        self.lstm = nn.LSTM(2048, self.lstm_hidden, batch_first=True)\n",
    "        # 16 logits: 4 states × 4 classes each\n",
    "        self.fc = nn.Linear(self.lstm_hidden, 16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len, 3, H, W]\n",
    "        Returns: [batch_size, 16 logits]\n",
    "        We'll shape them to [batch_size, 4, 4] for multi cross-entropy.\n",
    "        \"\"\"\n",
    "        bsz, seq_len, c, h, w = x.shape\n",
    "        x = x.view(-1, c, h, w)  # flatten for ResNet forward\n",
    "\n",
    "        # forward pass up to avgpool\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "\n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "\n",
    "        x = self.resnet.avgpool(x)  # shape: [batch_size*seq_len, 2048, 1, 1]\n",
    "        x = x.view(x.size(0), -1)   # [batch_size*seq_len, 2048]\n",
    "\n",
    "        # reshape for LSTM\n",
    "        x = x.view(bsz, seq_len, -1)  # [bsz, seq_len, 2048]\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # take the last time step\n",
    "        last_step = lstm_out[:, -1, :]  # [bsz, hidden]\n",
    "        logits = self.fc(last_step)     # [bsz, 16]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ConvLSMT Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=input_channels + hidden_channels,\n",
    "            out_channels=4 * hidden_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding\n",
    "        )\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        combined = torch.cat([x, h], dim=1)\n",
    "        gates = self.conv(combined)\n",
    "        i, f, o, g = torch.chunk(gates, 4, dim=1)\n",
    "        i = torch.sigmoid(i)\n",
    "        f = torch.sigmoid(f)\n",
    "        o = torch.sigmoid(o)\n",
    "        g = torch.tanh(g)\n",
    "        c_next = f * c + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.cell = ConvLSTMCell(input_channels, hidden_channels, kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x => [bsz, seq_len, in_ch, H, W]\n",
    "        \"\"\"\n",
    "        bsz, seq_len, in_ch, H, W = x.shape\n",
    "        h = torch.zeros((bsz, self.cell.hidden_channels, H, W), device=x.device)\n",
    "        c = torch.zeros((bsz, self.cell.hidden_channels, H, W), device=x.device)\n",
    "        for t in range(seq_len):\n",
    "            frame = x[:, t, :, :, :]\n",
    "            h, c = self.cell(frame, h, c)\n",
    "        return h, c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientNetB0 + ConvLSTM + Final FC => 16 Logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetB0ConvLSTM(nn.Module):\n",
    "    def __init__(self, freeze_until_block=2, convlstm_hidden=128, out_dim=16):\n",
    "        super().__init__()\n",
    "        effnet = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        self.features = effnet.features  # [bsz, 1280, h', w']\n",
    "\n",
    "        # freeze blocks 0..(freeze_until_block-1)\n",
    "        for i, child in enumerate(self.features.children()):\n",
    "            if i < freeze_until_block:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        # reduce conv from 1280 => 256\n",
    "        self.reduce_conv = nn.Conv2d(in_channels=1280, out_channels=256, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        # convlstm\n",
    "        self.convlstm = ConvLSTM(input_channels=256, hidden_channels=convlstm_hidden, kernel_size=3)\n",
    "\n",
    "        # final classification\n",
    "        self.class_conv = nn.Conv2d(convlstm_hidden, 128, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(128, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x => [bsz, seq_len, 3, 224, 224]\n",
    "        bsz, seq_len, c, H, W = x.shape\n",
    "        x = x.view(bsz * seq_len, c, H, W)\n",
    "        feats = self.features(x)         # => [bsz*seq_len, 1280, h', w']\n",
    "        feats = self.reduce_conv(feats)  # => [bsz*seq_len, 256, h', w']\n",
    "\n",
    "        _, c2, h2, w2 = feats.shape\n",
    "        feats = feats.view(bsz, seq_len, c2, h2, w2)\n",
    "\n",
    "        # convlstm => returns last hidden => [bsz, hidden, h2, w2]\n",
    "        h, c_ = self.convlstm(feats)\n",
    "\n",
    "        # classification\n",
    "        out = self.class_conv(h)   # [bsz, 128, h2, w2]\n",
    "        out = out.mean(dim=[2,3])  # global avg => [bsz, 128]\n",
    "        logits = self.fc(out)      # => [bsz, 16]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-CrossEntropy for 4 states\n",
    "def multi_ce_loss(logits, labels):\n",
    "    \"\"\"\n",
    "    logits: [batch_size, 16]\n",
    "    labels: [batch_size, 4], each label in [0..3]\n",
    "    We'll reshape logits => [batch_size, 4, 4], then do CrossEntropy for each dimension.\n",
    "    Final loss is the average or sum of the 4 cross entropies.\n",
    "    \"\"\"\n",
    "    batch_size = logits.size(0)\n",
    "    # reshape => [bsz, 4 states, 4 classes]\n",
    "    logits_reshaped = logits.view(batch_size, 4, 4)  # e.g. [bsz, 4, 4]\n",
    "\n",
    "    # separate each dimension’s logits & label\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    # We'll compute CE for each dimension (boredom, engagement, etc.)\n",
    "    total_loss = 0.0\n",
    "    for d in range(4):\n",
    "        # logits for dimension d: [bsz, 4]\n",
    "        dim_logits = logits_reshaped[:, d, :]\n",
    "        # labels for dimension d: [bsz]\n",
    "        dim_labels = labels[:, d]\n",
    "        loss_d = ce(dim_logits, dim_labels)\n",
    "        total_loss += loss_d\n",
    "\n",
    "    # average or sum\n",
    "    return total_loss / 4.0  # average across the 4 states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Loaders**\n",
    "\n",
    "**Configures train, validation, and test loaders with optimal settings.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_dataloaders(batch_size=16, seq_len=15):\n",
    "    \"\"\"\n",
    "    We'll load up to 'seq_len' frames from each folder. \n",
    "    \"\"\"\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    labels_path = DATASET_ROOT / \"Labels\"\n",
    "\n",
    "    train_ds = DAiSEEDataset(FRAMES_ROOT, labels_path / \"TrainLabels.csv\", transform=train_transform, seq_len=seq_len)\n",
    "    val_ds   = DAiSEEDataset(FRAMES_ROOT, labels_path / \"ValidationLabels.csv\", transform=val_transform, seq_len=seq_len)\n",
    "    test_ds  = DAiSEEDataset(FRAMES_ROOT, labels_path / \"TestLabels.csv\", transform=val_transform, seq_len=seq_len)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop with Optimizations\n",
    "\n",
    "- **Mixed Precision**: Uses FP16 for faster training.\n",
    "- **Checkpointing**: Saves the best model based on validation loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpointing\n",
    "def save_checkpoint(model, optimizer, epoch, best_val_loss, directory=\"models_class\"):\n",
    "    from pathlib import Path\n",
    "    import datetime\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    save_dir = Path(directory) / timestamp\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    checkpoint_path = save_dir / \"ResNet50_CNNLSTM_classification.pth\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "def load_latest_checkpoint(model, optimizer, model_dir=\"models_class\", filename=\"ResNet50_CNNLSTM_classification.pth\",\n",
    "                           device=None):\n",
    "    from pathlib import Path\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model_dir = Path(model_dir)\n",
    "    checkpoints = list(model_dir.rglob(filename))\n",
    "    if not checkpoints:\n",
    "        print(\"No checkpoint found. Starting from scratch.\")\n",
    "        return 0, float('inf')\n",
    "\n",
    "    latest_cpt = max(checkpoints, key=lambda p: p.stat().st_mtime)\n",
    "    print(f\"Loading checkpoint from {latest_cpt}\")\n",
    "    cpoint = torch.load(latest_cpt, map_location=device)\n",
    "\n",
    "    model.load_state_dict(cpoint['model_state_dict'], strict=False)\n",
    "    optimizer.load_state_dict(cpoint['optimizer_state_dict'])\n",
    "    start_epoch = cpoint.get('epoch', 0) + 1\n",
    "    best_val_loss = cpoint.get('best_val_loss', float('inf'))\n",
    "\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "    return start_epoch, best_val_loss\n",
    "\n",
    "\n",
    "# Training Loop (Classification)\n",
    "def train_classification_model(model, train_loader, val_loader,\n",
    "                               epochs=10, lr=1e-4, early_stopping_patience=2):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    model_save_dir = \"models_class\"\n",
    "    from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "    scheduler = ReduceLROnPlateau(optimizer, patience=1)\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} Train\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(enabled=True, device_type='cuda'):\n",
    "                logits = model(inputs)          # [batch, 16]\n",
    "                loss   = multi_ce_loss(logits, labels)  # custom multi-dim cross entropy\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1} Val\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                with autocast(enabled=True, device_type='cuda'):\n",
    "                    logits = model(inputs)\n",
    "                    loss = multi_ce_loss(logits, labels)\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        logger.info(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # checkpoint / early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            save_checkpoint(model, optimizer, epoch, best_val_loss, model_save_dir)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                logger.info(\"Early stopping triggered.\")\n",
    "                print(\"Early stopping triggered; training stopped.\")\n",
    "                break\n",
    "\n",
    "    print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientNetB0 + ConvLSTM Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, best_val_loss, directory=\"models_effb0_convlstm_final\"):\n",
    "    from pathlib import Path\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    save_dir = Path(directory)/ts\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ckpt_path = save_dir/\"EffB0_ConvLSTM_class.pth\"\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict()\n",
    "    }, ckpt_path)\n",
    "    print(f\"[Checkpoint Saved] {ckpt_path}\")\n",
    "\n",
    "def load_latest_checkpoint(model, optimizer, model_dir=\"models_effb0_convlstm_final\", filename=\"EffB0_ConvLSTM_class.pth\",\n",
    "                           device=None):\n",
    "    from pathlib import Path\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model_dir=Path(model_dir)\n",
    "    cpts = list(model_dir.rglob(filename))\n",
    "    if not cpts:\n",
    "        print(\"[Resume] No checkpoint found => starting from scratch.\")\n",
    "        return 0, float(\"inf\")\n",
    "    latest_ckpt = max(cpts, key=lambda p: p.stat().st_mtime)\n",
    "    print(f\"[Resume] Loading from {latest_ckpt}\")\n",
    "    checkpoint = torch.load(latest_ckpt, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    start_epoch = checkpoint.get(\"epoch\",0)+1\n",
    "    best_val = checkpoint.get(\"best_val_loss\", float(\"inf\"))\n",
    "    print(f\"[Resume] Starting from epoch={start_epoch}\")\n",
    "    return start_epoch, best_val\n",
    "\n",
    "# Evaluate\n",
    "def evaluate_classification(model, loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_preds, all_labels=[], []\n",
    "    with torch.no_grad():\n",
    "        for (inputs, labels) in tqdm(loader, desc=\"[Evaluate Final]\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            logits = model(inputs)\n",
    "            bsz = logits.size(0)\n",
    "            logits_reshaped = logits.view(bsz,4,4)\n",
    "            preds = torch.argmax(logits_reshaped, dim=2)\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "    all_labels= torch.cat(all_labels, dim=0).numpy()\n",
    "    dims = [\"Boredom\",\"Engagement\",\"Confusion\",\"Frustration\"]\n",
    "    from sklearn.metrics import classification_report\n",
    "    for d in range(4):\n",
    "        print(f\"\\nDimension: {dims[d]}\")\n",
    "        print(classification_report(all_labels[:,d], all_preds[:,d], labels=[0,1,2,3], digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Evaluation: CNN_LSTM** After training, use the evaluation function to compute additional metrics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(model, test_loader):\n",
    "    \"\"\"\n",
    "    We'll compute predicted classes for each dimension,\n",
    "    then compare them with ground truth to get classification_report\n",
    "    or custom accuracy for each dimension.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            logits = model(inputs)  # [batch, 16]\n",
    "            # shape => [batch, 4, 4]\n",
    "            logits_reshaped = logits.view(-1, 4, 4)  # 4 dims × 4 classes\n",
    "\n",
    "            # For each dimension, pick argmax\n",
    "            # shape => [batch, 4]\n",
    "            dimension_preds = torch.argmax(logits_reshaped, dim=2)\n",
    "            all_preds.append(dimension_preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0).numpy()   # shape [N, 4]\n",
    "    all_labels= torch.cat(all_labels, dim=0).numpy()  # shape [N, 4]\n",
    "\n",
    "    # We can do classification_report for each dimension\n",
    "    dimension_names = [\"Boredom\", \"Engagement\", \"Confusion\", \"Frustration\"]\n",
    "\n",
    "    for d in range(4):\n",
    "        print(f\"\\nDimension: {dimension_names[d]}\")\n",
    "        # We'll do classification report\n",
    "        print(classification_report(all_labels[:, d], all_preds[:, d],\n",
    "              labels=[0,1,2,3],\n",
    "              digits=3))\n",
    "\n",
    "    return all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Execution for CNN_LSTM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train: 100%|██████████| 304/304 [16:37<00:00,  3.28s/it]\n",
      "Epoch 1 Val: 100%|██████████| 90/90 [01:07<00:00,  1.33it/s]\n",
      "Epoch 1/15 | Train Loss: 0.8531 | Val Loss: 0.9780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to models_class\\20250213-140249\\ResNet50_CNNLSTM_classification.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train: 100%|██████████| 304/304 [13:54<00:00,  2.74s/it]\n",
      "Epoch 2 Val: 100%|██████████| 90/90 [00:49<00:00,  1.83it/s]\n",
      "Epoch 2/15 | Train Loss: 0.8068 | Val Loss: 0.9567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to models_class\\20250213-141733\\ResNet50_CNNLSTM_classification.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train: 100%|██████████| 304/304 [13:16<00:00,  2.62s/it]\n",
      "Epoch 3 Val: 100%|██████████| 90/90 [00:57<00:00,  1.56it/s]\n",
      "Epoch 3/15 | Train Loss: 0.7904 | Val Loss: 0.9583\n",
      "Epoch 4 Train: 100%|██████████| 304/304 [15:53<00:00,  3.14s/it]\n",
      "Epoch 4 Val: 100%|██████████| 90/90 [00:57<00:00,  1.55it/s]\n",
      "Epoch 4/15 | Train Loss: 0.7716 | Val Loss: 0.9696\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered; training stopped.\n",
      "Training finished.\n",
      "Loading best checkpoint for evaluation: models_class\\20250213-141733\\ResNet50_CNNLSTM_classification.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 103/103 [10:16<00:00,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimension: Boredom\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.570     0.560     0.565       747\n",
      "           1      0.395     0.495     0.440       519\n",
      "           2      0.263     0.200     0.227       335\n",
      "           3      0.000     0.000     0.000        37\n",
      "\n",
      "    accuracy                          0.453      1638\n",
      "   macro avg      0.307     0.314     0.308      1638\n",
      "weighted avg      0.439     0.453     0.443      1638\n",
      "\n",
      "\n",
      "Dimension: Engagement\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         4\n",
      "           1      0.000     0.000     0.000        81\n",
      "           2      0.548     0.678     0.606       849\n",
      "           3      0.500     0.416     0.454       704\n",
      "\n",
      "    accuracy                          0.531      1638\n",
      "   macro avg      0.262     0.274     0.265      1638\n",
      "weighted avg      0.499     0.531     0.509      1638\n",
      "\n",
      "\n",
      "Dimension: Confusion\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.704     0.963     0.813      1135\n",
      "           1      0.390     0.087     0.142       368\n",
      "           2      0.667     0.017     0.034       116\n",
      "           3      0.000     0.000     0.000        19\n",
      "\n",
      "    accuracy                          0.688      1638\n",
      "   macro avg      0.440     0.267     0.247      1638\n",
      "weighted avg      0.623     0.688     0.598      1638\n",
      "\n",
      "\n",
      "Dimension: Frustration\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.781     1.000     0.877      1279\n",
      "           1      0.000     0.000     0.000       280\n",
      "           2      0.000     0.000     0.000        56\n",
      "           3      0.000     0.000     0.000        23\n",
      "\n",
      "    accuracy                          0.781      1638\n",
      "   macro avg      0.195     0.250     0.219      1638\n",
      "weighted avg      0.610     0.781     0.685      1638\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# 1. Get Dataloaders\n",
    "train_loader, val_loader, test_loader = get_classification_dataloaders(\n",
    "    batch_size=16,  # or 16 if your GPU can handle it\n",
    "    seq_len=15     # bigger sequence length for more temporal data\n",
    ")\n",
    "\n",
    "# 2. Instantiate Model\n",
    "# e.g. freeze_until='layer3' or 'layer2' or 'layer1' depending on how much you want to fine-tune\n",
    "model = CNN_LSTM_Classification(freeze_until='layer2')\n",
    "\n",
    "# 3. Train\n",
    "train_classification_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=15,\n",
    "    lr=5e-5,              # adjust as needed\n",
    "    early_stopping_patience=2\n",
    ")\n",
    "\n",
    "# 4. Evaluate Best Checkpoint\n",
    "# Re-instantiate the same architecture\n",
    "model_eval = CNN_LSTM_Classification(freeze_until='layer3')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_eval.to(device)\n",
    "\n",
    "# load the best checkpoint\n",
    "from pathlib import Path\n",
    "model_dir = Path(\"models_class\")\n",
    "cpts = list(model_dir.rglob(\"ResNet50_CNNLSTM_classification.pth\"))\n",
    "if cpts:\n",
    "    best_ckpt = max(cpts, key=lambda p: p.stat().st_mtime)\n",
    "    print(f\"Loading best checkpoint for evaluation: {best_ckpt}\")\n",
    "    cpoint = torch.load(best_ckpt, map_location=device)\n",
    "    model_eval.load_state_dict(cpoint['model_state_dict'], strict=False)\n",
    "\n",
    "# classification evaluation\n",
    "all_preds, all_labels = evaluate_classification(model_eval, test_loader)\n",
    "# you get a classification_report per dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_previous_trials_to_study(study, old_trials):\n",
    "    import optuna\n",
    "    from optuna.distributions import CategoricalDistribution, IntDistribution, FloatDistribution\n",
    "    from optuna.trial import TrialState\n",
    "\n",
    "    # --- Step 1: Define a key mapping from backup keys to objective keys ---\n",
    "    key_mapping = {\n",
    "        'batch_sz': 'batch_size',\n",
    "        'freeze_block': 'freeze_until_block',\n",
    "        'hidden_ch': 'convlstm_hidden'\n",
    "    }\n",
    "\n",
    "    # --- Step 2: Define the distributions exactly as in your objective ---\n",
    "    param_dists = {\n",
    "        \"seq_len\": CategoricalDistribution([15]),\n",
    "        \"batch_size\": CategoricalDistribution([8, 16]),\n",
    "        \"freeze_until_block\": IntDistribution(0, 4),\n",
    "        \"convlstm_hidden\": CategoricalDistribution([64, 128, 256]),\n",
    "        \"lr\": FloatDistribution(1e-5, 5e-4, log=True)\n",
    "    }\n",
    "\n",
    "    # --- Step 3: Loop over each backup trial and convert keys/values ---\n",
    "    for trial_number, val_loss, param_dict in old_trials:\n",
    "        if val_loss is None:\n",
    "            continue  # Skip trials with no valid objective value\n",
    "\n",
    "        # Remap the old parameters into a new dict with the correct keys and types.\n",
    "        new_params = {}\n",
    "        for key, value in param_dict.items():\n",
    "            new_key = key_mapping.get(key, key)  # Use the mapped key if available; else keep as is.\n",
    "            # Convert to proper type: for integer keys and float for lr.\n",
    "            if new_key in [\"seq_len\", \"batch_size\", \"freeze_until_block\", \"convlstm_hidden\"]:\n",
    "                try:\n",
    "                    value = int(value)\n",
    "                except Exception as e:\n",
    "                    print(f\"[SKIP] Conversion error for key {new_key}: {value} -> {e}\")\n",
    "                    continue\n",
    "            elif new_key == \"lr\":\n",
    "                try:\n",
    "                    value = float(value)\n",
    "                except Exception as e:\n",
    "                    print(f\"[SKIP] Conversion error for key {new_key}: {value} -> {e}\")\n",
    "                    continue\n",
    "            new_params[new_key] = value\n",
    "\n",
    "        # --- Step 4: Validate that the converted parameters fall into the expected distributions ---\n",
    "        skip_trial = False\n",
    "        for p_name, dist in param_dists.items():\n",
    "            if p_name in new_params:\n",
    "                if not _check_if_value_in_distribution(new_params[p_name], dist):\n",
    "                    print(f\"[SKIP param mismatch] param={p_name}, value={new_params[p_name]} not in {dist}\")\n",
    "                    skip_trial = True\n",
    "                    break\n",
    "        if skip_trial:\n",
    "            continue\n",
    "\n",
    "        # --- Step 5: Create a new trial with the fixed distributions and force-set the parameters ---\n",
    "        new_trial = study.ask(fixed_distributions=param_dists)\n",
    "        for p_name, p_val in new_params.items():\n",
    "            if p_name in param_dists:\n",
    "                new_trial._suggest(p_name, p_val)\n",
    "        study.tell(new_trial, val_loss, state=TrialState.COMPLETE)\n",
    "\n",
    "    print(f\"[INFO] Imported old trials with parameter conversion.\")\n",
    "\n",
    "# Add old trials to the study with parameter conversion\n",
    "def _check_if_value_in_distribution(value, dist):\n",
    "    from optuna.distributions import CategoricalDistribution, IntDistribution, FloatDistribution\n",
    "    if isinstance(dist, CategoricalDistribution):\n",
    "        return value in dist.choices\n",
    "    elif isinstance(dist, IntDistribution):\n",
    "        return isinstance(value, int) and dist.low <= value <= dist.high\n",
    "    elif isinstance(dist, FloatDistribution):\n",
    "        return (isinstance(value, (float, int))\n",
    "                and dist.low <= float(value) <= dist.high)\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def objective(trial):\n",
    "    seq_len = trial.suggest_categorical('seq_len',[15])\n",
    "    batch_sz= trial.suggest_categorical('batch_sz',[8,16])\n",
    "    freeze_block = trial.suggest_int('freeze_block',0,4)\n",
    "    hidden_ch = trial.suggest_categorical('hidden_ch',[64,128,256])\n",
    "    lr = trial.suggest_float('lr',1e-5,5e-4, log=True)\n",
    "    # Deprecation fix => switch to suggest_float with log=True\n",
    "      \n",
    "\n",
    "    # === Skip if param combo was tested successfully before ===\n",
    "    current_params = {\n",
    "        'seq_len': seq_len,\n",
    "        'batch_sz': batch_sz,\n",
    "        'freeze_block': freeze_block,\n",
    "        'hidden_ch': hidden_ch,\n",
    "        'lr': lr\n",
    "    }\n",
    "    param_fro = frozenset(current_params.items())\n",
    "    if param_fro in TESTED_PARAMS:\n",
    "        print(f\"[SKIP] This param combo was tested with success before => pruning trial.\")\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "    # If we get here => not in the tested set or it was previously failed => proceed\n",
    "    train_loader, val_loader, _ = get_classification_dataloaders(batch_size=batch_sz, seq_len=seq_len)\n",
    "\n",
    "    model = EfficientNetB0ConvLSTM(\n",
    "        freeze_until_block=freeze_block,\n",
    "        convlstm_hidden=hidden_ch,\n",
    "        out_dim=16\n",
    "    )\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer=optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler=GradScaler()\n",
    "\n",
    "    best_val_loss= float(\"inf\")\n",
    "    patience=2\n",
    "    patience_ctr=0\n",
    "    max_epochs=5\n",
    "\n",
    "    for ep in range(max_epochs):\n",
    "        model.train()\n",
    "        run_train=0.\n",
    "        for (inputs,labels) in tqdm(train_loader, desc=f\"[Trial Ep{ep+1}]\"):\n",
    "            inputs,labels= inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast(enabled=True, device_type='cuda'):\n",
    "                logits= model(inputs)\n",
    "                loss = multi_ce_loss(logits, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            run_train+= loss.item() * inputs.size(0)\n",
    "        train_loss= run_train/ len(train_loader.dataset)\n",
    "\n",
    "        # validate\n",
    "        model.eval()\n",
    "        val_run=0.\n",
    "        with torch.no_grad():\n",
    "            for (inputs, labels) in val_loader:\n",
    "                inputs, labels= inputs.to(device), labels.to(device)\n",
    "                logits= model(inputs)\n",
    "                loss= multi_ce_loss(logits, labels)\n",
    "                val_run+= loss.item() * inputs.size(0)\n",
    "        val_loss= val_run/ len(val_loader.dataset)\n",
    "        print(f\"[Trial] Ep{ep+1}, train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
    "\n",
    "        if val_loss<best_val_loss:\n",
    "            best_val_loss= val_loss\n",
    "            patience_ctr=0\n",
    "        else:\n",
    "            patience_ctr+=1\n",
    "            if patience_ctr>= patience:\n",
    "                print(\"[Trial] Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        trial.report(val_loss, ep)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Training EfficientNetB0 + ConvLSTM Model & Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_training_with_resume(model, train_loader, val_loader,\n",
    "                               max_epochs=15, lr=1e-4, early_stop_patience=3,\n",
    "                               model_save_dir=\"models_effb0_convlstm_final\"):\n",
    "    device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer= optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler=GradScaler()\n",
    "\n",
    "    # attempt resume\n",
    "    start_ep, best_val_loss= load_latest_checkpoint(\n",
    "        model, optimizer, model_dir=model_save_dir, \n",
    "        filename=\"EffB0_ConvLSTM_class.pth\", device=device\n",
    "    )\n",
    "    patience_ctr=0\n",
    "\n",
    "    for ep in range(start_ep, max_epochs):\n",
    "        model.train()\n",
    "        run_train=0.\n",
    "        for (inputs,labels) in tqdm(train_loader, desc=f\"[Final Ep{ep+1}]\"):\n",
    "            inputs, labels= inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast(enabled=True, device_type='cuda'):\n",
    "                logits= model(inputs)\n",
    "                loss= multi_ce_loss(logits, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            run_train += loss.item()* inputs.size(0)\n",
    "        train_loss= run_train/ len(train_loader.dataset)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_run=0.\n",
    "        with torch.no_grad():\n",
    "            for (inputs,labels) in val_loader:\n",
    "                inputs, labels= inputs.to(device), labels.to(device)\n",
    "                logits= model(inputs)\n",
    "                loss= multi_ce_loss(logits, labels)\n",
    "                val_run+= loss.item()* inputs.size(0)\n",
    "        val_loss= val_run/ len(val_loader.dataset)\n",
    "        logger.info(f\"[Final Resume] Ep{ep+1}/{max_epochs}, train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
    "\n",
    "        if val_loss< best_val_loss:\n",
    "            best_val_loss= val_loss\n",
    "            patience_ctr=0\n",
    "            save_checkpoint(model, optimizer, ep, best_val_loss, directory=model_save_dir)\n",
    "        else:\n",
    "            patience_ctr+=1\n",
    "            if patience_ctr>= early_stop_patience:\n",
    "                print(\"[Final Resume] Early stopping triggered.\")\n",
    "                break\n",
    "    print(\"[Final Resume] Done or early-stopped.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Excuxecution for EfficientNetB0 + ConvLSTM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-14 22:38:21,392] Using an existing study with name 'my_effb0_convlstm_study' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using study 'my_effb0_convlstm_study' with storage 'sqlite:///optuna_study.db'.\n",
      "[WARN] No 'optuna_trials_backup.txt' found. Skipping old trials import.\n",
      "\n",
      "[OPTUNA] Completed Trials:\n",
      "Trial 0 | Val Loss: 0.9770944646974807 | Params: {'seq_len': 15, 'batch_size': 8, 'freeze_until_block': 1, 'convlstm_hidden': 64, 'lr': 3.428815705452256e-05}\n",
      "Trial 1 | Val Loss: 0.9723774410361923 | Params: {'seq_len': 15, 'batch_size': 8, 'freeze_until_block': 3, 'convlstm_hidden': 64, 'lr': 2.9768857623565067e-05}\n",
      "Trial 2 | Val Loss: 0.977101352516799 | Params: {'seq_len': 15, 'batch_size': 16, 'freeze_until_block': 4, 'convlstm_hidden': 64, 'lr': 2.3579914063343766e-05}\n",
      "Trial 3 | Val Loss: None | Params: {'seq_len': 15, 'batch_sz': 16, 'freeze_block': 4, 'hidden_ch': 64, 'lr': 0.0002524669209895806}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trial Ep1]: 100%|██████████| 607/607 [04:10<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial] Ep1, train_loss=0.8707, val_loss=0.9924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trial Ep2]: 100%|██████████| 607/607 [04:19<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial] Ep2, train_loss=0.8322, val_loss=0.9711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trial Ep3]: 100%|██████████| 607/607 [04:32<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial] Ep3, train_loss=0.8201, val_loss=1.0133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trial Ep4]: 100%|██████████| 607/607 [10:56<00:00,  1.08s/it]\n",
      "[I 2025-02-14 23:08:01,320] Trial 4 finished with value: 0.971092811324865 and parameters: {'seq_len': 15, 'batch_sz': 8, 'freeze_block': 1, 'hidden_ch': 128, 'lr': 0.00022544027989915264}. Best is trial 4 with value: 0.971092811324865.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial] Ep4, train_loss=0.8157, val_loss=1.0073\n",
      "[Trial] Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trial Ep1]: 100%|██████████| 607/607 [17:28<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial] Ep1, train_loss=0.8917, val_loss=0.9993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trial Ep2]: 100%|██████████| 607/607 [05:25<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial] Ep2, train_loss=0.8275, val_loss=0.9872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trial Ep3]: 100%|██████████| 607/607 [03:54<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial] Ep3, train_loss=0.8092, val_loss=0.9672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trial Ep4]: 100%|██████████| 607/607 [03:56<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial] Ep4, train_loss=0.8000, val_loss=1.0165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trial Ep5]: 100%|██████████| 607/607 [03:58<00:00,  2.54it/s]\n",
      "[I 2025-02-14 23:48:20,368] Trial 5 finished with value: 0.9671566304499825 and parameters: {'seq_len': 15, 'batch_sz': 8, 'freeze_block': 0, 'hidden_ch': 128, 'lr': 3.850605492222891e-05}. Best is trial 5 with value: 0.9671566304499825.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial] Ep5, train_loss=0.7903, val_loss=0.9847\n",
      "[Trial] Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trial Ep1]: 100%|██████████| 607/607 [03:42<00:00,  2.73it/s]\n",
      "[I 2025-02-14 23:52:47,057] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial] Ep1, train_loss=0.9631, val_loss=1.0131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trial Ep1]: 100%|██████████| 607/607 [02:54<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial] Ep1, train_loss=0.8713, val_loss=0.9722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trial Ep2]: 100%|██████████| 607/607 [02:53<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial] Ep2, train_loss=0.8291, val_loss=0.9871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trial Ep3]: 100%|██████████| 607/607 [02:55<00:00,  3.46it/s]\n",
      "[I 2025-02-15 00:03:39,026] Trial 7 finished with value: 0.9721880690712792 and parameters: {'seq_len': 15, 'batch_sz': 8, 'freeze_block': 4, 'hidden_ch': 128, 'lr': 0.00019389369423148}. Best is trial 5 with value: 0.9671566304499825.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial] Ep3, train_loss=0.8177, val_loss=0.9904\n",
      "[Trial] Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trial Ep1]: 100%|██████████| 607/607 [03:44<00:00,  2.70it/s]\n",
      "[I 2025-02-15 00:08:08,291] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial] Ep1, train_loss=0.8690, val_loss=1.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trial Ep1]: 100%|██████████| 607/607 [03:53<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial] Ep1, train_loss=0.8702, val_loss=0.9813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trial Ep2]: 100%|██████████| 607/607 [03:52<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial] Ep2, train_loss=0.8279, val_loss=0.9714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trial Ep3]: 100%|██████████| 607/607 [03:52<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial] Ep3, train_loss=0.8119, val_loss=0.9763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trial Ep4]: 100%|██████████| 607/607 [03:51<00:00,  2.62it/s]\n",
      "[I 2025-02-15 00:26:26,987] Trial 9 finished with value: 0.9714210024240386 and parameters: {'seq_len': 15, 'batch_sz': 8, 'freeze_block': 1, 'hidden_ch': 128, 'lr': 0.00010680591874223483}. Best is trial 5 with value: 0.9671566304499825.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial] Ep4, train_loss=0.8003, val_loss=1.0081\n",
      "[Trial] Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trial Ep1]: 100%|██████████| 304/304 [40:30<00:00,  8.00s/it]\n",
      "[I 2025-02-15 01:07:53,316] Trial 10 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial] Ep1, train_loss=0.9533, val_loss=1.0149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trial Ep1]: 100%|██████████| 304/304 [03:07<00:00,  1.62it/s]\n",
      "[I 2025-02-15 01:11:44,896] Trial 11 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial] Ep1, train_loss=0.9015, val_loss=0.9996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trial Ep1]: 100%|██████████| 607/607 [03:57<00:00,  2.56it/s]\n",
      "[I 2025-02-15 01:16:26,279] Trial 12 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial] Ep1, train_loss=0.8782, val_loss=1.1256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trial Ep1]: 100%|██████████| 607/607 [03:52<00:00,  2.62it/s]\n",
      "[I 2025-02-15 01:21:02,529] Trial 13 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial] Ep1, train_loss=0.8792, val_loss=1.0049\n",
      "\n",
      "[OPTUNA] Completed Trials:\n",
      "Trial 0 | Val Loss: 0.9770944646974807 | Params: {'seq_len': 15, 'batch_size': 8, 'freeze_until_block': 1, 'convlstm_hidden': 64, 'lr': 3.428815705452256e-05}\n",
      "Trial 1 | Val Loss: 0.9723774410361923 | Params: {'seq_len': 15, 'batch_size': 8, 'freeze_until_block': 3, 'convlstm_hidden': 64, 'lr': 2.9768857623565067e-05}\n",
      "Trial 2 | Val Loss: 0.977101352516799 | Params: {'seq_len': 15, 'batch_size': 16, 'freeze_until_block': 4, 'convlstm_hidden': 64, 'lr': 2.3579914063343766e-05}\n",
      "Trial 3 | Val Loss: None | Params: {'seq_len': 15, 'batch_sz': 16, 'freeze_block': 4, 'hidden_ch': 64, 'lr': 0.0002524669209895806}\n",
      "Trial 4 | Val Loss: 0.971092811324865 | Params: {'seq_len': 15, 'batch_sz': 8, 'freeze_block': 1, 'hidden_ch': 128, 'lr': 0.00022544027989915264}\n",
      "Trial 5 | Val Loss: 0.9671566304499825 | Params: {'seq_len': 15, 'batch_sz': 8, 'freeze_block': 0, 'hidden_ch': 128, 'lr': 3.850605492222891e-05}\n",
      "Trial 6 | Val Loss: 1.0130757861741362 | Params: {'seq_len': 15, 'batch_sz': 8, 'freeze_block': 2, 'hidden_ch': 64, 'lr': 1.0179607872791433e-05}\n",
      "Trial 7 | Val Loss: 0.9721880690712792 | Params: {'seq_len': 15, 'batch_sz': 8, 'freeze_block': 4, 'hidden_ch': 128, 'lr': 0.00019389369423148}\n",
      "Trial 8 | Val Loss: 1.0004188884714418 | Params: {'seq_len': 15, 'batch_sz': 8, 'freeze_block': 2, 'hidden_ch': 128, 'lr': 0.0001368718236226446}\n",
      "Trial 9 | Val Loss: 0.9714210024240386 | Params: {'seq_len': 15, 'batch_sz': 8, 'freeze_block': 1, 'hidden_ch': 128, 'lr': 0.00010680591874223483}\n",
      "Trial 10 | Val Loss: 1.0149002619103338 | Params: {'seq_len': 15, 'batch_sz': 16, 'freeze_block': 0, 'hidden_ch': 256, 'lr': 1.1247858205722808e-05}\n",
      "Trial 11 | Val Loss: 0.9995555904380419 | Params: {'seq_len': 15, 'batch_sz': 16, 'freeze_block': 4, 'hidden_ch': 64, 'lr': 6.173825144123174e-05}\n",
      "Trial 12 | Val Loss: 1.1255625445497377 | Params: {'seq_len': 15, 'batch_sz': 8, 'freeze_block': 0, 'hidden_ch': 128, 'lr': 0.000337051898060958}\n",
      "Trial 13 | Val Loss: 1.0048891759736294 | Params: {'seq_len': 15, 'batch_sz': 8, 'freeze_block': 1, 'hidden_ch': 128, 'lr': 0.000358247972872861}\n",
      "\n",
      "[OPTUNA] Best Trial Hyperparams:\n",
      "  seq_len: 15\n",
      "  batch_sz: 8\n",
      "  freeze_block: 0\n",
      "  hidden_ch: 128\n",
      "  lr: 3.850605492222891e-05\n",
      " best_val_loss: 0.9671566304499825\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'convlstm_hidden'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m best_batch_sz \u001b[38;5;241m=\u001b[39m best_trial\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_sz\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     53\u001b[0m best_frz_block \u001b[38;5;241m=\u001b[39m best_trial\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfreeze_block\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 54\u001b[0m best_hidden   \u001b[38;5;241m=\u001b[39m \u001b[43mbest_trial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconvlstm_hidden\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     55\u001b[0m best_lr       \u001b[38;5;241m=\u001b[39m best_trial\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# 3) Build Data w/ best hyperparams\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'convlstm_hidden'"
     ]
    }
   ],
   "source": [
    "# create or load the SQLite study\n",
    "study_name = \"my_effb0_convlstm_study\"\n",
    "storage_url = \"sqlite:///optuna_study.db\"\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    study_name=study_name,\n",
    "    storage=storage_url,\n",
    "    load_if_exists=True\n",
    ")\n",
    "print(f\"[INFO] Using study '{study_name}' with storage '{storage_url}'.\")\n",
    "\n",
    "# (Optional) load old trials from file only if file has ahead of database\n",
    "try:\n",
    "    old_saved_trials = load_old_trials(\"optuna_trials_backup.txt\")\n",
    "    if old_saved_trials:\n",
    "        print(\"\\n[BACKUP TRIALS] Loaded trials from file:\")\n",
    "        for trial_number, val_loss, params in old_saved_trials:\n",
    "            print(f\"Trial {trial_number} | Val Loss: {val_loss} | Params: {params}\")\n",
    "        \n",
    "        # Always import backup trials when starting from a fresh DB.\n",
    "        add_previous_trials_to_study(study, old_saved_trials)\n",
    "        print(f\"[INFO] Imported {len(old_saved_trials)} old trials into the study.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"[WARN] No 'optuna_trials_backup.txt' found. Skipping old trials import.\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] Could not import old trials: {e}\")\n",
    "\n",
    "    print(f\"[WARN] Could not import old trials: {e}\")\n",
    "    \n",
    "# Print out all completed trial results from the study for easy reference.\n",
    "print(\"\\n[OPTUNA] Completed Trials:\")\n",
    "for trial in study.trials:\n",
    "    print(f\"Trial {trial.number} | Val Loss: {trial.value} | Params: {trial.params}\")\n",
    "\n",
    "# Now do the new search\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Print out all completed trial results from the study for easy reference.\n",
    "print(\"\\n[OPTUNA] Completed Trials:\")\n",
    "for trial in study.trials:\n",
    "    print(f\"Trial {trial.number} | Val Loss: {trial.value} | Params: {trial.params}\")\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(\"\\n[OPTUNA] Best Trial Hyperparams:\")\n",
    "for k, v in best_trial.params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\" best_val_loss:\", best_trial.value)\n",
    "\n",
    "# 2) Grab best hyperparams\n",
    "best_seq_len  = best_trial.params['seq_len']\n",
    "best_batch_sz = best_trial.params['batch_sz']\n",
    "best_frz_block = best_trial.params['freeze_block']\n",
    "best_hidden   = best_trial.params['hidden_ch']\n",
    "best_lr       = best_trial.params['lr']\n",
    "\n",
    "# 3) Build Data w/ best hyperparams\n",
    "train_loader, val_loader, test_loader = get_classification_dataloaders(\n",
    "    batch_size=best_batch_sz, seq_len=best_seq_len\n",
    ")\n",
    "\n",
    "# 4) Build Model w/ best hyperparams\n",
    "final_model = EfficientNetB0ConvLSTM(\n",
    "    freeze_until_block=best_frz_block,\n",
    "    convlstm_hidden=best_hidden,\n",
    "    out_dim=16\n",
    ")\n",
    "\n",
    "# 5) Final Training w/ Resume\n",
    "final_training_with_resume(\n",
    "    model=final_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    max_epochs=15,\n",
    "    lr=best_lr,\n",
    "    early_stop_patience=3,\n",
    "    model_save_dir=\"models_effb0_convlstm_final\"\n",
    ")\n",
    "\n",
    "# 6) Evaluate on test set\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "final_model.to(device)\n",
    "\n",
    "# load best final checkpoint for eval\n",
    "from pathlib import Path\n",
    "final_ckpt_dir = Path(\"models_effb0_convlstm_final\")\n",
    "cpts = list(final_ckpt_dir.rglob(\"EffB0_ConvLSTM_class.pth\"))\n",
    "if cpts:\n",
    "    best_ckpt = max(cpts, key=lambda p: p.stat().st_mtime)\n",
    "    print(f\"[Final Resume] Loading best checkpoint for evaluation: {best_ckpt}\")\n",
    "    cpoint = torch.load(best_ckpt, map_location=device)\n",
    "    final_model.load_state_dict(cpoint['model_state_dict'], strict=False)\n",
    "\n",
    "# Evaluate\n",
    "final_model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for (inputs, labels) in tqdm(test_loader, desc=\"[Evaluate Final]\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits = final_model(inputs)\n",
    "        bsz = logits.size(0)\n",
    "        logits_reshaped = logits.view(bsz, 4, 4)\n",
    "        preds = torch.argmax(logits_reshaped, dim=2)\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "dims = [\"Boredom\", \"Engagement\", \"Confusion\", \"Frustration\"]\n",
    "from sklearn.metrics import classification_report\n",
    "for d in range(4):\n",
    "    print(f\"\\nDimension: {dims[d]}\")\n",
    "    print(classification_report(all_labels[:, d], all_preds[:, d], labels=[0, 1, 2, 3], digits=3))\n",
    "\n",
    "print(\"[Done] EfficientNetB0 + ConvLSTM + Optuna + Resume complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
