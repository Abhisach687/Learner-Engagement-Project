{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved successfully at models/model.pth\n"
     ]
    }
   ],
   "source": [
    "#this is to be removed ... this is just a dummy model to test the pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "# Define the model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(2048, 3)  # Assuming 3 classes: bored, attentive, confused\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Dummy training loop (Replace with real data)\n",
    "for epoch in range(1):  # Run real training loop instead\n",
    "    optimizer.zero_grad()\n",
    "    dummy_input = torch.randn(1, 3, 224, 224)\n",
    "    output = model(dummy_input)\n",
    "    loss = loss_fn(output, torch.tensor([1]))  # Example target class\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Save trained model\n",
    "torch.save(model.state_dict(), \"../models/model.pth\")\n",
    "print(\"✅ Model saved successfully at models/model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Required Libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm\n",
    "from torch import device\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from PIL import Image\n",
    "\n",
    "# Configure logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.FileHandler('model_training.log')\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.addHandler(logging.StreamHandler())  # Add console logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATASET_ROOT = Path(\"C:/Users/abhis/Downloads/Documents/Learner Engagement Project/data/DAiSEE/DataSet\").resolve()\n",
    "FRAMES_ROOT = Path(\"C:/Users/abhis/Downloads/Documents/Learner Engagement Project/data/DAiSEE/ExtractedFrames\").resolve()\n",
    "\n",
    "# Mapping function\n",
    "def get_csv_clip_id(video_stem: str) -> str:\n",
    "    base = video_stem.strip()\n",
    "    if base.startswith(\"110001\"):\n",
    "        base = base.replace(\"110001\", \"202614\", 1)\n",
    "    return base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the DAiSEEDataset Class\n",
    "\n",
    "This class loads video sequences and pairs them with engagement metrics.\n",
    "\n",
    "- **Features**: Uses precomputed features for faster training.\n",
    "- **Error Handling**: Skips missing video directories and logs errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class DAiSEEDataset(Dataset):\n",
    "    def __init__(self, root, csv_path, transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.transform = transform\n",
    "        self.video_paths = []\n",
    "        self.labels = []\n",
    "        self.missing_videos = 0\n",
    "        self.total_videos = 0\n",
    "\n",
    "        df = pd.read_csv(csv_path, dtype=str)  # Read all columns as strings\n",
    "        df.columns = df.columns.str.strip()  # Remove whitespace from column names\n",
    "        split = Path(csv_path).stem.replace(\"Labels\", \"\").strip()\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            self.total_videos += 1\n",
    "            try:\n",
    "                clip_id = row['ClipID'].strip()\n",
    "                filename = clip_id.split('/')[-1] if '/' in clip_id else clip_id\n",
    "                video_stem = filename.rsplit('.', 1)[0]\n",
    "                mapped_id = get_csv_clip_id(video_stem)\n",
    "                \n",
    "                print(f\"Processing video {video_stem} -> mapped to {mapped_id}\")  # Debug print\n",
    "                \n",
    "                video_dir = self.root / split / mapped_id\n",
    "                if not video_dir.exists():\n",
    "                    print(f\"Video directory does not exist: {video_dir}\")  # Debug print\n",
    "                    self.missing_videos += 1\n",
    "                    continue\n",
    "                \n",
    "                frames = list(video_dir.glob('frame_*.jpg'))\n",
    "                if len(frames) < 15:\n",
    "                    print(f\"Insufficient frames ({len(frames)}) in {video_dir}\")  # Debug print\n",
    "                    self.missing_videos += 1\n",
    "                    continue\n",
    "                \n",
    "                self.video_paths.append(video_dir)\n",
    "                self.labels.append([\n",
    "                    row['Boredom'].strip(),\n",
    "                    row['Engagement'].strip(),\n",
    "                    row['Confusion'].strip(),\n",
    "                    row['Frustration'].strip()\n",
    "                ])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {idx}: {e}\")\n",
    "        \n",
    "        if not self.video_paths:\n",
    "            raise ValueError(\"No valid videos found\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_dir = self.video_paths[idx]\n",
    "        label = [float(val) for val in self.labels[idx]]  # Convert labels to floats\n",
    "        \n",
    "        frames = sorted(video_dir.glob('frame_*.jpg'))[:15]\n",
    "        frame_tensors = []\n",
    "        for path in frames:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            else:\n",
    "                img = transforms.ToTensor()(img)\n",
    "            frame_tensors.append(img)\n",
    "        \n",
    "        sequence = torch.stack(frame_tensors)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        return sequence, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define the CNN-LSTM Model**\n",
    "\n",
    "**CNN-LSTM Model**:\n",
    "\n",
    "1.  ResNet50 extracts features from each frame.\n",
    "2.  LSTM processes temporal dependencies in sequences.\n",
    "3.  Outputs four-dimensional regression values (engagement metrics).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.feature_extractor = nn.Sequential(*modules)\n",
    "        \n",
    "        self.lstm = nn.LSTM(2048, 512, batch_first=True)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, c, h, w = x.size()\n",
    "        x = x.view(-1, c, h, w)\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(features)\n",
    "        lstm_out = lstm_out[:, -1]\n",
    "        out = self.fc(lstm_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Loaders**\n",
    "\n",
    "**Configures train, validation, and test loaders with optimal settings.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "valid_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_dataloaders(batch_size=16):\n",
    "    labels_path = DATASET_ROOT / \"Labels\"\n",
    "    \n",
    "    datasets = {\n",
    "        \"Train\": DAiSEEDataset(FRAMES_ROOT, labels_path / \"TrainLabels.csv\", transform=train_transform),\n",
    "        \"Validation\": DAiSEEDataset(FRAMES_ROOT, labels_path / \"ValidationLabels.csv\", transform=valid_transform),\n",
    "        \"Test\": DAiSEEDataset(FRAMES_ROOT, labels_path / \"TestLabels.csv\", transform=valid_transform)\n",
    "    }\n",
    "\n",
    "    dataloaders = {}\n",
    "    num_workers = 0  # Use 0 for Windows\n",
    "\n",
    "    for split, dataset in datasets.items():\n",
    "        dataloaders[split] = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True if split == \"Train\" else False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    return dataloaders\n",
    "\n",
    "\n",
    "# Get data loaders\n",
    "dataloaders = get_dataloaders(batch_size=16)\n",
    "train_loader, val_loader, test_loader = dataloaders['Train'], dataloaders['Validation'], dataloaders['Test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop with Optimizations\n",
    "\n",
    "- **Mixed Precision**: Uses FP16 for faster training.\n",
    "- **Checkpointing**: Saves the best model based on validation loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    scaler = torch.amp.GradScaler()  \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} Train\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1} Val\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        logger.info(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Training Process\n",
    "\n",
    "Execute the training loop and evaluate the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    mse = np.mean((all_preds - all_labels) ** 2, axis=0)\n",
    "    r2 = r2_score(all_labels, all_preds, multioutput='raw_values')\n",
    "    return mse, r2\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the model\n",
    "    model = CNN_LSTM(num_classes=4)\n",
    "    model = model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(model, train_loader, val_loader, epochs=20)\n",
    "    \n",
    "    # Load and evaluate the best model\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "    \n",
    "    test_mse, test_r2 = evaluate(model, test_loader)\n",
    "    logger.info(f\"Test MSE: {test_mse}\\nTest R²: {test_r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
