{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\abhis\\Downloads\\Documents\\Learner Engagement Project\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved successfully at models/model.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "# Define the model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(2048, 3)  # Assuming 3 classes: bored, attentive, confused\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Dummy training loop (Replace with real data)\n",
    "for epoch in range(1):  # Run real training loop instead\n",
    "    optimizer.zero_grad()\n",
    "    dummy_input = torch.randn(1, 3, 224, 224)\n",
    "    output = model(dummy_input)\n",
    "    loss = loss_fn(output, torch.tensor([1]))  # Example target class\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Save trained model\n",
    "torch.save(model.state_dict(), \"../models/model.pth\")\n",
    "print(\"✅ Model saved successfully at models/model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the DAiSEEDataset Class\n",
    "\n",
    "This cell defines the full custom dataset class for DAiSEE. This definition is required for loading the saved dataset checkpoints and for training the model. Make sure to run this cell before loading any checkpoints.\n",
    "\n",
    "Below is the code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "class DAiSEEDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for DAiSEE that reads a CSV with engagement metrics,\n",
    "    precomputes frame paths based on the ClipID, and loads images & labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, csv_path, transform=None, use_cache=True):\n",
    "        self.root = Path(root)\n",
    "        self.transform = transform\n",
    "        self.use_cache = use_cache\n",
    "        \n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.frame_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        logger.info(f\"Processing {len(self.data)} entries from {csv_path.name}\")\n",
    "        for idx, row in tqdm(self.data.iterrows(), total=len(self.data), desc=\"Loading dataset\"):\n",
    "            try:\n",
    "                clip_id = str(row['ClipID']).strip()\n",
    "                if '.avi' in clip_id:\n",
    "                    clip_id = clip_id.replace('.avi', '')\n",
    "                parts = clip_id.split('/')\n",
    "                \n",
    "                if len(parts) == 1:\n",
    "                    split_guess = csv_path.stem.replace(\"Labels\", \"\").strip()\n",
    "                    frame_path = self.root / split_guess / parts[0] / \"frame_0001.jpg\"\n",
    "                elif len(parts) == 2:\n",
    "                    frame_path = self.root / parts[0] / parts[1] / \"frame_0001.jpg\"\n",
    "                elif len(parts) >= 3:\n",
    "                    frame_path = self.root / parts[0] / parts[1] / parts[2] / \"frame_0001.jpg\"\n",
    "                else:\n",
    "                    logger.debug(f\"Skipping row {idx}: unexpected ClipID format: {clip_id}\")\n",
    "                    continue\n",
    "\n",
    "                if not frame_path.exists():\n",
    "                    logger.debug(f\"Frame path does not exist: {frame_path}\")\n",
    "                else:\n",
    "                    logger.debug(f\"Found frame: {frame_path}\")\n",
    "                \n",
    "                if frame_path.exists():\n",
    "                    self.frame_paths.append(frame_path)\n",
    "                    self.labels.append([\n",
    "                        float(row['Boredom']),\n",
    "                        float(row['Engagement']),\n",
    "                        float(row['Confusion']),\n",
    "                        float(str(row['Frustration ']).strip())\n",
    "                    ])\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Skipping row {idx} due to error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.frame_paths)} valid frames\")\n",
    "        if len(self.frame_paths) == 0:\n",
    "            raise ValueError(f\"No valid frames found in {csv_path}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.frame_paths[idx]\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        # Use torchvision.io.read_image for fast loading and convert if needed.\n",
    "        img_tensor = read_image(str(img_path))\n",
    "        if self.transform:\n",
    "            img = self.transform(to_pil_image(img_tensor))\n",
    "        else:\n",
    "            img = img_tensor\n",
    "        return img, label\n",
    "\n",
    "# Register the dataset class for safe deserialization.\n",
    "torch.serialization.add_safe_globals([\"DAiSEEDataset\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Saved Datasets\n",
    "\n",
    "This cell loads the full datasets that were saved in 001_data.ipynb (e.g., `train_set.pth`, `val_set.pth`, and `test_set.pth`). Make sure to run this cell before training the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the path where the dataset checkpoints were saved.\n",
    "FRAMES_ROOT = \"C:/Users/abhis/Downloads/Documents/Learner Engagement Project/data/DAiSEE/ExtractedFrames\"\n",
    "\n",
    "# Load the saved datasets using the full DAiSEEDataset class.\n",
    "train_dataset = torch.load(os.path.join(FRAMES_ROOT, \"train_set.pth\"), weights_only=False)\n",
    "val_dataset = torch.load(os.path.join(FRAMES_ROOT, \"val_set.pth\"), weights_only=False)\n",
    "test_dataset = torch.load(os.path.join(FRAMES_ROOT, \"test_set.pth\"), weights_only=False)\n",
    "\n",
    "print(\"✅ Datasets loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders\n",
    "\n",
    "This cell creates DataLoaders for the training, validation, and testing datasets. These loaders will feed batches of data to your ML/DL models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataLoaders created successfully!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4  # On Windows, consider using 0 if you encounter issues\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(\"✅ DataLoaders created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
